{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grid Point CMIP6 Retrieval and MET Conversion Notebook (v2)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook extracts CMIP6 data from NetCDF files and converts them to APSIM MET format. It processes climate variables from NC files organized in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}` folders, extracts data for specified coordinates, creates CSV cache files, and generates MET files for APSIM simulations.\n",
        "\n",
        "## File Structure\n",
        "\n",
        "- **Input**: NC files in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\` (e.g., `ACCESS CM2 SSP245`)\n",
        "- **User Input**: Model, Scenario, Latitude and longitude coordinates (decimal degrees)\n",
        "- **Output**: MET files, CSV cache files, and combined CSV files for the specified coordinate\n",
        "\n",
        "## Variables Processed\n",
        "\n",
        "The notebook processes 4 climate variables:\n",
        "\n",
        "- **tasmax**: Daily maximum temperature (°C) → **maxt** in MET format\n",
        "- **tasmin**: Daily minimum temperature (°C) → **mint** in MET format\n",
        "- **pr**: Daily precipitation (mm) → **rain** in MET format\n",
        "- **rsds**: Daily surface downwelling shortwave radiation (W/m²) → **radn** (MJ/m²) in MET format\n",
        "\n",
        "**Note**: vp (vapor pressure) and evap (evapotranspiration) fields are left blank in the MET file. code field is hardcoded to \"222222\".\n",
        "\n",
        "## MET Format Specifications\n",
        "\n",
        "The MET format is used by APSIM for weather data input. It includes:\n",
        "\n",
        "- **Required fields**: year, day, maxt (from tasmax), mint (from tasmin), rain (from pr), radn (from rsds)\n",
        "- **radn field**: Required, converted from rsds (W/m² to MJ/m²)\n",
        "- **Blank fields**: evap (evaporation), vp (vapor pressure) - left blank\n",
        "- **Code field**: Hardcoded to \"222222\" for all rows\n",
        "- **Metadata**: latitude, longitude, tav (annual average temperature), amp (annual amplitude)\n",
        "\n",
        "## File Naming Conventions\n",
        "\n",
        "- **Input NC files**: `{Model} {Scenario}\\*{variable}*.nc` (e.g., `ACCESS CM2 SSP245\\tasmax*.nc`)\n",
        "- **Output CSV cache files**: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60_tasmax.csv`)\n",
        "- **Output MET files**: `{Model}_{Scenario}_{Lat}_{Lon}.met` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60.met`)\n",
        "- **Output CSV files**: `{Model}_{Scenario}_{Lat}_{Lon}.csv` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60.csv`)\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "1. Set the configuration parameters (Model, Scenario, output directory) in Section 1\n",
        "2. Provide your target latitude and longitude coordinates in Section 2\n",
        "3. Run all cells sequentially to extract data from NetCDF files and create MET files\n",
        "4. Output files (CSV cache and MET) will be saved with coordinate-based naming\n",
        "\n",
        "## Coordinate Matching\n",
        "\n",
        "The notebook finds the nearest grid point to your specified coordinates within a tolerance (default: 0.01 degrees ≈ 1.1 km). If the nearest point is outside tolerance, a warning will be displayed.\n",
        "\n",
        "## Notes\n",
        "\n",
        "- **CMIP6 data structure**: Data is organized by Model and Scenario in separate folders\n",
        "- **Coordinate format**: Provide coordinates in decimal degrees (latitude: -90 to 90, longitude: -180 to 180)\n",
        "- **Variable extraction**: 4 variables (tasmax, tasmin, pr, rsds) are extracted from NC files and saved as individual CSV cache files\n",
        "- **MET conversion**: tasmax→maxt, tasmin→mint, pr→rain, rsds→radn\n",
        "- **Unit conversions**: rsds (W/m²) is converted to radn (MJ/m²) by multiplying by 0.0864\n",
        "- **Blank fields**: vp and evap are left blank in MET format\n",
        "- **Code field**: Hardcoded to \"222222\" for all rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  - CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
            "  - Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n",
            "  - Model: ACCESS CM2\n",
            "  - Scenario: obs\n",
            "  - Date Range: 1986 to 2014\n",
            "  - Coordinate Tolerance: 0.01 degrees\n",
            "  - Variables: tasmax, tasmin, pr, rsds\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
        "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\"  # Output directory for MET files and CSV cache\n",
        "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
        "\n",
        "# Model and Scenario - UPDATE THESE AS NEEDED\n",
        "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
        "SCENARIO = \"obs\"  # e.g., \"obs\", \"SSP245\", or \"SSP585\"\n",
        "\n",
        "# Date Range - UPDATE THESE AS NEEDED\n",
        "START_YEAR = 1986  # Start year for MET file (None = use all available data)\n",
        "END_YEAR = 2014  # End year for MET file (None = use all available data)\n",
        "\n",
        "# Variables to process (4 variables)\n",
        "# MET format mapping:\n",
        "# - tasmax → maxt (maximum temperature)\n",
        "# - tasmin → mint (minimum temperature)\n",
        "# - pr → rain (precipitation)\n",
        "# - rsds → radn (radiation, converted from W/m² to MJ/m²)\n",
        "VARIABLES = ['tasmax', 'tasmin', 'pr', 'rsds']\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  - CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
        "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"  - Model: {MODEL}\")\n",
        "print(f\"  - Scenario: {SCENARIO}\")\n",
        "print(f\"  - Date Range: {START_YEAR if START_YEAR is not None else 'all available'} to {END_YEAR if END_YEAR is not None else 'all available'}\")\n",
        "print(f\"  - Coordinate Tolerance: {COORD_TOLERANCE} degrees\")\n",
        "print(f\"  - Variables: {', '.join(VARIABLES)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: User Coordinate Input\n",
        "\n",
        "Provide the latitude and longitude coordinates for the grid point you want to process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Coordinate:\n",
            "  Latitude: -31.450000°\n",
            "  Longitude: 117.550000°\n",
            "  Model: ACCESS CM2\n",
            "  Scenario: obs\n",
            "  Tolerance: 0.01 degrees\n"
          ]
        }
      ],
      "source": [
        "# USER INPUT: Provide your target coordinates here\n",
        "LATITUDE = -31.45  # Target latitude in decimal degrees (-90 to 90)\n",
        "LONGITUDE = 117.55  # Target longitude in decimal degrees (-180 to 180)\n",
        "\n",
        "# Validate coordinates\n",
        "if not (-90 <= LATITUDE <= 90):\n",
        "    raise ValueError(f\"Latitude must be between -90 and 90. Provided: {LATITUDE}\")\n",
        "if not (-180 <= LONGITUDE <= 180):\n",
        "    raise ValueError(f\"Longitude must be between -180 and 180. Provided: {LONGITUDE}\")\n",
        "\n",
        "print(f\"Target Coordinate:\")\n",
        "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
        "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
        "print(f\"  Model: {MODEL}\")\n",
        "print(f\"  Scenario: {SCENARIO}\")\n",
        "print(f\"  Tolerance: {COORD_TOLERANCE} degrees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: NetCDF Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cached_variable_path(output_dir, model_scenario, lat_str, lon_str, variable):\n",
        "    \"\"\"\n",
        "    Generate the path for a cached variable CSV file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_dir : str\n",
        "        Output directory\n",
        "    model_scenario : str\n",
        "        Model and scenario string (e.g., \"ACCESS_CM2_SSP245\")\n",
        "    lat_str : str\n",
        "        Latitude formatted as string (e.g., \"-31.75\")\n",
        "    lon_str : str\n",
        "        Longitude formatted as string (e.g., \"117.60\")\n",
        "    variable : str\n",
        "        Variable name (e.g., \"tasmax\", \"tasmin\", \"pr\", \"rsds\")\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Path to cached CSV file\n",
        "    \"\"\"\n",
        "    cache_filename = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
        "    cache_path = os.path.join(output_dir, cache_filename)\n",
        "    return cache_path\n",
        "\n",
        "\n",
        "def load_cached_variable(cache_path):\n",
        "    \"\"\"\n",
        "    Load cached variable data from CSV file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    cache_path : str\n",
        "        Path to cached CSV file\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame or None\n",
        "        DataFrame with date and value columns if file exists and is valid, None otherwise\n",
        "    \"\"\"\n",
        "    print(f\"  [INFO] Checking cache: {os.path.basename(cache_path)}\")\n",
        "    \n",
        "    if not os.path.exists(cache_path):\n",
        "        print(f\"  [INFO] Cache file not found, will extract from NetCDF\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(cache_path)\n",
        "        if 'date' not in df.columns or 'value' not in df.columns:\n",
        "            print(f\"  [WARNING] Cached file missing required columns, will re-extract\")\n",
        "            return None\n",
        "        \n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        \n",
        "        # Basic validation - check if file has data\n",
        "        if len(df) == 0:\n",
        "            print(f\"  [WARNING] Cached file is empty, will re-extract\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  [INFO] Cache file found and valid - loaded {len(df):,} records\")\n",
        "        print(f\"  [INFO] Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        return df\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"  [WARNING] Error loading cached file: {e}, will re-extract\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def save_cached_variable(df, cache_path):\n",
        "    \"\"\"\n",
        "    Save extracted variable data to CSV cache file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with date and value columns\n",
        "    cache_path : str\n",
        "        Path to save cached CSV file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df[['date', 'value']].to_csv(\n",
        "            cache_path,\n",
        "            index=False,\n",
        "            encoding='utf-8',\n",
        "            float_format='%.6f'\n",
        "        )\n",
        "        print(f\"  [INFO] Saved to cache: {os.path.basename(cache_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [WARNING] Failed to save cache: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.558014Z",
          "iopub.status.busy": "2025-12-21T10:28:42.557889Z",
          "iopub.status.idle": "2025-12-21T10:28:42.569492Z",
          "shell.execute_reply": "2025-12-21T10:28:42.569072Z"
        }
      },
      "outputs": [],
      "source": [
        "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    netcdf_dir : str\n",
        "        Directory containing NetCDF files for the variable\n",
        "    variable : str\n",
        "        Variable name (tasmax, tasmin, pr, rsds)\n",
        "    target_lat : float\n",
        "        Target latitude\n",
        "    target_lon : float\n",
        "        Target longitude\n",
        "    tolerance : float\n",
        "        Coordinate matching tolerance in degrees\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with columns: date, value\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Find all NetCDF files in the directory\n",
        "    # Pattern 1: Files directly in the directory matching *{variable}*.nc\n",
        "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
        "    \n",
        "    # Pattern 2: Files in subdirectories named {variable}_* (e.g., pr_ACCESS CM2 SSP245)\n",
        "    if len(nc_files) == 0:\n",
        "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
        "        for var_subdir in var_subdirs:\n",
        "            if os.path.isdir(var_subdir):\n",
        "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
        "                    break\n",
        "    \n",
        "    # Pattern 2b: For rsds, also check for \"rad_\" folder (folder named \"rad_\" but files contain \"rsds\")\n",
        "    # Example: rad_ACCESS CM2 SSP245/ contains files named *rsds*.nc\n",
        "    if len(nc_files) == 0 and variable == 'rsds':\n",
        "        rad_subdirs = glob.glob(os.path.join(netcdf_dir, \"rad_*\"))\n",
        "        for rad_subdir in rad_subdirs:\n",
        "            if os.path.isdir(rad_subdir):\n",
        "                # Search for files containing \"rsds\" in the rad_ folder\n",
        "                found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*rsds*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
        "                    break\n",
        "                # Fallback: if no rsds files found, try all .nc files\n",
        "                if len(nc_files) == 0:\n",
        "                    found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*.nc\")))\n",
        "                    if found_files:\n",
        "                        nc_files.extend(found_files)\n",
        "                        print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
        "                        break\n",
        "    \n",
        "    # Pattern 3: Check subdirectory named exactly after the variable\n",
        "    if len(nc_files) == 0:\n",
        "        var_dir = os.path.join(netcdf_dir, variable)\n",
        "        if os.path.exists(var_dir) and os.path.isdir(var_dir):\n",
        "            nc_files = sorted(glob.glob(os.path.join(var_dir, \"*.nc\")))\n",
        "            if len(nc_files) > 0:\n",
        "                print(f\"  Found files in subdirectory: {variable}/\")\n",
        "    \n",
        "    if len(nc_files) == 0:\n",
        "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
        "        print(f\"  Searched patterns:\")\n",
        "        print(f\"    - {netcdf_dir}/*{variable}*.nc\")\n",
        "        print(f\"    - {netcdf_dir}/{variable}_*/*.nc\")\n",
        "        if variable == 'rsds':\n",
        "            print(f\"    - {netcdf_dir}/rad_*/*rsds*.nc\")\n",
        "            print(f\"    - {netcdf_dir}/rad_*/*.nc\")\n",
        "        print(f\"    - {netcdf_dir}/{variable}/*.nc\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
        "    \n",
        "    # Cache coordinate information from first file\n",
        "    lat_name = None\n",
        "    lon_name = None\n",
        "    time_name = None\n",
        "    lat_idx = None\n",
        "    lon_idx = None\n",
        "    actual_lat = None\n",
        "    actual_lon = None\n",
        "    var_name = None\n",
        "    \n",
        "    # List to store daily data\n",
        "    all_data = []\n",
        "    \n",
        "    # Process first file to get coordinate structure\n",
        "    if len(nc_files) > 0:\n",
        "        try:\n",
        "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
        "            \n",
        "            # Get variable name\n",
        "            for v in ds_sample.data_vars:\n",
        "                if variable in v.lower() or v.lower() in variable.lower():\n",
        "                    var_name = v\n",
        "                    break\n",
        "            \n",
        "            # For rsds, also check for \"rad\" variable name (some datasets use \"rad\" instead of \"rsds\")\n",
        "            if var_name is None and variable == 'rsds':\n",
        "                for v in ds_sample.data_vars:\n",
        "                    if 'rad' in v.lower() and 'rsds' not in v.lower():\n",
        "                        var_name = v\n",
        "                        break\n",
        "            \n",
        "            if var_name is None:\n",
        "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
        "                # For rsds, also try \"rad\" as variable name\n",
        "                if variable == 'rsds':\n",
        "                    possible_names.extend(['rad', 'RAD', 'rad_day'])\n",
        "                for name in possible_names:\n",
        "                    if name in ds_sample.data_vars:\n",
        "                        var_name = name\n",
        "                        break\n",
        "            \n",
        "            # Get coordinate names\n",
        "            for coord in ds_sample.coords:\n",
        "                coord_lower = coord.lower()\n",
        "                if 'lat' in coord_lower:\n",
        "                    lat_name = coord\n",
        "                elif 'lon' in coord_lower:\n",
        "                    lon_name = coord\n",
        "                elif 'time' in coord_lower:\n",
        "                    time_name = coord\n",
        "            \n",
        "            if lat_name and lon_name:\n",
        "                # Check coordinate bounds of the NetCDF file\n",
        "                lat_values = ds_sample[lat_name].values\n",
        "                lon_values = ds_sample[lon_name].values\n",
        "                \n",
        "                lat_min = float(np.min(lat_values))\n",
        "                lat_max = float(np.max(lat_values))\n",
        "                lon_min = float(np.min(lon_values))\n",
        "                lon_max = float(np.max(lon_values))\n",
        "                \n",
        "                # Check if target coordinate is within file bounds\n",
        "                lat_in_bounds = lat_min <= target_lat <= lat_max\n",
        "                lon_in_bounds = lon_min <= target_lon <= lon_max\n",
        "                \n",
        "                print(f\"  Grid bounds: Lat [{lat_min:.4f}, {lat_max:.4f}], Lon [{lon_min:.4f}, {lon_max:.4f}]\")\n",
        "                print(f\"  Target coordinate: ({target_lat:.4f}, {target_lon:.4f})\")\n",
        "                \n",
        "                if not lat_in_bounds:\n",
        "                    print(f\"  [WARNING] Target latitude {target_lat:.4f} is OUTSIDE file bounds [{lat_min:.4f}, {lat_max:.4f}]\")\n",
        "                    print(f\"  [WARNING] This may cause all values to be zero or incorrect!\")\n",
        "                if not lon_in_bounds:\n",
        "                    print(f\"  [WARNING] Target longitude {target_lon:.4f} is OUTSIDE file bounds [{lon_min:.4f}, {lon_max:.4f}]\")\n",
        "                    print(f\"  [WARNING] This may cause all values to be zero or incorrect!\")\n",
        "                \n",
        "                if lat_in_bounds and lon_in_bounds:\n",
        "                    print(f\"  [OK] Target coordinate is within file bounds\")\n",
        "                \n",
        "                # Find nearest grid point (cache indices)\n",
        "                lat_idx = np.abs(lat_values - target_lat).argmin()\n",
        "                lon_idx = np.abs(lon_values - target_lon).argmin()\n",
        "                \n",
        "                actual_lat = float(lat_values[lat_idx])\n",
        "                actual_lon = float(lon_values[lon_idx])\n",
        "                \n",
        "                # Check if within tolerance\n",
        "                lat_diff = abs(actual_lat - target_lat)\n",
        "                lon_diff = abs(actual_lon - target_lon)\n",
        "                \n",
        "                if lat_diff > tolerance or lon_diff > tolerance:\n",
        "                    print(f\"  [WARNING] Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
        "                    print(f\"  [WARNING] Distance: {lat_diff:.4f}° lat, {lon_diff:.4f}° lon (tolerance: {tolerance:.4f}°)\")\n",
        "                else:\n",
        "                    print(f\"  [OK] Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
        "                    print(f\"  [OK] Distance from target: {lat_diff:.4f}° lat, {lon_diff:.4f}° lon\")\n",
        "            \n",
        "            ds_sample.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not read sample file: {e}\")\n",
        "    \n",
        "    if var_name is None or lat_idx is None or lon_idx is None:\n",
        "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
        "        return None\n",
        "    \n",
        "    # Process all files with progress bar\n",
        "    print(f\"  Processing files...\")\n",
        "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
        "        try:\n",
        "            # Open NetCDF file with minimal decoding for speed\n",
        "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
        "            \n",
        "            # Extract data using cached indices\n",
        "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
        "            \n",
        "            # Convert to numpy array (load into memory)\n",
        "            values = data.values\n",
        "            if values.ndim > 1:\n",
        "                values = values.flatten()\n",
        "            \n",
        "            # Get time values - try multiple methods to ensure accuracy and handle leap years (366 days)\n",
        "            time_values = None\n",
        "            \n",
        "            # Method 1: Try to use time coordinate from NetCDF file (most reliable)\n",
        "            if time_name and time_name in ds.coords:\n",
        "                try:\n",
        "                    time_coord = ds[time_name]\n",
        "                    if len(time_coord) == len(values):\n",
        "                        # Try to decode times\n",
        "                        try:\n",
        "                            # Decode time coordinate\n",
        "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
        "                            time_values = pd.to_datetime(time_decoded.values)\n",
        "                            if len(time_values) == len(values):\n",
        "                                pass  # Success - using decoded time coordinate\n",
        "                        except:\n",
        "                            # If decoding fails, try manual conversion\n",
        "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
        "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
        "                                base_date = pd.to_datetime(base_date_str)\n",
        "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
        "                                if len(time_values) != len(values):\n",
        "                                    time_values = None\n",
        "                except Exception as e:\n",
        "                    pass  # Fall back to other methods\n",
        "            \n",
        "            # Method 2: Extract year from filename and create date range\n",
        "            # This method automatically handles leap years (366 days) correctly\n",
        "            if time_values is None:\n",
        "                year = None\n",
        "                filename = os.path.basename(nc_file)\n",
        "                all_years = re.findall(r'\\d{4}', filename)\n",
        "                for year_str in all_years:\n",
        "                    year_candidate = int(year_str)\n",
        "                    if 2000 <= year_candidate <= 2100:\n",
        "                        year = year_candidate\n",
        "                        break\n",
        "                \n",
        "                if year:\n",
        "                    # Create dates based on ACTUAL data length\n",
        "                    # pd.date_range with freq='D' automatically handles leap years\n",
        "                    # For leap years (e.g., 2024, 2028), it will include Feb 29 (366 days)\n",
        "                    # For non-leap years, it will have 365 days\n",
        "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
        "                else:\n",
        "                    # Fallback: use 2035 as default (start of typical CMIP6 data range)\n",
        "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
        "            \n",
        "            # Ensure we have the correct number of dates matching the data\n",
        "            # This handles edge cases where time coordinate might not match exactly\n",
        "            if len(time_values) != len(values):\n",
        "                if len(time_values) > len(values):\n",
        "                    time_values = time_values[:len(values)]\n",
        "                else:\n",
        "                    # Extend if needed (shouldn't happen normally, but handle it)\n",
        "                    additional_days = len(values) - len(time_values)\n",
        "                    last_date = time_values[-1]\n",
        "                    additional_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=additional_days, freq='D')\n",
        "                    time_values = pd.concat([pd.Series(time_values), pd.Series(additional_dates)]).values\n",
        "            \n",
        "            # Create DataFrame for this file\n",
        "            # Use actual data length to ensure all days are included (365 or 366 for leap years)\n",
        "            if len(values) > 0:\n",
        "                df_file = pd.DataFrame({\n",
        "                    'date': time_values[:len(values)],\n",
        "                    'value': values\n",
        "                })\n",
        "                all_data.append(df_file)\n",
        "            \n",
        "            ds.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(all_data) == 0:\n",
        "        print(f\"  ERROR: No data extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Combine all data\n",
        "    print(f\"  Combining data from {len(all_data)} files...\")\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Remove duplicate dates (keep first occurrence)\n",
        "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
        "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
        "    \n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: MET Conversion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_tav_amp(df):\n",
        "    \"\"\"\n",
        "    Calculate annual average temperature (tav) and annual amplitude (amp).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with 'date' as index and 'maxt' and 'mint' columns\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (tav, amp)\n",
        "        tav: Annual average ambient temperature\n",
        "        amp: Annual amplitude in mean monthly temperature\n",
        "    \"\"\"\n",
        "    # Calculate daily mean temperature\n",
        "    df = df.copy()\n",
        "    df['tmean'] = (df['maxt'] + df['mint']) / 2.0\n",
        "    \n",
        "    # Calculate monthly means\n",
        "    df['year'] = df.index.year\n",
        "    df['month'] = df.index.month\n",
        "    monthly_means = df.groupby(['year', 'month'])['tmean'].mean()\n",
        "    \n",
        "    # Calculate overall annual average (tav)\n",
        "    tav = df['tmean'].mean()\n",
        "    \n",
        "    # Calculate annual amplitude (amp)\n",
        "    # Average of all January means minus average of all July means, divided by 2\n",
        "    jan_means = monthly_means[monthly_means.index.get_level_values('month') == 1].mean()\n",
        "    jul_means = monthly_means[monthly_means.index.get_level_values('month') == 7].mean()\n",
        "    amp = (jan_means - jul_means) / 2.0\n",
        "    \n",
        "    return tav, amp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_met_file(tasmax_df, tasmin_df, pr_df, rsds_df, scenario=None, \n",
        "                    output_dir=None, latitude=None, longitude=None, model=None, \n",
        "                    start_year=None, end_year=None):\n",
        "    \"\"\"\n",
        "    Create MET format file from tasmax, tasmin, pr, rsds DataFrames.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    tasmax_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for maximum temperature\n",
        "    tasmin_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for minimum temperature\n",
        "    pr_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for precipitation\n",
        "    rsds_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for surface downwelling shortwave radiation (W/m²) - REQUIRED\n",
        "    scenario : str\n",
        "        Scenario name (e.g., SSP585 or SSP245)\n",
        "    output_dir : str\n",
        "        Output directory path\n",
        "    latitude : float\n",
        "        Latitude in decimal degrees\n",
        "    longitude : float\n",
        "        Longitude in decimal degrees\n",
        "    model : str\n",
        "        Model name (e.g., \"ACCESS CM2\")\n",
        "    start_year : int, optional\n",
        "        Start year for MET file. Only data from this year onwards will be included.\n",
        "        If None, uses the minimum date from the data.\n",
        "    end_year : int, optional\n",
        "        End year for MET file. Only data up to this year will be included.\n",
        "        If None, uses the maximum date from the data.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (tav, amp, num_rows, final_date_range)\n",
        "        tav: Annual average ambient temperature\n",
        "        amp: Annual amplitude in mean monthly temperature\n",
        "        num_rows: Number of rows in the MET file\n",
        "        final_date_range: Dict with 'start' and 'end' dates from the final data\n",
        "    \"\"\"\n",
        "    # Merge all dataframes on date\n",
        "    merged = tasmax_df.copy()\n",
        "    merged = merged.rename(columns={'value': 'maxt'})\n",
        "    merged['date'] = pd.to_datetime(merged['date'])\n",
        "    \n",
        "    # Merge tasmin\n",
        "    tasmin_df['date'] = pd.to_datetime(tasmin_df['date'])\n",
        "    merged = merged.merge(tasmin_df[['date', 'value']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value': 'mint'})\n",
        "    \n",
        "    # Merge pr (precipitation/rain)\n",
        "    pr_df['date'] = pd.to_datetime(pr_df['date'])\n",
        "    merged = merged.merge(pr_df[['date', 'value']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value': 'rain'})\n",
        "    \n",
        "    # Merge rsds (radiation) - REQUIRED\n",
        "    # rsds is in W/m², convert to MJ/m² by multiplying by 0.0864 (seconds per day / 1e6)\n",
        "    if rsds_df is None or len(rsds_df) == 0:\n",
        "        raise ValueError(\"rsds_df is required but is None or empty\")\n",
        "    \n",
        "    rsds_df['date'] = pd.to_datetime(rsds_df['date'])\n",
        "    rsds_df['date'] = rsds_df['date'].dt.normalize()  # Remove time component for proper date matching\n",
        "    # Convert W/m² to MJ/m² (multiply by seconds per day / 1e6)\n",
        "    rsds_df['value_mj'] = rsds_df['value'] * 0.0864\n",
        "    # Debug: Check rsds data before merge\n",
        "    print(f\"  [DEBUG] rsds_df: {len(rsds_df)} rows, value range: {rsds_df['value'].min():.2f} to {rsds_df['value'].max():.2f} W/m²\")\n",
        "    print(f\"  [DEBUG] After conversion: value_mj range: {rsds_df['value_mj'].min():.2f} to {rsds_df['value_mj'].max():.2f} MJ/m²\")\n",
        "    # Normalize merged dates before merge (in case they have time components)\n",
        "    merged['date'] = merged['date'].dt.normalize()\n",
        "    # Debug: Check date ranges before merge\n",
        "    print(f\"  [DEBUG] merged date range: {merged['date'].min()} to {merged['date'].max()}\")\n",
        "    print(f\"  [DEBUG] rsds_df date range: {rsds_df['date'].min()} to {rsds_df['date'].max()}\")\n",
        "    print(f\"  [DEBUG] rsds_df sample values (first 5): {rsds_df[['date', 'value_mj']].head().to_dict('records')}\")\n",
        "    merged = merged.merge(rsds_df[['date', 'value_mj']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value_mj': 'radn'})\n",
        "    # Debug: Check radn after merge\n",
        "    radn_nonzero = (merged['radn'].notna() & (merged['radn'] != 0)).sum()\n",
        "    radn_total = merged['radn'].notna().sum()\n",
        "    radn_zeros = (merged['radn'] == 0).sum()\n",
        "    radn_nan = merged['radn'].isna().sum()\n",
        "    print(f\"  [DEBUG] After merge: {radn_nonzero} non-zero, {radn_zeros} zeros, {radn_nan} NaN out of {len(merged)} total rows\")\n",
        "    if radn_nonzero == 0 and radn_total > 0:\n",
        "        print(f\"  [WARNING] All radn values are zero after merge! Checking date overlap...\")\n",
        "        merged_dates = set(merged['date'].dt.date)\n",
        "        rsds_dates = set(rsds_df['date'].dt.date)\n",
        "        matching_dates = merged_dates.intersection(rsds_dates)\n",
        "        print(f\"  [WARNING] Date overlap: {len(matching_dates)} matching dates out of {len(merged_dates)} merged dates and {len(rsds_dates)} rsds dates\")\n",
        "        if len(matching_dates) == 0:\n",
        "            print(f\"  [ERROR] NO DATE OVERLAP! Dates don't match between merged and rsds dataframes!\")\n",
        "    \n",
        "    # vp and evap are left blank, code is hardcoded to '222222'\n",
        "    merged['vp'] = ''\n",
        "    merged['evap'] = ''\n",
        "    merged['code'] = '222222'  # Hardcoded code value for all rows\n",
        "    \n",
        "    # Sort by date\n",
        "    merged = merged.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Filter data to only include dates from start_year onwards and up to end_year\n",
        "    if start_year is not None:\n",
        "        start_date = pd.Timestamp(year=start_year, month=1, day=1)\n",
        "        merged = merged[merged['date'] >= start_date].copy()\n",
        "    \n",
        "    if end_year is not None:\n",
        "        end_date = pd.Timestamp(year=end_year, month=12, day=31)\n",
        "        merged = merged[merged['date'] <= end_date].copy()\n",
        "    \n",
        "    if len(merged) == 0:\n",
        "        raise ValueError(\"No data remaining after filtering! Check START_YEAR and END_YEAR settings.\")\n",
        "    \n",
        "    # Normalize dates to remove time components\n",
        "    merged['date'] = pd.to_datetime(merged['date']).dt.normalize()\n",
        "    \n",
        "    # Remove duplicate dates (keep first occurrence)\n",
        "    merged = merged.drop_duplicates(subset='date', keep='first').reset_index(drop=True)\n",
        "    \n",
        "    # Get actual min and max dates from filtered data\n",
        "    actual_min_date = merged['date'].min()\n",
        "    actual_max_date = merged['date'].max()\n",
        "    \n",
        "    # Use the actual data range, rounded to full years\n",
        "    min_date = pd.Timestamp(year=actual_min_date.year, month=1, day=1)\n",
        "    max_date = pd.Timestamp(year=actual_max_date.year, month=12, day=31)\n",
        "    \n",
        "    # Create complete date range (includes all days, including day 366 for leap years)\n",
        "    complete_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "    complete_date_range = pd.to_datetime(complete_date_range).normalize()\n",
        "    \n",
        "    # Set date as index for reindexing\n",
        "    merged = merged.set_index('date')\n",
        "    \n",
        "    # Reindex to include all days in the complete range\n",
        "    merged = merged.reindex(complete_date_range)\n",
        "    \n",
        "    # Fill missing values for numeric columns using forward fill then backward fill\n",
        "    numeric_cols = ['maxt', 'mint', 'rain', 'radn']\n",
        "    for col in numeric_cols:\n",
        "        if col in merged.columns:\n",
        "            merged[col] = merged[col].ffill().bfill()\n",
        "            if merged[col].notna().any():\n",
        "                mean_val = merged[col].mean()\n",
        "                if pd.notna(mean_val):\n",
        "                    merged[col] = merged[col].fillna(mean_val)\n",
        "                else:\n",
        "                    merged[col] = merged[col].fillna(0.0)\n",
        "            else:\n",
        "                merged[col] = merged[col].fillna(0.0)\n",
        "    \n",
        "    # Handle vp, evap - ensure they remain blank (empty string)\n",
        "    for col in ['vp', 'evap']:\n",
        "        if col in merged.columns:\n",
        "            merged[col] = merged[col].fillna('')\n",
        "    \n",
        "    # Handle code - ensure it's hardcoded to '222222'\n",
        "    if 'code' in merged.columns:\n",
        "        merged['code'] = '222222'\n",
        "    \n",
        "    # Reset index to get date back as a column\n",
        "    merged = merged.reset_index()\n",
        "    merged = merged.rename(columns={'index': 'date'})\n",
        "    \n",
        "    # Calculate tav and amp\n",
        "    merged_temp = merged[['date', 'maxt', 'mint']].copy()\n",
        "    merged_temp = merged_temp.set_index('date')\n",
        "    merged_temp.index = pd.to_datetime(merged_temp.index)\n",
        "    tav, amp = calculate_tav_amp(merged_temp)\n",
        "    \n",
        "    # Create year and day columns\n",
        "    merged['year'] = merged['date'].dt.year\n",
        "    merged['day'] = merged['date'].dt.dayofyear\n",
        "    \n",
        "    met_data = merged[['year', 'day', 'radn', 'maxt', 'mint', 'rain', 'evap', 'vp', 'code']].copy()\n",
        "    \n",
        "    # Prepare header\n",
        "    current_date = datetime.now().strftime('%Y%m%d')\n",
        "    model_scenario = f\"{model.replace(' ', '_')}_{scenario}\" if model and scenario else \"CMIP6\"\n",
        "    \n",
        "    header = f\"\"\"[weather.met.weather]\n",
        "!Your Ref:  \"\n",
        "latitude = {latitude:.2f}  (DECIMAL DEGREES)\n",
        "longitude =  {longitude:.2f}  (DECIMAL DEGREES)\n",
        "tav = {tav:.2f} (oC) ! Annual average ambient temperature.\n",
        "amp = {amp:.2f} (oC) ! Annual amplitude in mean monthly temperature.\n",
        "!Data Extracted from CMIP6 {model} {scenario} dataset on {current_date} for APSIM\n",
        "!As evaporation is read at 9am, it has been shifted to day before\n",
        "!ie The evaporation measured on 20 April is in row for 19 April\n",
        "!The 6 digit code indicates the source of the 6 data columns\n",
        "!0 actual observation, 1 actual observation composite station\n",
        "!2 interpolated from daily observations\n",
        "!3 interpolated from daily observations using anomaly interpolation method for CLIMARC data\n",
        "!6 synthetic pan\n",
        "!7 interpolated long term averages\n",
        "!more detailed two digit codes are available in SILO's 'Standard' format files\n",
        "!\n",
        "!For further information see the documentation on the datadrill\n",
        "!  http://www.longpaddock.qld.gov.au/silo\n",
        "!\n",
        "year  day radn  maxt   mint  rain  evap    vp   code\n",
        " ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\n",
        "\"\"\"\n",
        "    \n",
        "    # Create output filename\n",
        "    lat_str = f\"{latitude:.2f}\"\n",
        "    lon_str = f\"{longitude:.2f}\"\n",
        "    output_filename = f\"{model_scenario}_{lat_str}_{lon_str}.met\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    \n",
        "    # Write MET file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(header)\n",
        "        # Write data rows\n",
        "        for _, row in met_data.iterrows():\n",
        "            # Format numbers with proper spacing\n",
        "            radn_val = row['radn'] if row['radn'] != '' and pd.notna(row['radn']) else ''\n",
        "            evap_val = row['evap'] if row['evap'] != '' and pd.notna(row['evap']) else ''\n",
        "            vp_val = row['vp'] if row['vp'] != '' and pd.notna(row['vp']) else ''\n",
        "            \n",
        "            if radn_val != '':\n",
        "                radn_str = f\"{float(radn_val):6.1f}\"\n",
        "            else:\n",
        "                radn_str = \"      \"  # 6 spaces\n",
        "                \n",
        "            if evap_val != '':\n",
        "                evap_str = f\"{float(evap_val):6.1f}\"\n",
        "            else:\n",
        "                evap_str = \"      \"  # 6 spaces\n",
        "                \n",
        "            if vp_val != '':\n",
        "                vp_str = f\"{float(vp_val):6.1f}\"\n",
        "            else:\n",
        "                vp_str = \"      \"  # 6 spaces\n",
        "                \n",
        "            \n",
        "            # Code is hardcoded to '222222' for all rows\n",
        "            code_str = \"222222\"\n",
        "            \n",
        "            # Handle NaN values for maxt, mint, rain - use 0.0 as default\n",
        "            maxt_val = row['maxt'] if pd.notna(row['maxt']) else 0.0\n",
        "            mint_val = row['mint'] if pd.notna(row['mint']) else 0.0\n",
        "            rain_val = row['rain'] if pd.notna(row['rain']) else 0.0\n",
        "            \n",
        "            # Format with proper column widths\n",
        "            line = f\"{int(row['year']):4d} {int(row['day']):4d} {radn_str} {maxt_val:6.1f} {mint_val:6.1f} {rain_val:6.1f} {evap_str} {vp_str} {code_str}\\n\"\n",
        "            f.write(line)\n",
        "    \n",
        "    print(f\"  [OK] Created MET file: {output_filename}\")\n",
        "    \n",
        "    # Also create CSV version\n",
        "    csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}.csv\"\n",
        "    csv_path = os.path.join(output_dir, csv_filename)\n",
        "    \n",
        "    # Write CSV (without header comments, just data)\n",
        "    met_data.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.1f')\n",
        "    print(f\"  [OK] Created CSV file: {csv_filename}\")\n",
        "    \n",
        "    # Get date range\n",
        "    if 'date' in merged.columns and len(merged) > 0:\n",
        "        final_date_range = {\n",
        "            'start': merged['date'].min(),\n",
        "            'end': merged['date'].max()\n",
        "        }\n",
        "    else:\n",
        "        final_date_range = {'start': None, 'end': None}\n",
        "    \n",
        "    return tav, amp, len(met_data), final_date_range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Main Processing\n",
        "\n",
        "This section loads CSV files for all variables and creates the MET file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.591211Z",
          "iopub.status.busy": "2025-12-21T10:28:42.591065Z",
          "iopub.status.idle": "2025-12-21T10:28:42.597501Z",
          "shell.execute_reply": "2025-12-21T10:28:42.597053Z"
        }
      },
      "outputs": [],
      "source": [
        "def process_coordinate(model, scenario, latitude, longitude, variables, cmip6_base_dir, output_dir, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Main processing function for user-provided coordinate.\n",
        "    Extract all variables from NC files and convert to MET format.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : str\n",
        "        Model name (e.g., \"ACCESS CM2\")\n",
        "    scenario : str\n",
        "        Scenario name (e.g., \"SSP245\")\n",
        "    latitude : float\n",
        "        Target latitude\n",
        "    longitude : float\n",
        "        Target longitude\n",
        "    variables : list\n",
        "        List of variable names to extract\n",
        "    cmip6_base_dir : str\n",
        "        Base directory containing Model Scenario folders\n",
        "    output_dir : str\n",
        "        Output directory for results\n",
        "    tolerance : float\n",
        "        Coordinate matching tolerance\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict: Summary statistics\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Processing Coordinate: ({latitude:.6f}, {longitude:.6f})\")\n",
        "    print(f\"Model: {model}, Scenario: {scenario}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Construct data directory path\n",
        "    data_dir = os.path.join(cmip6_base_dir, f\"{model} {scenario}\")\n",
        "    \n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"ERROR: Data directory not found: {data_dir}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\nData directory: {data_dir}\")\n",
        "    \n",
        "    \n",
        "    # Prepare coordinate strings for cache file naming\n",
        "    lat_str = f\"{latitude:.2f}\"\n",
        "    lon_str = f\"{longitude:.2f}\"\n",
        "    model_scenario = f\"{model.replace(' ', '_')}_{scenario}\"\n",
        "    \n",
        "    # Extract data for all variables (check cache first)\n",
        "    extracted_data = {}\n",
        "    \n",
        "    for variable in variables:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Processing variable: {variable}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        # Check if cached CSV file exists\n",
        "        cache_path = get_cached_variable_path(output_dir, model_scenario, lat_str, lon_str, variable)\n",
        "        df = load_cached_variable(cache_path)\n",
        "        \n",
        "        # If cache doesn't exist or is invalid, extract from NetCDF\n",
        "        if df is None:\n",
        "            # Extract data from NetCDF files\n",
        "            df = extract_daily_data_from_netcdf(\n",
        "                data_dir, \n",
        "                variable, \n",
        "                latitude, \n",
        "                longitude, \n",
        "                tolerance=tolerance\n",
        "            )\n",
        "            \n",
        "            # Save to cache if extraction was successful\n",
        "            if df is not None and len(df) > 0:\n",
        "                save_cached_variable(df, cache_path)\n",
        "        \n",
        "        # Add to extracted_data if we have valid data\n",
        "        if df is not None and len(df) > 0:\n",
        "            extracted_data[variable] = df\n",
        "        else:\n",
        "            print(f\"  WARNING: No data available for {variable}\")\n",
        "    # Check if required variables are available for MET conversion\n",
        "    # Note: rsds is now mandatory (required for radn in MET format)\n",
        "    required_vars = ['tasmax', 'tasmin', 'pr', 'rsds']\n",
        "    missing_vars = [v for v in required_vars if v not in extracted_data]\n",
        "    \n",
        "    if missing_vars:\n",
        "        print(f\"\\nERROR: Missing required variables for MET conversion: {missing_vars}\")\n",
        "        return None\n",
        "    \n",
        "    # Create MET file\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Creating MET file...\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Get required variables\n",
        "    tasmax_df = extracted_data['tasmax']\n",
        "    tasmin_df = extracted_data['tasmin']\n",
        "    pr_df = extracted_data['pr']\n",
        "    \n",
        "    # Get rsds variable for MET format (now mandatory)\n",
        "    rsds_df = extracted_data.get('rsds', None)\n",
        "    if rsds_df is None:\n",
        "        print(f\"  ERROR: rsds is required but was not extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Note: code is hardcoded to '222222', vp and evap are left blank\n",
        "    \n",
        "    tav, amp, num_rows, final_date_range = create_met_file(\n",
        "        tasmax_df=tasmax_df,\n",
        "        tasmin_df=tasmin_df,\n",
        "        pr_df=pr_df,\n",
        "        rsds_df=rsds_df,\n",
        "        scenario=scenario,\n",
        "        output_dir=output_dir,\n",
        "        latitude=latitude,\n",
        "        longitude=longitude,\n",
        "        model=model,\n",
        "        start_year=START_YEAR,\n",
        "        end_year=END_YEAR\n",
        "    )\n",
        "    \n",
        "    # Summary\n",
        "    summary = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'model': model,\n",
        "        'scenario': scenario,\n",
        "        'variables_extracted': list(extracted_data.keys()),\n",
        "        'num_variables': len(extracted_data),\n",
        "        'tav': tav,\n",
        "        'amp': amp,\n",
        "        'num_rows': num_rows,\n",
        "        'date_range': final_date_range if final_date_range['start'] is not None else {\n",
        "            'start': tasmax_df['date'].min(),\n",
        "            'end': tasmax_df['date'].max()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Processing Summary\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Variables extracted: {len(extracted_data)}\")\n",
        "    print(f\"    - {', '.join(extracted_data.keys())}\")\n",
        "    print(f\"  MET file rows: {num_rows}\")\n",
        "    print(f\"  Date range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
        "    print(f\"  tav (annual average temp): {tav:.2f} °C\")\n",
        "    print(f\"  amp (annual amplitude): {amp:.2f} °C\")\n",
        "    print(f\"  Output directory: {output_dir}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Execute Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING PROCESSING\n",
            "======================================================================\n",
            "Model: ACCESS CM2\n",
            "Scenario: obs\n",
            "Coordinates: (-31.450000, 117.550000)\n",
            "Variables to process: 4 (tasmax, tasmin, pr, rsds)\n",
            "CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing Coordinate: (-31.450000, 117.550000)\n",
            "Model: ACCESS CM2, Scenario: obs\n",
            "======================================================================\n",
            "\n",
            "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 obs\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmax\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_obs_-31.45_117.55_tasmax.csv\n",
            "  [INFO] Cache file found and valid - loaded 10,957 records\n",
            "  [INFO] Date range: 1985-01-01 09:00:00 to 2014-12-31 09:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmin\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_obs_-31.45_117.55_tasmin.csv\n",
            "  [INFO] Cache file found and valid - loaded 10,957 records\n",
            "  [INFO] Date range: 1985-01-01 09:00:00 to 2014-12-31 09:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: pr\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_obs_-31.45_117.55_pr.csv\n",
            "  [INFO] Cache file found and valid - loaded 10,957 records\n",
            "  [INFO] Date range: 1985-01-01 09:00:00 to 2014-12-31 09:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: rsds\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_obs_-31.45_117.55_rsds.csv\n",
            "  [INFO] Cache file found and valid - loaded 10,957 records\n",
            "  [INFO] Date range: 1985-01-01 12:00:00 to 2014-12-31 12:00:00\n",
            "\n",
            "======================================================================\n",
            "Creating MET file...\n",
            "======================================================================\n",
            "  [DEBUG] rsds_df: 10957 rows, value range: 15.53 to 403.05 W/m²\n",
            "  [DEBUG] After conversion: value_mj range: 1.34 to 34.82 MJ/m²\n",
            "  [DEBUG] merged date range: 1985-01-01 00:00:00 to 2014-12-31 00:00:00\n",
            "  [DEBUG] rsds_df date range: 1985-01-01 00:00:00 to 2014-12-31 00:00:00\n",
            "  [DEBUG] rsds_df sample values (first 5): [{'date': Timestamp('1985-01-01 00:00:00'), 'value_mj': 34.0659685152}, {'date': Timestamp('1985-01-02 00:00:00'), 'value_mj': 34.011137952}, {'date': Timestamp('1985-01-03 00:00:00'), 'value_mj': 33.7434820704}, {'date': Timestamp('1985-01-04 00:00:00'), 'value_mj': 33.1887111264}, {'date': Timestamp('1985-01-05 00:00:00'), 'value_mj': 30.572743334400002}]\n",
            "  [DEBUG] After merge: 10957 non-zero, 0 zeros, 0 NaN out of 10957 total rows\n",
            "  [OK] Created MET file: ACCESS_CM2_obs_-31.45_117.55.met\n",
            "  [OK] Created CSV file: ACCESS_CM2_obs_-31.45_117.55.csv\n",
            "\n",
            "======================================================================\n",
            "Processing Summary\n",
            "======================================================================\n",
            "  Variables extracted: 4\n",
            "    - tasmax, tasmin, pr, rsds\n",
            "  MET file rows: 10592\n",
            "  Date range: 1986-01-01 00:00:00 to 2014-12-31 00:00:00\n",
            "  tav (annual average temp): 18.34 °C\n",
            "  amp (annual amplitude): 7.36 °C\n",
            "  Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "✓ PROCESSING COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Output files created:\n",
            "  - MET file: ACCESS_CM2_obs_-31.45_117.55.met\n",
            "  - CSV file: ACCESS_CM2_obs_-31.45_117.55.csv\n",
            "  - Individual variable CSV cache files: 4 files\n",
            "\n",
            "All files saved to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n"
          ]
        }
      ],
      "source": [
        "# Execute main processing\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING PROCESSING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {MODEL}\")\n",
        "print(f\"Scenario: {SCENARIO}\")\n",
        "print(f\"Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
        "print(f\"Variables to process: {len(VARIABLES)} ({', '.join(VARIABLES)})\")\n",
        "print(f\"CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "summary = process_coordinate(\n",
        "    model=MODEL,\n",
        "    scenario=SCENARIO,\n",
        "    latitude=LATITUDE,\n",
        "    longitude=LONGITUDE,\n",
        "    variables=VARIABLES,\n",
        "    cmip6_base_dir=CMIP6_BASE_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    tolerance=COORD_TOLERANCE\n",
        ")\n",
        "\n",
        "if summary:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ PROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nOutput files created:\")\n",
        "    print(f\"  - MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
        "    print(f\"  - CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
        "    print(f\"  - Individual variable CSV cache files: {len(summary.get('variables_extracted', []))} files\")\n",
        "    print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✗ PROCESSING FAILED\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Please check error messages above.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
