{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMIP6 VP+Evap Calibration Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs monthly mean/variance calibration of VP and Evap across multiple models, scenarios, and coordinates, then generates calibrated APSIM .met files matching the template format.\n",
    "\n",
    "## Calculation Process\n",
    "\n",
    "### Step 1: Parametrization (Historical Baseline Calibration)\n",
    "\n",
    "**Data Requirements:**\n",
    "- **SILO comparison file (1986-2014):** Extracted using SILO API Module_Workflow (silo_met_file_creation.py)\n",
    "  - Period 1986-2014: Used for VP and EVAP calibration (CMIP6 obs data only covers 1985-2014)\n",
    "  - Period 1995-2024: Used for trends analysis (report storyline of last 30 years) - *not used in this calibration step*\n",
    "- **CMIP6 obs data (1985-2014):** Historical baseline period for calibration parametrization\n",
    "\n",
    "**For each model–coordinate combination:**\n",
    "- Daily time series are aligned for the calibration period (1986-2014)\n",
    "- **VP calibration:**\n",
    "  - SILO observed VP (hPa) - target series\n",
    "  - CMIP6-derived VP proxy (daily-mean VP, hPa) - source series\n",
    "- **EVAP calibration:**\n",
    "  - SILO observed pan evaporation (mm/day) - target series\n",
    "  - CMIP6-derived ET₀ (FAO-56 ET₀) proxy (mm/day) - source series\n",
    "\n",
    "**Monthly Bias Correction Framework:**\n",
    "Bias correction is applied by calendar month to respect seasonal structure. For each month m (1-12), separately for VP and EVAP:\n",
    "\n",
    "1. **Compute statistics:**\n",
    "   - Mean and standard deviation for SILO (target): μₛ(m), σₛ(m)\n",
    "   - Mean and standard deviation for CMIP6 obs (source): μ꜀(m), σ꜀(m)\n",
    "\n",
    "2. **Apply mean–variance bias correction (standardisation + rescaling):**\n",
    "   For each CMIP6 value x in month m, the calibrated value x′ is computed as:\n",
    "   ```\n",
    "   x′ = μₛ(m) + (x − μ꜀(m)) × (σₛ(m) / σ꜀(m))\n",
    "   ```\n",
    "   Where:\n",
    "   - μₛ(m) = SILO mean for month m (target)\n",
    "   - μ꜀(m) = CMIP6 obs mean for month m (source)\n",
    "   - σₛ(m) = SILO standard deviation for month m (target)\n",
    "   - σ꜀(m) = CMIP6 obs standard deviation for month m (source)\n",
    "   \n",
    "   **Special cases:**\n",
    "   - If σ꜀(m) ≈ 0, apply mean shift only: `x′ = x + (μₛ(m) − μ꜀(m))`\n",
    "   - Enforce physical bounds: VP ≥ 0, EVAP ≥ 0\n",
    "\n",
    "3. **Calibration period:** 1986-2014 (historical overlap period where both SILO and CMIP6 obs data are available)\n",
    "\n",
    "**Note:** This calibration period (1986-2014) is used only for calibration parametrization, not for trend reporting.\n",
    "\n",
    "### Step 2: Calibration of Future Scenarios\n",
    "\n",
    "**Apply calibration to all CMIP6 time periods:** The fixed monthly bias-correction relationship—derived once from 1986–2014—is applied to every CMIP6 value outside the calibration period, without recomputing or updating any statistics.\n",
    "\n",
    "**Calibration targets:**\n",
    "- `ACCESS_CM2_obs` (QC / verification) - applies calibration to verify the correction works on the baseline period\n",
    "- `ACCESS_CM2_SSP245` - future scenario\n",
    "- `ACCESS_CM2_SSP585` - future scenario\n",
    "\n",
    "**Calibration parameters (calculated once in Step 1):**\n",
    "From the 1986-2014 baseline period, for each month m (1-12), separately for VP and EVAP:\n",
    "- μ꜀(m) — CMIP6 baseline mean\n",
    "- σ꜀(m) — CMIP6 baseline std\n",
    "- μₛ(m) — SILO baseline mean\n",
    "- σₛ(m) — SILO baseline std\n",
    "\n",
    "**Application process:**\n",
    "For every CMIP6 (modelled) day, including future periods:\n",
    "\n",
    "1. **Identify the calendar month m** of the current day\n",
    "2. **Take the raw VP or EVAP CMIP6 value x**\n",
    "3. **Apply the already-derived transformation:**\n",
    "   ```\n",
    "   x′ = μₛ(m) + (x − μ꜀(m)) × (σₛ(m) / σ꜀(m))\n",
    "   ```\n",
    "   Where:\n",
    "   - x = raw CMIP6 value for the day\n",
    "   - μₛ(m) = SILO baseline mean for month m (target anchor)\n",
    "   - μ꜀(m) = CMIP6 baseline mean for month m (source reference)\n",
    "   - σₛ(m) = SILO baseline std for month m (target scale)\n",
    "   - σ꜀(m) = CMIP6 baseline std for month m (source scale)\n",
    "   \n",
    "   **Special cases:**\n",
    "   - If σ꜀(m) ≈ 0, apply mean shift only: `x′ = x + (μₛ(m) − μ꜀(m))`\n",
    "   - Enforce physical bounds: VP ≥ 0, EVAP ≥ 0\n",
    "\n",
    "**Key properties:**\n",
    "- The baseline anchor is SILO (μₛ(m))\n",
    "- The anomaly is preserved and only rescaled\n",
    "- The trend embedded in (x − μ꜀(m)) survives intact\n",
    "\n",
    "**Important:** The bias-correction parameters derived over the historical overlap period (1986–2014) are held fixed and applied to all CMIP6 time periods, including future SSP projections. Each daily CMIP6 value is transformed using the month-specific correction relationship, ensuring consistency with SILO climatology while preserving the magnitude and temporal structure of projected anomalies.\n",
    "\n",
    "## Process Flow\n",
    "\n",
    "1. Uses configured model/scenario/coordinates from configuration section (specified per run)\n",
    "2. Loads SILO baseline (1986-2014) and CMIP6 obs .met files + VP/Evap proxy CSVs\n",
    "3. Applies monthly mean/variance scaling calibration (Step 1: parametrization on 1986-2014 baseline)\n",
    "4. Applies calibration to future scenarios (Step 2: uses parametrization from Step 1)\n",
    "5. Generates calibrated .met files matching template format\n",
    "6. Produces diagnostics and calibration reports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Configuration\n",
    "\n",
    "**IMPORTANT:** Adjust the values below for each run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION\n",
      "======================================================================\n",
      "  Model: ACCESS_CM2\n",
      "  Scenarios: ['obs', 'ssp245', 'ssp585']\n",
      "  Coordinates: [(-31.45, 117.55)]\n",
      "  Input MET Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.45_117.55_Climate Files\n",
      "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.45_117.55_Climate Files\n",
      "  Template MET File: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\APSIM met file structure.met\n",
      "  Baseline Period: 1986-01-01 to 2014-12-31\n",
      "======================================================================\n",
      "\n",
      "All paths and filenames will automatically use the above settings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION - CHANGE VALUES BELOW AS NEEDED\n",
    "# ============================================================================\n",
    "# All other settings will automatically adjust based on these values\n",
    "\n",
    "# Input directories\n",
    "INPUT_MET_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.45_117.55_Climate Files\"  # Directory containing raw .met files (SILO + CMIP6 obs + CMIP6 futures) and VP/Evap proxy CSVs\n",
    "\n",
    "# Output directory (adjust per run)\n",
    "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.45_117.55_Climate Files\"  # Directory where calibrated .met files and reports will be saved\n",
    "\n",
    "# Template .met file path\n",
    "TEMPLATE_MET_FILE = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\APSIM met file structure.met\"\n",
    "\n",
    "# Baseline period (fixed)\n",
    "BASELINE_START = \"1986-01-01\"\n",
    "BASELINE_END = \"2014-12-31\"\n",
    "\n",
    "MODEL = \"ACCESS_CM2\"\n",
    "SCENARIOS = [\"obs\", \"ssp245\", \"ssp585\"]  # Process obs, SSP245, and SSP585\n",
    "# Note: The notebook will process all scenarios (obs, SSP245, SSP585) automatically\n",
    "\n",
    "COORDINATES = [\n",
    "    (-31.45, 117.55),  # Adjust per run - can include multiple coordinates\n",
    "\n",
    "]\n",
    "\n",
    "# ============================================================================\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Scenarios: {SCENARIOS}\")\n",
    "print(f\"  Coordinates: {COORDINATES}\")\n",
    "print(f\"  Input MET Directory: {INPUT_MET_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Template MET File: {TEMPLATE_MET_FILE}\")\n",
    "print(f\"  Baseline Period: {BASELINE_START} to {BASELINE_END}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll paths and filenames will automatically use the above settings.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Target Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 target(s) for processing\n"
     ]
    }
   ],
   "source": [
    "def generate_targets(model, scenarios, coordinates):\n",
    "    \"\"\"\n",
    "    Generate targets for the configured model/scenario across coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name (must use underscores, e.g., \"ACCESS_CM2\")\n",
    "    scenarios : list\n",
    "        List of scenario names (e.g., [\"ssp245\", \"ssp585\"])\n",
    "        Scenario name\n",
    "    coordinates : list\n",
    "        List of (latitude, longitude) tuples\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of target dictionaries: [{\"model\": str, \"scenario\": str, \"lat\": float, \"lon\": float}, ...]\n",
    "    \"\"\"\n",
    "    # Simplified: Use list comprehension instead of nested loops\n",
    "    return [\n",
    "        {\"model\": model, \"scenario\": scenario, \"lat\": lat, \"lon\": lon}\n",
    "        for lat, lon in coordinates\n",
    "        for scenario in scenarios\n",
    "    ]\n",
    "\n",
    "\n",
    "print(f\"Generated {len(generate_targets(MODEL, SCENARIOS, COORDINATES))} target(s) for processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: File Loading Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_met_file(filepath):\n",
    "    \"\"\"\n",
    "    Load and parse .met file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to .met file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (header_dict, data_df)\n",
    "        header_dict: Dictionary with metadata (lat, lon, tav, amp, etc.)\n",
    "        data_df: DataFrame with columns: date, year, day, radn, maxt, mint, rain, evap, vp, code\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"MET file not found: {filepath}\")\n",
    "    \n",
    "    header_dict = {}\n",
    "    data_lines = []\n",
    "    in_header = True\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if in_header:\n",
    "                # Parse header metadata\n",
    "                if line.startswith('latitude'):\n",
    "                    match = re.search(r'latitude\\s*=\\s*([-\\d.]+)', line)\n",
    "                    if match:\n",
    "                        header_dict['latitude'] = float(match.group(1))\n",
    "                elif line.startswith('longitude'):\n",
    "                    match = re.search(r'longitude\\s*=\\s*([-\\d.]+)', line)\n",
    "                    if match:\n",
    "                        header_dict['longitude'] = float(match.group(1))\n",
    "                elif line.startswith('tav'):\n",
    "                    match = re.search(r'tav\\s*=\\s*([-\\d.]+)', line)\n",
    "                    if match:\n",
    "                        header_dict['tav'] = float(match.group(1))\n",
    "                elif line.startswith('amp'):\n",
    "                    match = re.search(r'amp\\s*=\\s*([-\\d.]+)', line)\n",
    "                    if match:\n",
    "                        header_dict['amp'] = float(match.group(1))\n",
    "                \n",
    "                # Check if we've reached the data section\n",
    "                if line.startswith('year') and 'day' in line:\n",
    "                    in_header = False\n",
    "                    continue  # Skip the column header line\n",
    "            else:\n",
    "                # Data section\n",
    "                if line and not line.startswith('!'):\n",
    "                    # Skip unit line like \" ()   () (MJ/m^2) ...\"\n",
    "                    if not re.match(r'^\\s*\\(', line):\n",
    "                        data_lines.append(line)\n",
    "    \n",
    "    # Parse data lines\n",
    "    data_rows = []\n",
    "    for line in data_lines:\n",
    "        # Split by whitespace\n",
    "        parts = line.split()\n",
    "        if len(parts) >= 9:\n",
    "            try:\n",
    "                year = int(parts[0])\n",
    "                day = int(parts[1])\n",
    "                radn = float(parts[2]) if parts[2] else np.nan\n",
    "                maxt = float(parts[3]) if parts[3] else np.nan\n",
    "                mint = float(parts[4]) if parts[4] else np.nan\n",
    "                rain = float(parts[5]) if parts[5] else np.nan\n",
    "                evap = float(parts[6]) if parts[6] else np.nan\n",
    "                vp = float(parts[7]) if parts[7] else np.nan\n",
    "                code = parts[8] if len(parts) > 8 else ''\n",
    "                \n",
    "                # Convert year and day to date\n",
    "                date = pd.Timestamp(year=year, month=1, day=1) + pd.Timedelta(days=day-1)\n",
    "                \n",
    "                data_rows.append({\n",
    "                    'date': date,\n",
    "                    'year': year,\n",
    "                    'day': day,\n",
    "                    'radn': radn,\n",
    "                    'maxt': maxt,\n",
    "                    'mint': mint,\n",
    "                    'rain': rain,\n",
    "                    'evap': evap,\n",
    "                    'vp': vp,\n",
    "                    'code': code\n",
    "                })\n",
    "            except (ValueError, IndexError) as e:\n",
    "                continue  # Skip malformed lines\n",
    "    \n",
    "    data_df = pd.DataFrame(data_rows)\n",
    "    if len(data_df) > 0:\n",
    "        data_df['date'] = pd.to_datetime(data_df['date'])\n",
    "        data_df = data_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    return header_dict, data_df\n",
    "\n",
    "\n",
    "def load_proxy_csv(filepath, variable='vp'):\n",
    "    \"\"\"\n",
    "    Load proxy CSV file (VP or Evap).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to CSV file\n",
    "    variable : str\n",
    "        Variable name ('vp' or 'evap') for logging\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Proxy CSV file not found: {filepath}\")\n",
    "    \n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    # Check required columns\n",
    "    if 'date' not in df.columns or 'value' not in df.columns:\n",
    "        raise ValueError(f\"CSV file missing required columns. Expected 'date' and 'value', got: {list(df.columns)}\")\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    return df[['date', 'value']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_for_target(model, scenario, lat, lon, input_met_dir):\n",
    "    \"\"\"\n",
    "    Find all required files for a target (model, scenario, coordinate).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name (e.g., \"ACCESS_CM2\")\n",
    "    scenario : str\n",
    "        Scenario name (e.g., \"obs\", \"ssp245\", \"ssp585\")\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    input_met_dir : str\n",
    "        Directory containing .met files and VP/Evap proxy CSV files\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with file paths\n",
    "    \"\"\"\n",
    "    files = {\n",
    "        'silo_met': None,\n",
    "        'cmip6_obs_met': None,\n",
    "        'cmip6_future_met': None,\n",
    "        'vp_obs': None,\n",
    "        'vp_future': None,\n",
    "        'evap_obs': None,\n",
    "        'evap_future': None\n",
    "    }\n",
    "    \n",
    "    # Generate coordinate string variants\n",
    "    # Files use both formats: neg31.45 (for CSVs) and -31.45 (for MET files)\n",
    "    lat_str_variants = [\n",
    "        f\"{lat:.2f}\".replace(\"-\", \"neg\"),  # neg31.45 (for VP/Evap CSVs)\n",
    "        f\"{lat:.2f}\",  # -31.45 (for MET files)\n",
    "        f\"{abs(lat):.2f}\".replace(\".\", \"_\")  # 31_45 (underscore format)\n",
    "    ]\n",
    "    lon_str_variants = [\n",
    "        f\"{lon:.2f}\",  # 117.55 (decimal format)\n",
    "        f\"{lon:.2f}\".replace(\".\", \"_\")  # 117_55 (underscore format)\n",
    "    ]\n",
    "    \n",
    "    # Generate scenario name variants (files use uppercase SSP245, SSP585)\n",
    "    scenario_variants = [scenario]\n",
    "    if scenario.lower() != \"obs\":\n",
    "        # Try uppercase version (SSP245, SSP585)\n",
    "        scenario_upper = scenario.upper()\n",
    "        if scenario_upper != scenario:\n",
    "            scenario_variants.append(scenario_upper)\n",
    "        # Try title case (Ssp245, Ssp585)\n",
    "        scenario_title = scenario.title()\n",
    "        if scenario_title != scenario and scenario_title != scenario_upper:\n",
    "            scenario_variants.append(scenario_title)\n",
    "    \n",
    "    # Find SILO baseline .met (try multiple date ranges)\n",
    "    silo_date_ranges = [\"1986-2014\", \"1985-2014\", \"1995-2024\"]  # Try common date ranges\n",
    "    for date_range in silo_date_ranges:\n",
    "        for lat_var in lat_str_variants:\n",
    "            for lon_var in lon_str_variants:\n",
    "                silo_pattern = f\"SILO_{date_range}_{lat_var}_{lon_var}.met\"\n",
    "                silo_path = os.path.join(input_met_dir, silo_pattern)\n",
    "                if os.path.exists(silo_path):\n",
    "                    files['silo_met'] = silo_path\n",
    "                    break\n",
    "            if files['silo_met']:\n",
    "                break\n",
    "        if files['silo_met']:\n",
    "            break\n",
    "    \n",
    "    # Find CMIP6 obs .met\n",
    "    for lat_var in lat_str_variants:\n",
    "        for lon_var in lon_str_variants:\n",
    "            obs_pattern = f\"{model}_obs_{lat_var}_{lon_var}.met\"\n",
    "            obs_path = os.path.join(input_met_dir, obs_pattern)\n",
    "            if os.path.exists(obs_path):\n",
    "                files['cmip6_obs_met'] = obs_path\n",
    "                break\n",
    "        if files['cmip6_obs_met']:\n",
    "            break\n",
    "    \n",
    "    # Find CMIP6 future .met (skip for obs scenario)\n",
    "    if scenario.lower() != \"obs\":\n",
    "        for scenario_var in scenario_variants:\n",
    "            for lat_var in lat_str_variants:\n",
    "                for lon_var in lon_str_variants:\n",
    "                    future_pattern = f\"{model}_{scenario_var}_{lat_var}_{lon_var}.met\"\n",
    "                    future_path = os.path.join(input_met_dir, future_pattern)\n",
    "                    if os.path.exists(future_path):\n",
    "                        files['cmip6_future_met'] = future_path\n",
    "                        break\n",
    "                if files['cmip6_future_met']:\n",
    "                    break\n",
    "            if files['cmip6_future_met']:\n",
    "                break\n",
    "    else:\n",
    "        # For obs scenario, use obs .met as the \"future\" met file\n",
    "        files['cmip6_future_met'] = files['cmip6_obs_met']\n",
    "    \n",
    "    # Find VP proxy CSVs (obs and future)\n",
    "    for lat_var in lat_str_variants:\n",
    "        for lon_var in lon_str_variants:\n",
    "            # VP obs (only with and without .csv extension)\n",
    "            vp_obs_patterns = [\n",
    "                f\"{model}_obs_{lat_var}_{lon_var}_vp.csv\",  # With .csv\n",
    "                f\"{model}_obs_{lat_var}_{lon_var}_vp\"  # Without .csv\n",
    "            ]\n",
    "            for pattern in vp_obs_patterns:\n",
    "                vp_obs_path = os.path.join(input_met_dir, pattern)\n",
    "                if os.path.exists(vp_obs_path):\n",
    "                    files['vp_obs'] = vp_obs_path\n",
    "                    break\n",
    "            \n",
    "            # VP future (skip for obs scenario)\n",
    "            if scenario.lower() != \"obs\":\n",
    "                for scenario_var in scenario_variants:\n",
    "                    vp_future_patterns = [\n",
    "                        f\"{model}_{scenario_var}_{lat_var}_{lon_var}_vp.csv\",  # With .csv\n",
    "                        f\"{model}_{scenario_var}_{lat_var}_{lon_var}_vp\"  # Without .csv\n",
    "                    ]\n",
    "                    for pattern in vp_future_patterns:\n",
    "                        vp_future_path = os.path.join(input_met_dir, pattern)\n",
    "                        if os.path.exists(vp_future_path):\n",
    "                            files['vp_future'] = vp_future_path\n",
    "                            break\n",
    "                    if files['vp_future']:\n",
    "                        break\n",
    "                if files['vp_future']:\n",
    "                    break\n",
    "            else:\n",
    "                # For obs scenario, use obs proxy as future\n",
    "                files['vp_future'] = files['vp_obs']\n",
    "            \n",
    "            # Evap obs (only with and without .csv extension, only eto suffix)\n",
    "            evap_obs_patterns = [\n",
    "                f\"{model}_obs_{lat_var}_{lon_var}_eto.csv\",  # With .csv\n",
    "                f\"{model}_obs_{lat_var}_{lon_var}_eto\"  # Without .csv\n",
    "            ]\n",
    "            for pattern in evap_obs_patterns:\n",
    "                evap_obs_path = os.path.join(input_met_dir, pattern)\n",
    "                if os.path.exists(evap_obs_path):\n",
    "                    files['evap_obs'] = evap_obs_path\n",
    "                    break\n",
    "            \n",
    "            # Evap future (skip for obs scenario, only eto suffix)\n",
    "            if scenario.lower() != \"obs\":\n",
    "                for scenario_var in scenario_variants:\n",
    "                    evap_future_patterns = [\n",
    "                        f\"{model}_{scenario_var}_{lat_var}_{lon_var}_eto.csv\",  # With .csv\n",
    "                        f\"{model}_{scenario_var}_{lat_var}_{lon_var}_eto\"  # Without .csv\n",
    "                    ]\n",
    "                    for pattern in evap_future_patterns:\n",
    "                        evap_future_path = os.path.join(input_met_dir, pattern)\n",
    "                        if os.path.exists(evap_future_path):\n",
    "                            files['evap_future'] = evap_future_path\n",
    "                            break\n",
    "                    if files['evap_future']:\n",
    "                        break\n",
    "                if files['evap_future']:\n",
    "                    break\n",
    "            else:\n",
    "                # For obs scenario, use obs proxy as future\n",
    "                files['evap_future'] = files['evap_obs']\n",
    "            \n",
    "            # If we found all files, break\n",
    "            if files['vp_obs'] and files['vp_future'] and files['evap_obs'] and files['evap_future']:\n",
    "                break\n",
    "        if files['vp_obs'] and files['vp_future'] and files['evap_obs'] and files['evap_future']:\n",
    "            break\n",
    "    \n",
    "    return files\n",
    "\n",
    "\n",
    "def all_required_files_exist(files, scenario):\n",
    "    \"\"\"\n",
    "    Check if all required files exist.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    files : dict\n",
    "        Dictionary with file paths\n",
    "    scenario : str\n",
    "        Scenario name\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if all required files exist, False otherwise\n",
    "    \"\"\"\n",
    "    # Required files depend on scenario\n",
    "    if scenario.lower() == 'obs':\n",
    "        required = ['silo_met', 'cmip6_obs_met', \n",
    "                'vp_obs', 'evap_obs']\n",
    "    else:\n",
    "        required = ['silo_met', 'cmip6_obs_met', 'cmip6_future_met', \n",
    "                    'vp_obs', 'vp_future', 'evap_obs', 'evap_future']\n",
    "    \n",
    "    missing = [key for key in required if files.get(key) is None]\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"  [ERROR] Missing required files:\")\n",
    "        for key in missing:\n",
    "            print(f\"    - {key}\")\n",
    "        return False\n",
    "    \n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Calibration Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_stats(series, dates):\n",
    "    \"\"\"\n",
    "    Calculate monthly mean and standard deviation statistics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series or array-like\n",
    "        Data values\n",
    "    dates : pd.Series or array-like\n",
    "        Corresponding dates\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: month, mean, std\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'date': pd.to_datetime(dates), 'value': series})\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    monthly_stats = df.groupby('month')['value'].agg(['mean', 'std']).reset_index()\n",
    "    monthly_stats.columns = ['month', 'mean', 'std']\n",
    "    \n",
    "    # Fill NaN std with 0 (for months with constant values)\n",
    "    monthly_stats['std'] = monthly_stats['std'].fillna(0.0)\n",
    "    \n",
    "    return monthly_stats\n",
    "\n",
    "\n",
    "def apply_monthly_calibration(source_series, source_dates, target_stats, source_stats):\n",
    "    \"\"\"\n",
    "    Apply monthly mean/variance scaling calibration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    source_series : pd.Series or array-like\n",
    "        Source data values to calibrate\n",
    "    source_dates : pd.Series or array-like\n",
    "        Corresponding dates\n",
    "    target_stats : pd.DataFrame\n",
    "        Target monthly statistics (columns: month, mean, std)\n",
    "    source_stats : pd.DataFrame\n",
    "        Source monthly statistics (columns: month, mean, std)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Calibrated series\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'date': pd.to_datetime(source_dates),\n",
    "        'value': source_series\n",
    "    })\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    # Merge with statistics\n",
    "    # Merge with target statistics and rename columns\n",
    "    df = df.merge(target_stats, on='month', how='left')\n",
    "    df = df.rename(columns={'mean': 'mean_target', 'std': 'std_target'})\n",
    "    \n",
    "    # Merge with source statistics and rename columns\n",
    "    df = df.merge(source_stats, on='month', how='left')\n",
    "    df = df.rename(columns={'mean': 'mean_source', 'std': 'std_source'})\n",
    "    \n",
    "    # Apply calibration (vectorized - no loops!)\n",
    "    # Handle zero variance case: use np.where for conditional logic\n",
    "    sigma_source_safe = df['std_source'].fillna(0.0)\n",
    "    zero_variance_mask = sigma_source_safe < 1e-6\n",
    "    \n",
    "    # Calculate calibrated values\n",
    "    calibrated = np.where(\n",
    "        zero_variance_mask,\n",
    "        df['value'] + (df['mean_target'] - df['mean_source']),  # Mean shift only\n",
    "        df['mean_target'] + (df['value'] - df['mean_source']) * (df['std_target'] / sigma_source_safe)  # Full calibration\n",
    "    )\n",
    "    \n",
    "    # Apply bounds: ensure >= 0\n",
    "    calibrated = np.maximum(0.0, calibrated)\n",
    "    \n",
    "    return pd.Series(calibrated, index=df.index)\n",
    "\n",
    "\n",
    "def calibrate_variable(silo_df, cmip6_obs_df, cmip6_future_df, baseline_start, baseline_end):\n",
    "    \"\"\"\n",
    "    Generic calibration function for VP or Evap using monthly mean/variance scaling.\n",
    "    Simplified: Single function handles both VP and Evap instead of duplicate code.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_df : pd.DataFrame\n",
    "        SILO data (columns: date, value)\n",
    "    cmip6_obs_df : pd.DataFrame\n",
    "        CMIP6 obs proxy (columns: date, value)\n",
    "    cmip6_future_df : pd.DataFrame\n",
    "        CMIP6 future proxy (columns: date, value)\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary with calibrated DataFrames: {'obs': df, 'future': df, 'silo_stats': df, 'cmip6_obs_stats': df}\n",
    "    \"\"\"\n",
    "    baseline_start_dt = pd.to_datetime(baseline_start)\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end)\n",
    "    \n",
    "    # Filter to baseline period\n",
    "    silo_baseline = silo_df[\n",
    "        (silo_df['date'] >= baseline_start_dt) & \n",
    "        (silo_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_baseline = cmip6_obs_df[\n",
    "        (cmip6_obs_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate monthly statistics\n",
    "    silo_stats = calculate_monthly_stats(silo_baseline['value'], silo_baseline['date'])\n",
    "    cmip6_obs_stats = calculate_monthly_stats(cmip6_obs_baseline['value'], cmip6_obs_baseline['date'])\n",
    "    \n",
    "    # Apply calibration to obs and future (simplified: same logic for both)\n",
    "    def apply_calibration(df):\n",
    "        calibrated = apply_monthly_calibration(\n",
    "            df['value'],\n",
    "            df['date'],\n",
    "            silo_stats,\n",
    "            cmip6_obs_stats\n",
    "        )\n",
    "        result_df = df.copy()\n",
    "        result_df['value'] = calibrated.values\n",
    "        return result_df\n",
    "    \n",
    "    return {\n",
    "        'obs': apply_calibration(cmip6_obs_df),\n",
    "        'future': apply_calibration(cmip6_future_df),\n",
    "        'silo_stats': silo_stats,\n",
    "        'cmip6_obs_stats': cmip6_obs_stats\n",
    "    }\n",
    "\n",
    "\n",
    "def calibrate_vp(silo_vp_df, cmip6_obs_vp_df, cmip6_future_vp_df, baseline_start, baseline_end):\n",
    "    \"\"\"Calibrate VP - wrapper for calibrate_variable.\"\"\"\n",
    "    return calibrate_variable(silo_vp_df, cmip6_obs_vp_df, cmip6_future_vp_df, baseline_start, baseline_end)\n",
    "\n",
    "\n",
    "def calibrate_evap(silo_evap_df, cmip6_obs_evap_df, cmip6_future_evap_df, baseline_start, baseline_end):\n",
    "    \"\"\"Calibrate Evap - wrapper for calibrate_variable.\"\"\"\n",
    "    return calibrate_variable(silo_evap_df, cmip6_obs_evap_df, cmip6_future_evap_df, baseline_start, baseline_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Diagnostic Validation (Baseline + Future Checks)\n",
    "\n",
    "This section runs comprehensive diagnostic validation after calibration to ensure:\n",
    "\n",
    "**Baseline Validation (1986-2014):**\n",
    "- Calibration successfully matches SILO mean and variance\n",
    "- Corrected CMIP6 obs closely matches SILO climatology\n",
    "\n",
    "**Future Scenario Checks:**\n",
    "- Signal preservation: calibration doesn't distort trends or variability\n",
    "- Scenario separation: SSP245 and SSP585 maintain expected relationships\n",
    "- Physical bounds: all values are valid and within expected ranges\n",
    "\n",
    "**Note:** This validation is integrated into the main processing loop (Step 8) and runs automatically after calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration Note\n",
    "\n",
    "The diagnostic validation is automatically called in the main processing loop (Step 8). The code below shows the integration point - it's already included in the main loop cell above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tav_amp(df):\n",
    "    \"\"\"\n",
    "    Calculate annual average temperature (tav) and annual amplitude (amp).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with date index and maxt, mint columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (tav, amp)\n",
    "    \"\"\"\n",
    "    # Calculate mean temperature for each day\n",
    "    df = df.copy()\n",
    "    df['tmean'] = (df['maxt'] + df['mint']) / 2.0\n",
    "    \n",
    "    # Group by month and calculate monthly means\n",
    "    df['month'] = df.index.month\n",
    "    monthly_means = df.groupby('month')['tmean'].mean()\n",
    "    \n",
    "    # Calculate annual average (tav)\n",
    "    tav = monthly_means.mean()\n",
    "    \n",
    "    # Calculate annual amplitude (amp)\n",
    "    # amp = (max monthly mean - min monthly mean) / 2\n",
    "    amp = (monthly_means.max() - monthly_means.min()) / 2.0\n",
    "    \n",
    "    return tav, amp\n",
    "\n",
    "\n",
    "def parse_template_met(template_path):\n",
    "    \"\"\"\n",
    "    Parse template .met file to extract header structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    template_path : str\n",
    "        Path to template .met file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Header template string (everything before data section)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(template_path):\n",
    "        raise FileNotFoundError(f\"Template file not found: {template_path}\")\n",
    "    \n",
    "    header_lines = []\n",
    "    in_header = True\n",
    "    \n",
    "    with open(template_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if in_header:\n",
    "                header_lines.append(line)\n",
    "                # Check if we've reached the data section\n",
    "                if line.strip().startswith('year') and 'day' in line:\n",
    "                    in_header = False\n",
    "            else:\n",
    "                break  # Stop after header\n",
    "    \n",
    "    return ''.join(header_lines)\n",
    "\n",
    "\n",
    "def write_calibrated_met(original_met_data, calibrated_vp, calibrated_evap, \n",
    "                        template_header, output_path, model, scenario, lat, lon):\n",
    "    \"\"\"\n",
    "    Write calibrated .met file matching template format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original_met_data : pd.DataFrame\n",
    "        Original .met file data (with all columns)\n",
    "    calibrated_vp : pd.DataFrame\n",
    "        Calibrated VP data (columns: date, value)\n",
    "    calibrated_evap : pd.DataFrame\n",
    "        Calibrated evap data (columns: date, value)\n",
    "    template_header : str\n",
    "        Template header string\n",
    "    output_path : str\n",
    "        Output file path\n",
    "    model : str\n",
    "        Model name\n",
    "    scenario : str\n",
    "        Scenario name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    # Start with original data\n",
    "    met_data = original_met_data.copy()\n",
    "    \n",
    "    # Merge calibrated VP and Evap\n",
    "    calibrated_vp['date'] = pd.to_datetime(calibrated_vp['date'])\n",
    "    calibrated_evap['date'] = pd.to_datetime(calibrated_evap['date'])\n",
    "    \n",
    "    met_data['date'] = pd.to_datetime(met_data['date'])\n",
    "    \n",
    "    # Merge VP - rename 'value' to 'value_vp' before merge\n",
    "    calibrated_vp_renamed = calibrated_vp[['date', 'value']].rename(columns={'value': 'value_vp'})\n",
    "    met_data = met_data.merge(calibrated_vp_renamed, on='date', how='left')\n",
    "    met_data['vp'] = met_data['value_vp'].fillna(met_data['vp'])\n",
    "    met_data = met_data.drop(columns=['value_vp'])\n",
    "    \n",
    "    # Merge Evap - rename 'value' to 'value_evap' before merge\n",
    "    calibrated_evap_renamed = calibrated_evap[['date', 'value']].rename(columns={'value': 'value_evap'})\n",
    "    met_data = met_data.merge(calibrated_evap_renamed, on='date', how='left')\n",
    "    met_data['evap'] = met_data['value_evap'].fillna(met_data['evap'])\n",
    "    met_data = met_data.drop(columns=['value_evap'])\n",
    "    \n",
    "    # Ensure numeric columns are properly formatted\n",
    "    for col in ['radn', 'maxt', 'mint', 'rain', 'evap', 'vp']:\n",
    "        if col in met_data.columns:\n",
    "            met_data[col] = pd.to_numeric(met_data[col], errors='coerce').fillna(0.0)\n",
    "    \n",
    "    # Calculate tav and amp\n",
    "    met_data_temp = met_data[['date', 'maxt', 'mint']].copy()\n",
    "    met_data_temp = met_data_temp.set_index('date')\n",
    "    tav, amp = calculate_tav_amp(met_data_temp)\n",
    "    \n",
    "    # Create year and day columns\n",
    "    met_data['year'] = met_data['date'].dt.year\n",
    "    met_data['day'] = met_data['date'].dt.dayofyear\n",
    "    \n",
    "    # Ensure code column exists\n",
    "    if 'code' not in met_data.columns:\n",
    "        met_data['code'] = '222222'\n",
    "    \n",
    "    # Prepare header\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    \n",
    "    # Replace template values in header\n",
    "    header = template_header\n",
    "    header = re.sub(r'latitude\\s*=\\s*[-\\d.]+', f'latitude = {lat:.2f}', header)\n",
    "    header = re.sub(r'longitude\\s*=\\s*[-\\d.]+', f'longitude =  {lon:.2f}', header)\n",
    "    header = re.sub(r'tav\\s*=\\s*[-\\d.]+', f'tav = {tav:.2f}', header)\n",
    "    header = re.sub(r'amp\\s*=\\s*[-\\d.]+', f'amp = {amp:.2f}', header)\n",
    "    \n",
    "    # Add model/scenario info to comments if not present\n",
    "    if f'{model} {scenario}' not in header:\n",
    "        # Find the line with \"Data Extracted\" and add model info\n",
    "        header = header.replace(\n",
    "            '!Data Extracted',\n",
    "            f'!Data Extracted from CMIP6 {model} {scenario} dataset on {current_date} for APSIM'\n",
    "        )\n",
    "    \n",
    "    # Write .met file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(header)\n",
    "        \n",
    "        # Write Units row after header (before data)\n",
    "        # Format: ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\n",
    "        units_line = \" ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\\n\"\n",
    "        f.write(units_line)\n",
    "        \n",
    "        # Write data rows\n",
    "        for _, row in met_data.iterrows():\n",
    "            # Format with proper column widths: year(4) day(4) radn(6) maxt(6) mint(6) rain(6) evap(6) vp(6) code(6)\n",
    "            radn_val = row['radn'] if pd.notna(row['radn']) else 0.0\n",
    "            evap_val = row['evap'] if pd.notna(row['evap']) else 0.0\n",
    "            vp_val = row['vp'] if pd.notna(row['vp']) else 0.0\n",
    "            maxt_val = row['maxt'] if pd.notna(row['maxt']) else 0.0\n",
    "            mint_val = row['mint'] if pd.notna(row['mint']) else 0.0\n",
    "            rain_val = row['rain'] if pd.notna(row['rain']) else 0.0\n",
    "            code_val = row['code'] if pd.notna(row['code']) else '222222'\n",
    "            \n",
    "            line = f\"{int(row['year']):4d} {int(row['day']):4d} {radn_val:6.1f} {maxt_val:6.1f} {mint_val:6.1f} {rain_val:6.1f} {evap_val:6.1f} {vp_val:6.1f} {code_val}\\n\"\n",
    "            f.write(line)\n",
    "    \n",
    "    print(f\"  [OK] Created calibrated MET file: {os.path.basename(output_path)}\")\n",
    "    print(f\"  [OK] Total rows: {len(met_data):,}\")\n",
    "    print(f\"  [OK] TAV: {tav:.2f}°C, AMP: {amp:.2f}°C\")\n",
    "    \n",
    "    # Also write CSV file\n",
    "    csv_output_path = output_path.replace('.met', '.csv')\n",
    "    csv_data = met_data[['date', 'year', 'day', 'radn', 'maxt', 'mint', 'rain', 'evap', 'vp', 'code']].copy()\n",
    "    csv_data.to_csv(csv_output_path, index=False, encoding='utf-8')\n",
    "    print(f\"  [OK] Created CSV file: {os.path.basename(csv_output_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Diagnostics and Reporting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diagnostics(silo_df, obs_raw_df, obs_calibrated_df, future_calibrated_df, \n",
    "                         silo_stats, cmip6_obs_stats, variable='vp', baseline_start=None, baseline_end=None):\n",
    "    \"\"\"\n",
    "    Generate diagnostics for calibration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_df : pd.DataFrame\n",
    "        SILO data (columns: date, value)\n",
    "    obs_raw_df : pd.DataFrame\n",
    "        CMIP6 obs raw data (columns: date, value)\n",
    "    obs_calibrated_df : pd.DataFrame\n",
    "        CMIP6 obs calibrated data (columns: date, value)\n",
    "    future_calibrated_df : pd.DataFrame\n",
    "        CMIP6 future calibrated data (columns: date, value)\n",
    "    silo_stats : pd.DataFrame\n",
    "        SILO monthly statistics\n",
    "    cmip6_obs_stats : pd.DataFrame\n",
    "        CMIP6 obs monthly statistics\n",
    "    variable : str\n",
    "        Variable name ('vp' or 'evap')\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Diagnostics dictionary\n",
    "    \"\"\"\n",
    "    baseline_start_dt = pd.to_datetime(baseline_start) if baseline_start else None\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end) if baseline_end else None\n",
    "    \n",
    "    # Filter to baseline if dates provided\n",
    "    if baseline_start_dt and baseline_end_dt:\n",
    "        silo_baseline = silo_df[\n",
    "            (silo_df['date'] >= baseline_start_dt) & \n",
    "            (silo_df['date'] <= baseline_end_dt)\n",
    "        ]\n",
    "        obs_raw_baseline = obs_raw_df[\n",
    "            (obs_raw_df['date'] >= baseline_start_dt) & \n",
    "            (obs_raw_df['date'] <= baseline_end_dt)\n",
    "        ]\n",
    "        obs_calibrated_baseline = obs_calibrated_df[\n",
    "            (obs_calibrated_df['date'] >= baseline_start_dt) & \n",
    "            (obs_calibrated_df['date'] <= baseline_end_dt)\n",
    "        ]\n",
    "    else:\n",
    "        silo_baseline = silo_df\n",
    "        obs_raw_baseline = obs_raw_df\n",
    "        obs_calibrated_baseline = obs_calibrated_df\n",
    "    \n",
    "    # Create monthly comparison table\n",
    "    monthly_comparison = pd.DataFrame({\n",
    "        'month': range(1, 13),\n",
    "        'SILO_mean': silo_stats['mean'].values,\n",
    "        'SILO_std': silo_stats['std'].values,\n",
    "        'CMIP6_obs_raw_mean': cmip6_obs_stats['mean'].values,\n",
    "        'CMIP6_obs_raw_std': cmip6_obs_stats['std'].values,\n",
    "    })\n",
    "    \n",
    "    # Calculate calibrated stats\n",
    "    obs_calibrated_baseline['month'] = obs_calibrated_baseline['date'].dt.month\n",
    "    calibrated_stats = obs_calibrated_baseline.groupby('month')['value'].agg(['mean', 'std']).reset_index()\n",
    "    monthly_comparison['CMIP6_obs_calibrated_mean'] = calibrated_stats['mean'].values\n",
    "    monthly_comparison['CMIP6_obs_calibrated_std'] = calibrated_stats['std'].values\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary = {\n",
    "        'silo': {\n",
    "            'min': float(silo_baseline['value'].min()),\n",
    "            'mean': float(silo_baseline['value'].mean()),\n",
    "            'max': float(silo_baseline['value'].max()),\n",
    "        },\n",
    "        'obs_raw': {\n",
    "            'min': float(obs_raw_baseline['value'].min()),\n",
    "            'mean': float(obs_raw_baseline['value'].mean()),\n",
    "            'max': float(obs_raw_baseline['value'].max()),\n",
    "        },\n",
    "        'obs_calibrated': {\n",
    "            'min': float(obs_calibrated_baseline['value'].min()),\n",
    "            'mean': float(obs_calibrated_baseline['value'].mean()),\n",
    "            'max': float(obs_calibrated_baseline['value'].max()),\n",
    "        },\n",
    "        'future_calibrated': {\n",
    "            'min': float(future_calibrated_df['value'].min()),\n",
    "            'mean': float(future_calibrated_df['value'].mean()),\n",
    "            'max': float(future_calibrated_df['value'].max()),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Check for warnings\n",
    "    warnings_list = []\n",
    "    \n",
    "    # Check for zero variance months\n",
    "    zero_var_months = monthly_comparison[monthly_comparison['CMIP6_obs_raw_std'] < 1e-6]['month'].tolist()\n",
    "    if zero_var_months:\n",
    "        warnings_list.append(f\"Zero variance months detected: {zero_var_months} (mean-only correction applied)\")\n",
    "    \n",
    "    # Check date alignment\n",
    "    silo_dates = set(silo_baseline['date'].dt.date)\n",
    "    obs_dates = set(obs_raw_baseline['date'].dt.date)\n",
    "    overlap = len(silo_dates & obs_dates)\n",
    "    if overlap < len(silo_baseline) * 0.9:  # Less than 90% overlap\n",
    "        warnings_list.append(f\"Date alignment warning: Only {overlap} overlapping days out of {len(silo_baseline)} SILO days\")\n",
    "    \n",
    "    return {\n",
    "        'monthly_comparison': monthly_comparison,\n",
    "        'summary': summary,\n",
    "        'warnings': warnings_list,\n",
    "        'overlapping_days': overlap,\n",
    "        'variable': variable\n",
    "    }\n",
    "\n",
    "\n",
    "def write_calibration_report(model, lat, lon, diagnostics_vp, diagnostics_evap, output_dir, baseline_start, baseline_end):\n",
    "    \"\"\"\n",
    "    Write calibration report to markdown file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    diagnostics_vp : dict\n",
    "        VP diagnostics dictionary\n",
    "    diagnostics_evap : dict\n",
    "        Evap diagnostics dictionary\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    \"\"\"\n",
    "    report_filename = f\"calibration_report_{model}_{lat:.2f}_{lon:.2f}.md\"\n",
    "    report_path = os.path.join(output_dir, report_filename)\n",
    "    \n",
    "    # Try to load comprehensive analysis report and validation stats if available\n",
    "    diagnostics_dir = os.path.join(output_dir, 'diagnostics', model, f\"{lat:.2f}_{lon:.2f}\")\n",
    "    analysis_report_path = os.path.join(diagnostics_dir, f\"{model}_{lat:.2f}_{lon:.2f}_comprehensive_analysis_report.txt\")\n",
    "    analysis_report_text = None\n",
    "    if os.path.exists(analysis_report_path):\n",
    "        try:\n",
    "            with open(analysis_report_path, 'r', encoding='utf-8') as af:\n",
    "                analysis_report_text = af.read()\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Could not load analysis report: {e}\")\n",
    "    \n",
    "    # Try to load validation summary from saved CSVs\n",
    "    validation_summary = {}\n",
    "    vp_stats_path = os.path.join(diagnostics_dir, f\"{model}_{lat:.2f}_{lon:.2f}_baseline_monthly_stats_vp.csv\")\n",
    "    evap_stats_path = os.path.join(diagnostics_dir, f\"{model}_{lat:.2f}_{lon:.2f}_baseline_monthly_stats_evap.csv\")\n",
    "    \n",
    "    if os.path.exists(vp_stats_path):\n",
    "        try:\n",
    "            validation_summary['vp_baseline_stats'] = pd.read_csv(vp_stats_path)\n",
    "            # Calculate deltas\n",
    "            vp_stats = validation_summary['vp_baseline_stats']\n",
    "            deltas = []\n",
    "            for month in range(1, 13):\n",
    "                month_data = vp_stats[vp_stats['month'] == month]\n",
    "                silo_row = month_data[month_data['dataset'] == 'SILO']\n",
    "                corrected_row = month_data[month_data['dataset'] == 'CMIP6_obs_corrected']\n",
    "                if len(silo_row) > 0 and len(corrected_row) > 0:\n",
    "                    mean_delta = corrected_row.iloc[0]['mean'] - silo_row.iloc[0]['mean']\n",
    "                    std_delta = corrected_row.iloc[0]['std'] - silo_row.iloc[0]['std']\n",
    "                    deltas.append({'month': month, 'mean_delta': mean_delta, 'std_delta': std_delta})\n",
    "            if deltas:\n",
    "                validation_summary['vp_baseline_deltas'] = pd.DataFrame(deltas)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Could not load VP validation stats: {e}\")\n",
    "    \n",
    "    if os.path.exists(evap_stats_path):\n",
    "        try:\n",
    "            validation_summary['evap_baseline_stats'] = pd.read_csv(evap_stats_path)\n",
    "            # Calculate deltas\n",
    "            evap_stats = validation_summary['evap_baseline_stats']\n",
    "            deltas = []\n",
    "            for month in range(1, 13):\n",
    "                month_data = evap_stats[evap_stats['month'] == month]\n",
    "                silo_row = month_data[month_data['dataset'] == 'SILO']\n",
    "                corrected_row = month_data[month_data['dataset'] == 'CMIP6_obs_corrected']\n",
    "                if len(silo_row) > 0 and len(corrected_row) > 0:\n",
    "                    mean_delta = corrected_row.iloc[0]['mean'] - silo_row.iloc[0]['mean']\n",
    "                    std_delta = corrected_row.iloc[0]['std'] - silo_row.iloc[0]['std']\n",
    "                    deltas.append({'month': month, 'mean_delta': mean_delta, 'std_delta': std_delta})\n",
    "            if deltas:\n",
    "                validation_summary['evap_baseline_deltas'] = pd.DataFrame(deltas)\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Could not load Evap validation stats: {e}\")\n",
    "    \n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"# Calibration Report\\n\\n\")\n",
    "        f.write(f\"**Model:** {model}\\n\")\n",
    "        f.write(f\"**Coordinates:** ({lat:.2f}, {lon:.2f})\\n\")\n",
    "        f.write(f\"**Baseline Window:** {baseline_start} to {baseline_end}\\n\\n\")\n",
    "        \n",
    "        # Executive Summary with Diagnostic Validation Status\n",
    "        f.write(f\"## Executive Summary\\n\\n\")\n",
    "        \n",
    "        # Extract key findings from validation summary if available\n",
    "        if validation_summary:\n",
    "            vp_deltas = validation_summary.get('vp_baseline_deltas')\n",
    "            evap_deltas = validation_summary.get('evap_baseline_deltas')\n",
    "            \n",
    "            # Count anomalies\n",
    "            vp_anomalies = []\n",
    "            evap_anomalies = []\n",
    "            \n",
    "            if vp_deltas is not None and len(vp_deltas) > 0:\n",
    "                # Check for mean deviations > 0.5 units or std > 0.3\n",
    "                for _, row in vp_deltas.iterrows():\n",
    "                    mean_delta = abs(row.get('mean_delta', 0))\n",
    "                    std_delta = abs(row.get('std_delta', 0))\n",
    "                    month = int(row.get('month', 0))\n",
    "                    \n",
    "                    if mean_delta > 0.5:\n",
    "                        vp_anomalies.append(f\"Month {month}: Mean deviation = {mean_delta:.3f} hPa\")\n",
    "                    if std_delta > 0.3:\n",
    "                        vp_anomalies.append(f\"Month {month}: Std deviation = {std_delta:.3f} hPa\")\n",
    "            \n",
    "            if evap_deltas is not None and len(evap_deltas) > 0:\n",
    "                for _, row in evap_deltas.iterrows():\n",
    "                    mean_delta = abs(row.get('mean_delta', 0))\n",
    "                    std_delta = abs(row.get('std_delta', 0))\n",
    "                    month = int(row.get('month', 0))\n",
    "                    \n",
    "                    if mean_delta > 0.5:\n",
    "                        evap_anomalies.append(f\"Month {month}: Mean deviation = {mean_delta:.3f} mm/day\")\n",
    "                    if std_delta > 0.3:\n",
    "                        evap_anomalies.append(f\"Month {month}: Std deviation = {std_delta:.3f} mm/day\")\n",
    "            \n",
    "            # Overall status\n",
    "            total_anomalies = len(vp_anomalies) + len(evap_anomalies)\n",
    "            if total_anomalies == 0:\n",
    "                f.write(f\"### ✅ **CALIBRATION STATUS: PASSED**\\n\\n\")\n",
    "                f.write(f\"All diagnostic validation checks passed. Calibration successfully matches SILO statistics.\\n\\n\")\n",
    "            elif total_anomalies <= 5:\n",
    "                f.write(f\"### ⚠️ **CALIBRATION STATUS: MINOR ISSUES DETECTED**\\n\\n\")\n",
    "                f.write(f\"Some minor deviations detected. Review recommended.\\n\\n\")\n",
    "            else:\n",
    "                f.write(f\"### ❌ **CALIBRATION STATUS: ISSUES DETECTED**\\n\\n\")\n",
    "                f.write(f\"Multiple anomalies detected. Calibration review required.\\n\\n\")\n",
    "            \n",
    "            f.write(f\"**Anomalies Detected:**\\n\")\n",
    "            f.write(f\"- VP: {len(vp_anomalies)} issues\\n\")\n",
    "            f.write(f\"- Evap: {len(evap_anomalies)} issues\\n\")\n",
    "            f.write(f\"- **Total:** {total_anomalies} issues\\n\\n\")\n",
    "            \n",
    "            if vp_anomalies or evap_anomalies:\n",
    "                f.write(f\"### 🔍 **Anomaly Details**\\n\\n\")\n",
    "                if vp_anomalies:\n",
    "                    f.write(f\"#### VP Anomalies:\\n\\n\")\n",
    "                    for anomaly in vp_anomalies:\n",
    "                        f.write(f\"- ❌ **{anomaly}**\\n\")\n",
    "                    f.write(f\"\\n\")\n",
    "                if evap_anomalies:\n",
    "                    f.write(f\"#### Evap Anomalies:\\n\\n\")\n",
    "                    for anomaly in evap_anomalies:\n",
    "                        f.write(f\"- ❌ **{anomaly}**\\n\")\n",
    "                    f.write(f\"\\n\")\n",
    "        \n",
    "        # VP Section\n",
    "        f.write(f\"## Vapor Pressure (VP) Calibration\\n\\n\")\n",
    "        f.write(f\"### Baseline Statistics\\n\\n\")\n",
    "        f.write(f\"- **Overlapping Days:** {diagnostics_vp['overlapping_days']}\\n\\n\")\n",
    "        \n",
    "        # Add diagnostic validation results for VP\n",
    "        if validation_summary and 'vp_baseline_stats' in validation_summary:\n",
    "            f.write(f\"### ✅ Diagnostic Validation Results (VP)\\n\\n\")\n",
    "            vp_stats = validation_summary['vp_baseline_stats']\n",
    "            vp_deltas = validation_summary.get('vp_baseline_deltas')\n",
    "            \n",
    "            f.write(f\"#### Monthly Statistics Comparison\\n\\n\")\n",
    "            f.write(f\"| Month | Dataset | Mean (hPa) | Std (hPa) | Count | Status |\\n\")\n",
    "            f.write(f\"|-------|---------|------------|-----------|-------|--------|\\n\")\n",
    "            \n",
    "            for month in range(1, 13):\n",
    "                month_data = vp_stats[vp_stats['month'] == month]\n",
    "                for _, row in month_data.iterrows():\n",
    "                    dataset = row['dataset']\n",
    "                    mean_val = row['mean']\n",
    "                    std_val = row['std']\n",
    "                    count = int(row['n'])\n",
    "                    \n",
    "                    # Determine status\n",
    "                    if dataset == 'SILO':\n",
    "                        status = \"📊 Reference\"\n",
    "                    elif dataset == 'CMIP6_obs_raw':\n",
    "                        status = \"📉 Raw\"\n",
    "                    elif dataset == 'CMIP6_obs_corrected':\n",
    "                        # Check if corrected matches SILO\n",
    "                        silo_row = month_data[month_data['dataset'] == 'SILO']\n",
    "                        if len(silo_row) > 0:\n",
    "                            silo_mean = silo_row.iloc[0]['mean']\n",
    "                            silo_std = silo_row.iloc[0]['std']\n",
    "                            mean_diff = abs(mean_val - silo_mean)\n",
    "                            std_diff = abs(std_val - silo_std)\n",
    "                            \n",
    "                            if mean_diff <= 0.5 and std_diff <= 0.3:\n",
    "                                status = \"✅ Pass\"\n",
    "                            elif mean_diff <= 1.0 and std_diff <= 0.5:\n",
    "                                status = \"⚠️ Minor\"\n",
    "                            else:\n",
    "                                status = \"❌ Fail\"\n",
    "                        else:\n",
    "                            status = \"❓ Unknown\"\n",
    "                    else:\n",
    "                        status = \"\"\n",
    "                    \n",
    "                    f.write(f\"| {int(month)} | {dataset} | {mean_val:.2f} | {std_val:.2f} | {count} | {status} |\\n\")\n",
    "            \n",
    "            # Post-correction deltas with highlighting\n",
    "            if vp_deltas is not None and len(vp_deltas) > 0:\n",
    "                f.write(f\"\\n#### Post-Correction Deltas (Corrected - SILO)\\n\\n\")\n",
    "                f.write(f\"| Month | Mean Delta (hPa) | Std Delta (hPa) | Status |\\n\")\n",
    "                f.write(f\"|-------|------------------|-----------------|--------|\\n\")\n",
    "                \n",
    "                for _, row in vp_deltas.iterrows():\n",
    "                    month = int(row['month'])\n",
    "                    mean_delta = row.get('mean_delta', 0)\n",
    "                    std_delta = row.get('std_delta', 0)\n",
    "                    mean_abs = abs(mean_delta)\n",
    "                    std_abs = abs(std_delta)\n",
    "                    \n",
    "                    # Determine status\n",
    "                    if mean_abs <= 0.5 and std_abs <= 0.3:\n",
    "                        status = \"✅ Pass\"\n",
    "                    elif mean_abs <= 1.0 and std_abs <= 0.5:\n",
    "                        status = \"⚠️ Minor Deviation\"\n",
    "                    else:\n",
    "                        status = \"❌ **ANOMALY**\"\n",
    "                    \n",
    "                    # Highlight anomalies\n",
    "                    if mean_abs > 0.5 or std_abs > 0.3:\n",
    "                        mean_str = f\"**{mean_delta:.3f}**\"\n",
    "                        std_str = f\"**{std_delta:.3f}**\"\n",
    "                    else:\n",
    "                        mean_str = f\"{mean_delta:.3f}\"\n",
    "                        std_str = f\"{std_delta:.3f}\"\n",
    "                    \n",
    "                    f.write(f\"| {month} | {mean_str} | {std_str} | {status} |\\n\")\n",
    "        \n",
    "        f.write(f\"\\n### Baseline Statistics\\n\\n\")\n",
    "        f.write(f\"- **Overlapping Days:** {diagnostics_vp['overlapping_days']}\\n\\n\")\n",
    "        f.write(f\"- **Overlapping Days:** {diagnostics_vp['overlapping_days']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"### Monthly Mean/Std Comparison\\n\\n\")\n",
    "        f.write(f\"| Month | SILO μ | SILO σ | CMIP6 Raw μ | CMIP6 Raw σ | CMIP6 Calibrated μ | CMIP6 Calibrated σ |\\n\")\n",
    "        f.write(f\"|-------|--------|--------|-------------|-------------|-------------------|-------------------|\\n\")\n",
    "        for _, row in diagnostics_vp['monthly_comparison'].iterrows():\n",
    "            f.write(f\"| {int(row['month'])} | {row['SILO_mean']:.2f} | {row['SILO_std']:.2f} | \"\n",
    "                   f\"{row['CMIP6_obs_raw_mean']:.2f} | {row['CMIP6_obs_raw_std']:.2f} | \"\n",
    "                   f\"{row['CMIP6_obs_calibrated_mean']:.2f} | {row['CMIP6_obs_calibrated_std']:.2f} |\\n\")\n",
    "        \n",
    "        f.write(f\"\\n### Summary Statistics\\n\\n\")\n",
    "        f.write(f\"| Series | Min | Mean | Max |\\n\")\n",
    "        f.write(f\"|--------|-----|------|-----|\\n\")\n",
    "        for series_name, stats in diagnostics_vp['summary'].items():\n",
    "            f.write(f\"| {series_name} | {stats['min']:.2f} | {stats['mean']:.2f} | {stats['max']:.2f} |\\n\")\n",
    "        \n",
    "        if diagnostics_vp['warnings']:\n",
    "            f.write(f\"\\n### Warnings\\n\\n\")\n",
    "            for warning in diagnostics_vp['warnings']:\n",
    "                f.write(f\"- {warning}\\n\")\n",
    "        \n",
    "        # Evap Section\n",
    "        f.write(f\"\\n## Evaporation (Evap) Calibration\\n\\n\")\n",
    "        f.write(f\"### Baseline Statistics\\n\\n\")\n",
    "        f.write(f\"- **Overlapping Days:** {diagnostics_evap['overlapping_days']}\\n\\n\")\n",
    "        \n",
    "        # Add diagnostic validation results for Evap\n",
    "        if validation_summary and 'evap_baseline_stats' in validation_summary:\n",
    "            f.write(f\"### ✅ Diagnostic Validation Results (Evap)\\n\\n\")\n",
    "            evap_stats = validation_summary['evap_baseline_stats']\n",
    "            evap_deltas = validation_summary.get('evap_baseline_deltas')\n",
    "            \n",
    "            f.write(f\"#### Monthly Statistics Comparison\\n\\n\")\n",
    "            f.write(f\"| Month | Dataset | Mean (mm/day) | Std (mm/day) | Count | Status |\\n\")\n",
    "            f.write(f\"|-------|---------|---------------|--------------|-------|--------|\\n\")\n",
    "            \n",
    "            for month in range(1, 13):\n",
    "                month_data = evap_stats[evap_stats['month'] == month]\n",
    "                for _, row in month_data.iterrows():\n",
    "                    dataset = row['dataset']\n",
    "                    mean_val = row['mean']\n",
    "                    std_val = row['std']\n",
    "                    count = int(row['n'])\n",
    "                    \n",
    "                    # Determine status\n",
    "                    if dataset == 'SILO':\n",
    "                        status = \"📊 Reference\"\n",
    "                    elif dataset == 'CMIP6_obs_raw':\n",
    "                        status = \"📉 Raw\"\n",
    "                    elif dataset == 'CMIP6_obs_corrected':\n",
    "                        # Check if corrected matches SILO\n",
    "                        silo_row = month_data[month_data['dataset'] == 'SILO']\n",
    "                        if len(silo_row) > 0:\n",
    "                            silo_mean = silo_row.iloc[0]['mean']\n",
    "                            silo_std = silo_row.iloc[0]['std']\n",
    "                            mean_diff = abs(mean_val - silo_mean)\n",
    "                            std_diff = abs(std_val - silo_std)\n",
    "                            \n",
    "                            if mean_diff <= 0.5 and std_diff <= 0.3:\n",
    "                                status = \"✅ Pass\"\n",
    "                            elif mean_diff <= 1.0 and std_diff <= 0.5:\n",
    "                                status = \"⚠️ Minor\"\n",
    "                            else:\n",
    "                                status = \"❌ Fail\"\n",
    "                        else:\n",
    "                            status = \"❓ Unknown\"\n",
    "                    else:\n",
    "                        status = \"\"\n",
    "                    \n",
    "                    f.write(f\"| {int(month)} | {dataset} | {mean_val:.2f} | {std_val:.2f} | {count} | {status} |\\n\")\n",
    "            \n",
    "            # Post-correction deltas with highlighting\n",
    "            if evap_deltas is not None and len(evap_deltas) > 0:\n",
    "                f.write(f\"\\n#### Post-Correction Deltas (Corrected - SILO)\\n\\n\")\n",
    "                f.write(f\"| Month | Mean Delta (mm/day) | Std Delta (mm/day) | Status |\\n\")\n",
    "                f.write(f\"|-------|---------------------|-------------------|--------|\\n\")\n",
    "                \n",
    "                for _, row in evap_deltas.iterrows():\n",
    "                    month = int(row['month'])\n",
    "                    mean_delta = row.get('mean_delta', 0)\n",
    "                    std_delta = row.get('std_delta', 0)\n",
    "                    mean_abs = abs(mean_delta)\n",
    "                    std_abs = abs(std_delta)\n",
    "                    \n",
    "                    # Determine status\n",
    "                    if mean_abs <= 0.5 and std_abs <= 0.3:\n",
    "                        status = \"✅ Pass\"\n",
    "                    elif mean_abs <= 1.0 and std_abs <= 0.5:\n",
    "                        status = \"⚠️ Minor Deviation\"\n",
    "                    else:\n",
    "                        status = \"❌ **ANOMALY**\"\n",
    "                    \n",
    "                    # Highlight anomalies\n",
    "                    if mean_abs > 0.5 or std_abs > 0.3:\n",
    "                        mean_str = f\"**{mean_delta:.3f}**\"\n",
    "                        std_str = f\"**{std_delta:.3f}**\"\n",
    "                    else:\n",
    "                        mean_str = f\"{mean_delta:.3f}\"\n",
    "                        std_str = f\"{std_delta:.3f}\"\n",
    "                    \n",
    "                    f.write(f\"| {month} | {mean_str} | {std_str} | {status} |\\n\")\n",
    "        \n",
    "        f.write(f\"\\n### Baseline Statistics\\n\\n\")\n",
    "        f.write(f\"- **Overlapping Days:** {diagnostics_evap['overlapping_days']}\\n\\n\")\n",
    "        f.write(f\"- **Overlapping Days:** {diagnostics_evap['overlapping_days']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"### Monthly Mean/Std Comparison\\n\\n\")\n",
    "        f.write(f\"| Month | SILO μ | SILO σ | CMIP6 Raw μ | CMIP6 Raw σ | CMIP6 Calibrated μ | CMIP6 Calibrated σ |\\n\")\n",
    "        f.write(f\"|-------|--------|--------|-------------|-------------|-------------------|-------------------|\\n\")\n",
    "        for _, row in diagnostics_evap['monthly_comparison'].iterrows():\n",
    "            f.write(f\"| {int(row['month'])} | {row['SILO_mean']:.2f} | {row['SILO_std']:.2f} | \"\n",
    "                   f\"{row['CMIP6_obs_raw_mean']:.2f} | {row['CMIP6_obs_raw_std']:.2f} | \"\n",
    "                   f\"{row['CMIP6_obs_calibrated_mean']:.2f} | {row['CMIP6_obs_calibrated_std']:.2f} |\\n\")\n",
    "        \n",
    "        f.write(f\"\\n### Summary Statistics\\n\\n\")\n",
    "        f.write(f\"| Series | Min | Mean | Max |\\n\")\n",
    "        f.write(f\"|--------|-----|------|-----|\\n\")\n",
    "        for series_name, stats in diagnostics_evap['summary'].items():\n",
    "            f.write(f\"| {series_name} | {stats['min']:.2f} | {stats['mean']:.2f} | {stats['max']:.2f} |\\n\")\n",
    "        \n",
    "        if diagnostics_evap['warnings']:\n",
    "            f.write(f\"\\n### Warnings\\n\\n\")\n",
    "            for warning in diagnostics_evap['warnings']:\n",
    "                f.write(f\"- {warning}\\n\")\n",
    "    \n",
    "        \n",
    "        # Comprehensive Analysis Report Section\n",
    "        if analysis_report_text:\n",
    "            f.write(f\"\\n## 📋 Comprehensive Diagnostic Validation Summary\\n\\n\")\n",
    "            f.write(f\"### Full Analysis Report\\n\\n\")\n",
    "            f.write(f\"The complete diagnostic validation analysis is available in:\\n\")\n",
    "            f.write(f\"`{os.path.basename(analysis_report_path)}`\\n\\n\")\n",
    "            \n",
    "            # Extract key findings from analysis report\n",
    "            lines = analysis_report_text.split('\\n')\n",
    "            in_summary = False\n",
    "            summary_lines = []\n",
    "            issues_found = []\n",
    "            warnings_found = []\n",
    "            \n",
    "            for line in lines:\n",
    "                if '[OVERALL STATUS]' in line:\n",
    "                    in_summary = True\n",
    "                    summary_lines.append(line)\n",
    "                elif in_summary and line.strip() and not line.startswith('='):\n",
    "                    summary_lines.append(line)\n",
    "                elif '[ISSUE]' in line or '[ERROR]' in line:\n",
    "                    issues_found.append(line.strip())\n",
    "                elif '[WARNING]' in line:\n",
    "                    warnings_found.append(line.strip())\n",
    "            \n",
    "            if summary_lines:\n",
    "                f.write(f\"### Overall Status\\n\\n\")\n",
    "                for line in summary_lines:\n",
    "                    f.write(f\"{line}\\n\")\n",
    "                f.write(f\"\\n\")\n",
    "            \n",
    "            if issues_found:\n",
    "                f.write(f\"### ❌ Critical Issues Detected\\n\\n\")\n",
    "                for issue in issues_found[:20]:  # Limit to first 20\n",
    "                    f.write(f\"- {issue}\\n\")\n",
    "                if len(issues_found) > 20:\n",
    "                    f.write(f\"- ... and {len(issues_found) - 20} more issues\\n\")\n",
    "                f.write(f\"\\n\")\n",
    "            \n",
    "            if warnings_found:\n",
    "                f.write(f\"### ⚠️ Warnings\\n\\n\")\n",
    "                for warning in warnings_found[:15]:  # Limit to first 15\n",
    "                    f.write(f\"- {warning}\\n\")\n",
    "                if len(warnings_found) > 15:\n",
    "                    f.write(f\"- ... and {len(warnings_found) - 15} more warnings\\n\")\n",
    "                f.write(f\"\\n\")\n",
    "    \n",
    "    print(f\"  [OK] Created calibration report: {report_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Diagnostic Validation Functions (Baseline + Future Checks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This section provides comprehensive diagnostic validation to ensure:\n",
    "\n",
    "**Baseline Validation (1986-2014):**\n",
    "- Calibration successfully matches SILO mean and variance\n",
    "- Corrected CMIP6 obs closely matches SILO climatology\n",
    "\n",
    "**Future Scenario Checks:**\n",
    "- Signal preservation: calibration doesn't distort trends or variability\n",
    "- Scenario separation: SSP245 and SSP585 maintain expected relationships\n",
    "- Physical bounds: all values are valid and within expected ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_monthly_stats_table(silo_df, cmip6_obs_raw_df, cmip6_obs_corrected_df, \n",
    "                                        baseline_start, baseline_end, variable='vp'):\n",
    "    \"\"\"\n",
    "    Create monthly statistics table for baseline validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_df : pd.DataFrame\n",
    "        SILO data (columns: date, value)\n",
    "    cmip6_obs_raw_df : pd.DataFrame\n",
    "        CMIP6 obs raw data (columns: date, value)\n",
    "    cmip6_obs_corrected_df : pd.DataFrame\n",
    "        CMIP6 obs corrected data (columns: date, value)\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    variable : str\n",
    "        Variable name ('vp' or 'evap')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Monthly statistics table\n",
    "    \"\"\"\n",
    "    baseline_start_dt = pd.to_datetime(baseline_start)\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end)\n",
    "    \n",
    "    # Filter to baseline period\n",
    "    silo_baseline = silo_df[\n",
    "        (silo_df['date'] >= baseline_start_dt) & \n",
    "        (silo_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_raw_baseline = cmip6_obs_raw_df[\n",
    "        (cmip6_obs_raw_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_raw_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_corrected_baseline = cmip6_obs_corrected_df[\n",
    "        (cmip6_obs_corrected_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_corrected_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate monthly statistics for each dataset\n",
    "    results = []\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        # SILO\n",
    "        silo_month = silo_baseline[silo_baseline['date'].dt.month == month]['value']\n",
    "        results.append({\n",
    "            'month': month,\n",
    "            'dataset': 'SILO',\n",
    "            'mean': silo_month.mean(),\n",
    "            'std': silo_month.std(),\n",
    "            'n': len(silo_month)\n",
    "        })\n",
    "        \n",
    "        # CMIP6 obs raw\n",
    "        cmip6_raw_month = cmip6_obs_raw_baseline[cmip6_obs_raw_baseline['date'].dt.month == month]['value']\n",
    "        results.append({\n",
    "            'month': month,\n",
    "            'dataset': 'CMIP6_obs_raw',\n",
    "            'mean': cmip6_raw_month.mean(),\n",
    "            'std': cmip6_raw_month.std(),\n",
    "            'n': len(cmip6_raw_month)\n",
    "        })\n",
    "        \n",
    "        # CMIP6 obs corrected\n",
    "        cmip6_corrected_month = cmip6_obs_corrected_baseline[cmip6_obs_corrected_baseline['date'].dt.month == month]['value']\n",
    "        results.append({\n",
    "            'month': month,\n",
    "            'dataset': 'CMIP6_obs_corrected',\n",
    "            'mean': cmip6_corrected_month.mean(),\n",
    "            'std': cmip6_corrected_month.std(),\n",
    "            'n': len(cmip6_corrected_month)\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Create deltas table\n",
    "    deltas = []\n",
    "    for month in range(1, 13):\n",
    "        silo_mean = stats_df[(stats_df['month'] == month) & (stats_df['dataset'] == 'SILO')]['mean'].values[0]\n",
    "        silo_std = stats_df[(stats_df['month'] == month) & (stats_df['dataset'] == 'SILO')]['std'].values[0]\n",
    "        corrected_mean = stats_df[(stats_df['month'] == month) & (stats_df['dataset'] == 'CMIP6_obs_corrected')]['mean'].values[0]\n",
    "        corrected_std = stats_df[(stats_df['month'] == month) & (stats_df['dataset'] == 'CMIP6_obs_corrected')]['std'].values[0]\n",
    "        \n",
    "        deltas.append({\n",
    "            'month': month,\n",
    "            'mean_delta': corrected_mean - silo_mean,\n",
    "            'std_delta': corrected_std - silo_std\n",
    "        })\n",
    "    \n",
    "    deltas_df = pd.DataFrame(deltas)\n",
    "    \n",
    "    return stats_df, deltas_df\n",
    "\n",
    "\n",
    "def plot_baseline_monthly_climatology(silo_df, cmip6_obs_raw_df, cmip6_obs_corrected_df,\n",
    "                                      baseline_start, baseline_end, variable='vp', \n",
    "                                      output_dir=None, model=None, lat=None, lon=None):\n",
    "    \"\"\"\n",
    "    Plot baseline monthly climatology (mean and std).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_df : pd.DataFrame\n",
    "        SILO data\n",
    "    cmip6_obs_raw_df : pd.DataFrame\n",
    "        CMIP6 obs raw data\n",
    "    cmip6_obs_corrected_df : pd.DataFrame\n",
    "        CMIP6 obs corrected data\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    variable : str\n",
    "        Variable name ('vp' or 'evap')\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    baseline_start_dt = pd.to_datetime(baseline_start)\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end)\n",
    "    \n",
    "    # Filter to baseline\n",
    "    silo_baseline = silo_df[\n",
    "        (silo_df['date'] >= baseline_start_dt) & \n",
    "        (silo_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_raw_baseline = cmip6_obs_raw_df[\n",
    "        (cmip6_obs_raw_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_raw_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_corrected_baseline = cmip6_obs_corrected_df[\n",
    "        (cmip6_obs_corrected_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_corrected_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    # Calculate monthly means and stds\n",
    "    months = range(1, 13)\n",
    "    silo_means = [silo_baseline[silo_baseline['date'].dt.month == m]['value'].mean() for m in months]\n",
    "    silo_stds = [silo_baseline[silo_baseline['date'].dt.month == m]['value'].std() for m in months]\n",
    "    \n",
    "    cmip6_raw_means = [cmip6_obs_raw_baseline[cmip6_obs_raw_baseline['date'].dt.month == m]['value'].mean() for m in months]\n",
    "    cmip6_raw_stds = [cmip6_obs_raw_baseline[cmip6_obs_raw_baseline['date'].dt.month == m]['value'].std() for m in months]\n",
    "    \n",
    "    cmip6_corrected_means = [cmip6_obs_corrected_baseline[cmip6_obs_corrected_baseline['date'].dt.month == m]['value'].mean() for m in months]\n",
    "    cmip6_corrected_stds = [cmip6_obs_corrected_baseline[cmip6_obs_corrected_baseline['date'].dt.month == m]['value'].std() for m in months]\n",
    "    \n",
    "    # Plot monthly mean\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(months, silo_means, 'o-', label='SILO', linewidth=2, markersize=8)\n",
    "    ax.plot(months, cmip6_raw_means, 's--', label='CMIP6 obs raw', linewidth=2, markersize=6)\n",
    "    ax.plot(months, cmip6_corrected_means, '^-', label='CMIP6 obs corrected', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Month', fontsize=12)\n",
    "    ax.set_ylabel(f'{variable.upper()} Mean ({get_unit(variable)})', fontsize=12)\n",
    "    ax.set_title(f'{model} - Baseline Monthly Mean ({baseline_start} to {baseline_end})\\n{variable.upper()}', fontsize=14)\n",
    "    ax.set_xticks(months)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"{model}_{lat:.2f}_{lon:.2f}_baseline_monthly_mean_{variable}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot monthly std\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(months, silo_stds, 'o-', label='SILO', linewidth=2, markersize=8)\n",
    "    ax.plot(months, cmip6_raw_stds, 's--', label='CMIP6 obs raw', linewidth=2, markersize=6)\n",
    "    ax.plot(months, cmip6_corrected_stds, '^-', label='CMIP6 obs corrected', linewidth=2, markersize=6)\n",
    "    ax.set_xlabel('Month', fontsize=12)\n",
    "    ax.set_ylabel(f'{variable.upper()} Std ({get_unit(variable)})', fontsize=12)\n",
    "    ax.set_title(f'{model} - Baseline Monthly Std ({baseline_start} to {baseline_end})\\n{variable.upper()}', fontsize=14)\n",
    "    ax.set_xticks(months)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        filename = f\"{model}_{lat:.2f}_{lon:.2f}_baseline_monthly_std_{variable}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_unit(variable):\n",
    "    \"\"\"Get unit for variable.\"\"\"\n",
    "    units = {'vp': 'hPa', 'evap': 'mm/day'}\n",
    "    return units.get(variable, '')\n",
    "\n",
    "\n",
    "def plot_baseline_daily_overlay(silo_df, cmip6_obs_raw_df, cmip6_obs_corrected_df,\n",
    "                                baseline_start, baseline_end, variable='vp',\n",
    "                                output_dir=None, model=None, lat=None, lon=None):\n",
    "    \"\"\"\n",
    "    Plot daily time-series overlay for a representative baseline year.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_df : pd.DataFrame\n",
    "        SILO data\n",
    "    cmip6_obs_raw_df : pd.DataFrame\n",
    "        CMIP6 obs raw data\n",
    "    cmip6_obs_corrected_df : pd.DataFrame\n",
    "        CMIP6 obs corrected data\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    variable : str\n",
    "        Variable name\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    baseline_start_dt = pd.to_datetime(baseline_start)\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end)\n",
    "    \n",
    "    # Filter to baseline\n",
    "    silo_baseline = silo_df[\n",
    "        (silo_df['date'] >= baseline_start_dt) & \n",
    "        (silo_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_raw_baseline = cmip6_obs_raw_df[\n",
    "        (cmip6_obs_raw_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_raw_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    cmip6_obs_corrected_baseline = cmip6_obs_corrected_df[\n",
    "        (cmip6_obs_corrected_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_corrected_df['date'] <= baseline_end_dt)\n",
    "    ].copy()\n",
    "    \n",
    "    # Find a representative year (e.g., 2005, or first year with complete data)\n",
    "    target_year = 2005\n",
    "    if target_year not in silo_baseline['date'].dt.year.values:\n",
    "        # Find first year with data in all datasets\n",
    "        silo_years = set(silo_baseline['date'].dt.year.values)\n",
    "        cmip6_years = set(cmip6_obs_raw_baseline['date'].dt.year.values)\n",
    "        common_years = sorted(list(silo_years & cmip6_years))\n",
    "        if common_years:\n",
    "            target_year = common_years[0]\n",
    "        else:\n",
    "            target_year = silo_baseline['date'].dt.year.min()\n",
    "    \n",
    "    # Filter to target year\n",
    "    silo_year = silo_baseline[silo_baseline['date'].dt.year == target_year].copy()\n",
    "    cmip6_raw_year = cmip6_obs_raw_baseline[cmip6_obs_raw_baseline['date'].dt.year == target_year].copy()\n",
    "    cmip6_corrected_year = cmip6_obs_corrected_baseline[cmip6_obs_corrected_baseline['date'].dt.year == target_year].copy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    ax.plot(silo_year['date'], silo_year['value'], label='SILO', linewidth=1.5, alpha=0.8)\n",
    "    ax.plot(cmip6_raw_year['date'], cmip6_raw_year['value'], label='CMIP6 obs raw', linewidth=1.5, alpha=0.7, linestyle='--')\n",
    "    ax.plot(cmip6_corrected_year['date'], cmip6_corrected_year['value'], label='CMIP6 obs corrected', linewidth=1.5, alpha=0.8)\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel(f'{variable.upper()} ({get_unit(variable)})', fontsize=12)\n",
    "    ax.set_title(f'{model} - Baseline Daily Overlay Year {target_year}\\n{variable.upper()}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"{model}_{lat:.2f}_{lon:.2f}_baseline_daily_overlay_{target_year}_{variable}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_baseline_distribution(silo_df, cmip6_obs_raw_df, cmip6_obs_corrected_df,\n",
    "                               baseline_start, baseline_end, variable='vp',\n",
    "                               output_dir=None, model=None, lat=None, lon=None):\n",
    "    \"\"\"\n",
    "    Plot distribution (histogram/density) for baseline period.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_df : pd.DataFrame\n",
    "        SILO data\n",
    "    cmip6_obs_raw_df : pd.DataFrame\n",
    "        CMIP6 obs raw data\n",
    "    cmip6_obs_corrected_df : pd.DataFrame\n",
    "        CMIP6 obs corrected data\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    variable : str\n",
    "        Variable name\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    baseline_start_dt = pd.to_datetime(baseline_start)\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end)\n",
    "    \n",
    "    # Filter to baseline\n",
    "    silo_baseline = silo_df[\n",
    "        (silo_df['date'] >= baseline_start_dt) & \n",
    "        (silo_df['date'] <= baseline_end_dt)\n",
    "    ]['value'].dropna()\n",
    "    \n",
    "    cmip6_obs_raw_baseline = cmip6_obs_raw_df[\n",
    "        (cmip6_obs_raw_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_raw_df['date'] <= baseline_end_dt)\n",
    "    ]['value'].dropna()\n",
    "    \n",
    "    cmip6_obs_corrected_baseline = cmip6_obs_corrected_df[\n",
    "        (cmip6_obs_corrected_df['date'] >= baseline_start_dt) & \n",
    "        (cmip6_obs_corrected_df['date'] <= baseline_end_dt)\n",
    "    ]['value'].dropna()\n",
    "    \n",
    "    # Plot histogram\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.hist(silo_baseline, bins=50, alpha=0.6, label='SILO', density=True, edgecolor='black')\n",
    "    ax.hist(cmip6_obs_raw_baseline, bins=50, alpha=0.6, label='CMIP6 obs raw', density=True, edgecolor='black')\n",
    "    ax.hist(cmip6_obs_corrected_baseline, bins=50, alpha=0.6, label='CMIP6 obs corrected', density=True, edgecolor='black')\n",
    "    ax.set_xlabel(f'{variable.upper()} ({get_unit(variable)})', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.set_title(f'{model} - Baseline Distribution ({baseline_start} to {baseline_end})\\n{variable.upper()}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"{model}_{lat:.2f}_{lon:.2f}_baseline_distribution_{variable}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_analysis_report(silo_vp_df, silo_evap_df,\n",
    "                                           vp_proxy_obs, evap_proxy_obs,\n",
    "                                           vp_calibration, evap_calibration,\n",
    "                                           vp_proxy_future, evap_proxy_future,\n",
    "                                           original_met_data,\n",
    "                                           baseline_start, baseline_end,\n",
    "                                           model, scenario, lat, lon,\n",
    "                                           diagnostics_dir, validation_summary):\n",
    "    \"\"\"\n",
    "    Generate comprehensive text analysis report identifying deviations and potential errors.\n",
    "    \n",
    "    Performs checks A-F:\n",
    "    A) Baseline distribution checks (mean/std comparison)\n",
    "    B) Daily time-series overlay analysis\n",
    "    C) Variance preservation check\n",
    "    D) Future signal preservation\n",
    "    E) Physical bounds & plausibility\n",
    "    F) Cross-variable consistency (VP vs evap)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Comprehensive text analysis report\n",
    "    \"\"\"\n",
    "    report_lines = []\n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(f\"COMPREHENSIVE CALIBRATION ANALYSIS REPORT\")\n",
    "    report_lines.append(f\"Model: {model} | Scenario: {scenario} | Location: ({lat}, {lon})\")\n",
    "    report_lines.append(f\"Baseline Period: {baseline_start} to {baseline_end}\")\n",
    "    report_lines.append(\"=\"*80)\n",
    "    report_lines.append(\"\")\n",
    "    \n",
    "    baseline_start_dt = pd.to_datetime(baseline_start)\n",
    "    baseline_end_dt = pd.to_datetime(baseline_end)\n",
    "    \n",
    "    # Filter data to baseline period\n",
    "    silo_vp_baseline = silo_vp_df[(silo_vp_df['date'] >= baseline_start_dt) & \n",
    "                                   (silo_vp_df['date'] <= baseline_end_dt)].copy()\n",
    "    silo_evap_baseline = silo_evap_df[(silo_evap_df['date'] >= baseline_start_dt) & \n",
    "                                       (silo_evap_df['date'] <= baseline_end_dt)].copy()\n",
    "    \n",
    "    vp_obs_raw_baseline = vp_proxy_obs[(vp_proxy_obs['date'] >= baseline_start_dt) & \n",
    "                                        (vp_proxy_obs['date'] <= baseline_end_dt)].copy()\n",
    "    evap_obs_raw_baseline = evap_proxy_obs[(evap_proxy_obs['date'] >= baseline_start_dt) & \n",
    "                                            (evap_proxy_obs['date'] <= baseline_end_dt)].copy()\n",
    "    \n",
    "    vp_obs_corrected = vp_calibration['obs'].copy()\n",
    "    evap_obs_corrected = evap_calibration['obs'].copy()\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CHECK A: Baseline Distribution Checks (Mean/Std Comparison)\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"CHECK A: BASELINE DISTRIBUTION CHECKS\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    for variable, silo_df, raw_df, corrected_df in [\n",
    "        ('VP', silo_vp_baseline, vp_obs_raw_baseline, vp_obs_corrected),\n",
    "        ('Evap', silo_evap_baseline, evap_obs_raw_baseline, evap_obs_corrected)\n",
    "    ]:\n",
    "        report_lines.append(f\"\\n{variable} Analysis:\")\n",
    "        \n",
    "        # Monthly mean/std comparison\n",
    "        issues = []\n",
    "        warnings = []\n",
    "        \n",
    "        for month in range(1, 13):\n",
    "            silo_month = silo_df[silo_df['date'].dt.month == month]['value']\n",
    "            raw_month = raw_df[raw_df['date'].dt.month == month]['value']\n",
    "            corrected_month = corrected_df[corrected_df['date'].dt.month == month]['value']\n",
    "            \n",
    "            if len(silo_month) == 0 or len(corrected_month) == 0:\n",
    "                continue\n",
    "            \n",
    "            silo_mean = silo_month.mean()\n",
    "            silo_std = silo_month.std()\n",
    "            corrected_mean = corrected_month.mean()\n",
    "            corrected_std = corrected_month.std()\n",
    "            \n",
    "            # Check mean agreement (within 10% or 0.5 units)\n",
    "            mean_diff = abs(corrected_mean - silo_mean)\n",
    "            mean_tolerance = max(0.1 * abs(silo_mean), 0.5)\n",
    "            if mean_diff > mean_tolerance:\n",
    "                issues.append(f\"Month {month:2d}: Mean deviation = {mean_diff:.3f} \"\n",
    "                            f\"(SILO: {silo_mean:.3f}, Corrected: {corrected_mean:.3f})\")\n",
    "            \n",
    "            # Check std agreement (within 20% or 0.3 units)\n",
    "            std_diff = abs(corrected_std - silo_std)\n",
    "            std_tolerance = max(0.2 * silo_std, 0.3)\n",
    "            if std_diff > std_tolerance:\n",
    "                warnings.append(f\"Month {month:2d}: Std deviation = {std_diff:.3f} \"\n",
    "                              f\"(SILO: {silo_std:.3f}, Corrected: {corrected_std:.3f})\")\n",
    "        \n",
    "        if issues:\n",
    "            report_lines.append(f\"  [ISSUES] Mean deviations detected:\")\n",
    "            for issue in issues[:5]:  # Limit to first 5\n",
    "                report_lines.append(f\"    - {issue}\")\n",
    "            if len(issues) > 5:\n",
    "                report_lines.append(f\"    ... and {len(issues) - 5} more\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] Mean(CMIP6 corrected) ≈ Mean(SILO) - All months within tolerance\")\n",
    "        \n",
    "        if warnings:\n",
    "            report_lines.append(f\"  [WARNINGS] Standard deviation deviations detected:\")\n",
    "            for warning in warnings[:5]:\n",
    "                report_lines.append(f\"    - {warning}\")\n",
    "            if len(warnings) > 5:\n",
    "                report_lines.append(f\"    ... and {len(warnings) - 5} more\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] Std(CMIP6 corrected) ≈ Std(SILO) - All months within tolerance\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CHECK B: Daily Time-Series Overlay Analysis\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"CHECK B: DAILY TIME-SERIES OVERLAY ANALYSIS\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    for variable, silo_df, raw_df, corrected_df in [\n",
    "        ('VP', silo_vp_baseline, vp_obs_raw_baseline, vp_obs_corrected),\n",
    "        ('Evap', silo_evap_baseline, evap_obs_raw_baseline, evap_obs_corrected)\n",
    "    ]:\n",
    "        report_lines.append(f\"\\n{variable} Analysis:\")\n",
    "        \n",
    "        # Find a representative year with complete data\n",
    "        available_years = sorted(set(silo_df['date'].dt.year) & \n",
    "                                 set(raw_df['date'].dt.year) & \n",
    "                                 set(corrected_df['date'].dt.year))\n",
    "        \n",
    "        if not available_years:\n",
    "            report_lines.append(f\"  [ERROR] No overlapping years found for comparison\")\n",
    "            continue\n",
    "        \n",
    "        # Analyze multiple years\n",
    "        pattern_issues = []\n",
    "        magnitude_issues = []\n",
    "        \n",
    "        for year in available_years[:3]:  # Check first 3 available years\n",
    "            silo_year = silo_df[silo_df['date'].dt.year == year].set_index('date')['value']\n",
    "            raw_year = raw_df[raw_df['date'].dt.year == year].set_index('date')['value']\n",
    "            corrected_year = corrected_df[corrected_df['date'].dt.year == year].set_index('date')['value']\n",
    "            \n",
    "            # Align dates\n",
    "            common_dates = silo_year.index.intersection(corrected_year.index).intersection(raw_year.index)\n",
    "            if len(common_dates) < 300:  # Need at least ~10 months\n",
    "                continue\n",
    "            \n",
    "            silo_aligned = silo_year.loc[common_dates]\n",
    "            raw_aligned = raw_year.loc[common_dates]\n",
    "            corrected_aligned = corrected_year.loc[common_dates]\n",
    "            \n",
    "            # Check if peaks/troughs occur on same days (correlation)\n",
    "            raw_corr = raw_aligned.corr(corrected_aligned)\n",
    "            if raw_corr < 0.7:\n",
    "                pattern_issues.append(f\"Year {year}: Low correlation between raw and corrected ({raw_corr:.3f})\")\n",
    "            \n",
    "            # Check if corrected magnitudes resemble SILO\n",
    "            mean_diff = abs(corrected_aligned.mean() - silo_aligned.mean())\n",
    "            mean_tolerance = max(0.15 * abs(silo_aligned.mean()), 1.0)\n",
    "            if mean_diff > mean_tolerance:\n",
    "                magnitude_issues.append(f\"Year {year}: Mean magnitude difference = {mean_diff:.3f}\")\n",
    "        \n",
    "        if pattern_issues:\n",
    "            report_lines.append(f\"  [ISSUES] Day-to-day pattern deviations:\")\n",
    "            for issue in pattern_issues:\n",
    "                report_lines.append(f\"    - {issue}\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] Corrected series follows same day-to-day pattern as raw CMIP6\")\n",
    "        \n",
    "        if magnitude_issues:\n",
    "            report_lines.append(f\"  [WARNINGS] Magnitude deviations from SILO:\")\n",
    "            for issue in magnitude_issues:\n",
    "                report_lines.append(f\"    - {issue}\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] CMIP6 corrected magnitudes resemble SILO\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CHECK C: Variance Preservation Check\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"CHECK C: VARIANCE PRESERVATION CHECK\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    for variable, silo_df, corrected_df in [\n",
    "        ('VP', silo_vp_baseline, vp_obs_corrected),\n",
    "        ('Evap', silo_evap_baseline, evap_obs_corrected)\n",
    "    ]:\n",
    "        report_lines.append(f\"\\n{variable} Analysis:\")\n",
    "        \n",
    "        # Annual means comparison\n",
    "        silo_annual = silo_df.groupby(silo_df['date'].dt.year)['value'].mean()\n",
    "        corrected_annual = corrected_df.groupby(corrected_df['date'].dt.year)['value'].mean()\n",
    "        \n",
    "        silo_annual_std = silo_annual.std()\n",
    "        corrected_annual_std = corrected_annual.std()\n",
    "        \n",
    "        std_ratio = corrected_annual_std / silo_annual_std if silo_annual_std > 0 else 0\n",
    "        \n",
    "        if std_ratio < 0.7:\n",
    "            report_lines.append(f\"  [ISSUE] Interannual variability collapsed: \"\n",
    "                              f\"Corrected std ({corrected_annual_std:.3f}) < 70% of SILO ({silo_annual_std:.3f})\")\n",
    "        elif std_ratio > 1.5:\n",
    "            report_lines.append(f\"  [ISSUE] Interannual variability inflated: \"\n",
    "                              f\"Corrected std ({corrected_annual_std:.3f}) > 150% of SILO ({silo_annual_std:.3f})\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] Interannual variability preserved: \"\n",
    "                              f\"Ratio = {std_ratio:.3f} (Corrected: {corrected_annual_std:.3f}, SILO: {silo_annual_std:.3f})\")\n",
    "        \n",
    "        # For Evap: check seasonal totals\n",
    "        if variable == 'Evap':\n",
    "            silo_seasonal = silo_df.groupby([silo_df['date'].dt.year, silo_df['date'].dt.quarter])['value'].sum()\n",
    "            corrected_seasonal = corrected_df.groupby([corrected_df['date'].dt.year, corrected_df['date'].dt.quarter])['value'].sum()\n",
    "            \n",
    "            silo_seasonal_std = silo_seasonal.std()\n",
    "            corrected_seasonal_std = corrected_seasonal.std()\n",
    "            \n",
    "            seasonal_ratio = corrected_seasonal_std / silo_seasonal_std if silo_seasonal_std > 0 else 0\n",
    "            \n",
    "            if seasonal_ratio < 0.7 or seasonal_ratio > 1.5:\n",
    "                report_lines.append(f\"  [WARNING] Seasonal total variance issue: Ratio = {seasonal_ratio:.3f}\")\n",
    "            else:\n",
    "                report_lines.append(f\"  [OK] Seasonal total variance preserved: Ratio = {seasonal_ratio:.3f}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CHECK D: Future Signal Preservation\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"CHECK D: FUTURE SIGNAL PRESERVATION\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    if scenario.lower() == 'obs':\n",
    "        report_lines.append(\"  [SKIP] Future signal preservation check not applicable for 'obs' scenario\")\n",
    "    else:\n",
    "        # Get future corrected data\n",
    "        vp_future_corrected = vp_calibration.get('future', pd.DataFrame())\n",
    "        evap_future_corrected = evap_calibration.get('future', pd.DataFrame())\n",
    "        \n",
    "        if len(vp_future_corrected) > 0 and len(vp_proxy_future) > 0:\n",
    "            # VP trend analysis\n",
    "            report_lines.append(\"\\nVP Analysis:\")\n",
    "            \n",
    "            # Calculate annual means\n",
    "            vp_raw_annual = vp_proxy_future.groupby(vp_proxy_future['date'].dt.year)['value'].mean()\n",
    "            vp_corrected_annual = vp_future_corrected.groupby(vp_future_corrected['date'].dt.year)['value'].mean()\n",
    "            \n",
    "            # Check if trends are parallel (correlation of year-to-year changes)\n",
    "            if len(vp_raw_annual) > 1 and len(vp_corrected_annual) > 1:\n",
    "                raw_changes = vp_raw_annual.diff().dropna()\n",
    "                corrected_changes = vp_corrected_annual.diff().dropna()\n",
    "                \n",
    "                common_years = raw_changes.index.intersection(corrected_changes.index)\n",
    "                if len(common_years) > 2:\n",
    "                    trend_corr = raw_changes.loc[common_years].corr(corrected_changes.loc[common_years])\n",
    "                    \n",
    "                    if trend_corr < 0.5:\n",
    "                        report_lines.append(f\"  [ISSUE] Trends not parallel: Correlation = {trend_corr:.3f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"  [OK] Trends are parallel: Correlation = {trend_corr:.3f}\")\n",
    "            \n",
    "            # Check long-term trend direction\n",
    "            if len(vp_raw_annual) > 5:\n",
    "                raw_trend = np.polyfit(range(len(vp_raw_annual)), vp_raw_annual.values, 1)[0]\n",
    "                corrected_trend = np.polyfit(range(len(vp_corrected_annual)), vp_corrected_annual.values, 1)[0]\n",
    "                \n",
    "                if (raw_trend > 0 and corrected_trend < 0) or (raw_trend < 0 and corrected_trend > 0):\n",
    "                    report_lines.append(f\"  [ISSUE] Trend direction reversed: Raw={raw_trend:.4f}, Corrected={corrected_trend:.4f}\")\n",
    "                else:\n",
    "                    report_lines.append(f\"  [OK] Trend direction preserved: Raw={raw_trend:.4f}, Corrected={corrected_trend:.4f}\")\n",
    "        \n",
    "        if len(evap_future_corrected) > 0 and len(evap_proxy_future) > 0:\n",
    "            # Evap trend analysis\n",
    "            report_lines.append(\"\\nEvap Analysis:\")\n",
    "            \n",
    "            evap_raw_annual = evap_proxy_future.groupby(evap_proxy_future['date'].dt.year)['value'].mean()\n",
    "            evap_corrected_annual = evap_future_corrected.groupby(evap_future_corrected['date'].dt.year)['value'].mean()\n",
    "            \n",
    "            if len(evap_raw_annual) > 1 and len(evap_corrected_annual) > 1:\n",
    "                raw_changes = evap_raw_annual.diff().dropna()\n",
    "                corrected_changes = evap_corrected_annual.diff().dropna()\n",
    "                \n",
    "                common_years = raw_changes.index.intersection(corrected_changes.index)\n",
    "                if len(common_years) > 2:\n",
    "                    trend_corr = raw_changes.loc[common_years].corr(corrected_changes.loc[common_years])\n",
    "                    \n",
    "                    if trend_corr < 0.5:\n",
    "                        report_lines.append(f\"  [ISSUE] Trends not parallel: Correlation = {trend_corr:.3f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"  [OK] Trends are parallel: Correlation = {trend_corr:.3f}\")\n",
    "        \n",
    "        # Scenario separation check (if we have multiple scenarios)\n",
    "        # Note: This would require loading both SSP245 and SSP585 data\n",
    "        report_lines.append(\"\\n  [INFO] Scenario separation check requires comparison with other scenarios\")\n",
    "        report_lines.append(\"         (SSP585 should remain more extreme than SSP245)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CHECK E: Physical Bounds & Plausibility\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"CHECK E: PHYSICAL BOUNDS & PLAUSIBILITY\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    for variable, corrected_df, var_name in [\n",
    "        ('VP', vp_obs_corrected, 'VP'),\n",
    "        ('Evap', evap_obs_corrected, 'Evap')\n",
    "    ]:\n",
    "        report_lines.append(f\"\\n{var_name} Analysis:\")\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        # Check bounds\n",
    "        negative_count = (corrected_df['value'] < 0).sum()\n",
    "        if negative_count > 0:\n",
    "            issues.append(f\"Negative values detected: {negative_count} days\")\n",
    "        \n",
    "        # Check NaNs\n",
    "        nan_count = corrected_df['value'].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            issues.append(f\"NaN values detected: {nan_count} days\")\n",
    "        \n",
    "        # Check for spikes (values > 3 standard deviations from mean)\n",
    "        mean_val = corrected_df['value'].mean()\n",
    "        std_val = corrected_df['value'].std()\n",
    "        spike_threshold = mean_val + 3 * std_val\n",
    "        spike_count = (corrected_df['value'] > spike_threshold).sum()\n",
    "        if spike_count > len(corrected_df) * 0.001:  # More than 0.1% of data\n",
    "            issues.append(f\"Potential spikes detected: {spike_count} values > 3σ above mean\")\n",
    "        \n",
    "        # Check for discontinuities at year boundaries\n",
    "        corrected_df_sorted = corrected_df.sort_values('date').copy()\n",
    "        year_changes = corrected_df_sorted.groupby(corrected_df_sorted['date'].dt.year)['value'].agg(['mean', 'std'])\n",
    "        \n",
    "        # Check for large jumps between consecutive years\n",
    "        year_means = year_changes['mean']\n",
    "        year_diffs = year_means.diff().abs()\n",
    "        large_jumps = year_diffs[year_diffs > 2 * std_val]\n",
    "        if len(large_jumps) > 0:\n",
    "            issues.append(f\"Large discontinuities at year boundaries: {len(large_jumps)} instances\")\n",
    "        \n",
    "        if issues:\n",
    "            report_lines.append(f\"  [ISSUES] Physical bounds/plausibility problems:\")\n",
    "            for issue in issues:\n",
    "                report_lines.append(f\"    - {issue}\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] All physical bounds and plausibility checks passed\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CHECK F: Cross-Variable Consistency (VP vs Evap)\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"CHECK F: CROSS-VARIABLE CONSISTENCY (VP vs Evap)\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    # Align VP and Evap dataframes\n",
    "    vp_aligned = vp_obs_corrected.set_index('date')\n",
    "    evap_aligned = evap_obs_corrected.set_index('date')\n",
    "    \n",
    "    common_dates = vp_aligned.index.intersection(evap_aligned.index)\n",
    "    \n",
    "    if len(common_dates) > 0:\n",
    "        vp_common = vp_aligned.loc[common_dates, 'value']\n",
    "        evap_common = evap_aligned.loc[common_dates, 'value']\n",
    "        \n",
    "        # Check inverse relationship (high evap should coincide with low VP)\n",
    "        # This is a simplified check - in reality, temperature also matters\n",
    "        correlation = vp_common.corr(evap_common)\n",
    "        \n",
    "        if correlation > 0.3:\n",
    "            report_lines.append(f\"  [WARNING] Unexpected positive correlation between VP and Evap: {correlation:.3f}\")\n",
    "            report_lines.append(f\"           (High evap days should generally coincide with low VP)\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] VP-Evap relationship: Correlation = {correlation:.3f} (expected negative)\")\n",
    "        \n",
    "        # Seasonal coherence check\n",
    "        vp_seasonal = vp_common.groupby(vp_common.index.month).mean()\n",
    "        evap_seasonal = evap_common.groupby(evap_common.index.month).mean()\n",
    "        \n",
    "        # Summer (Dec-Feb in Southern Hemisphere, Jun-Aug in Northern)\n",
    "        # Assuming Southern Hemisphere for now\n",
    "        summer_months = [12, 1, 2]\n",
    "        winter_months = [6, 7, 8]\n",
    "        \n",
    "        vp_summer = vp_seasonal[summer_months].mean()\n",
    "        vp_winter = vp_seasonal[winter_months].mean()\n",
    "        evap_summer = evap_seasonal[summer_months].mean()\n",
    "        evap_winter = evap_seasonal[winter_months].mean()\n",
    "        \n",
    "        if evap_summer < evap_winter:\n",
    "            report_lines.append(f\"  [ISSUE] Seasonal coherence problem: Evap summer ({evap_summer:.3f}) < winter ({evap_winter:.3f})\")\n",
    "        else:\n",
    "            report_lines.append(f\"  [OK] Seasonal coherence: Summer evap ({evap_summer:.3f}) > winter ({evap_winter:.3f})\")\n",
    "        \n",
    "        if vp_summer < vp_winter:\n",
    "            report_lines.append(f\"  [WARNING] VP seasonal pattern: Summer ({vp_summer:.3f}) < winter ({vp_winter:.3f})\")\n",
    "    else:\n",
    "        report_lines.append(f\"  [ERROR] No overlapping dates between VP and Evap data\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SUMMARY\n",
    "    # ============================================================================\n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    report_lines.append(\"SUMMARY\")\n",
    "    report_lines.append(\"-\"*80)\n",
    "    \n",
    "    # Count issues and warnings\n",
    "    total_issues = sum(1 for line in report_lines if '[ISSUE]' in line or '[ERROR]' in line)\n",
    "    total_warnings = sum(1 for line in report_lines if '[WARNING]' in line)\n",
    "    total_ok = sum(1 for line in report_lines if '[OK]' in line)\n",
    "    \n",
    "    report_lines.append(f\"\\nTotal Issues/Errors: {total_issues}\")\n",
    "    report_lines.append(f\"Total Warnings: {total_warnings}\")\n",
    "    report_lines.append(f\"Checks Passed: {total_ok}\")\n",
    "    \n",
    "    if total_issues == 0 and total_warnings == 0:\n",
    "        report_lines.append(\"\\n[OVERALL STATUS] ✓ All checks passed - Calibration appears successful\")\n",
    "    elif total_issues == 0:\n",
    "        report_lines.append(\"\\n[OVERALL STATUS] ⚠ Some warnings detected - Review recommended\")\n",
    "    else:\n",
    "        report_lines.append(\"\\n[OVERALL STATUS] ✗ Issues detected - Calibration review required\")\n",
    "    \n",
    "    report_lines.append(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return \"\\n\".join(report_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_future_trend_sanity_check(cmip6_raw_df, cmip6_corrected_df, scenario, variable='vp',\n",
    "                                   output_dir=None, model=None, lat=None, lon=None):\n",
    "    \"\"\"\n",
    "    Plot trend sanity check: raw vs corrected for future scenarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cmip6_raw_df : pd.DataFrame\n",
    "        CMIP6 raw future data (columns: date, value)\n",
    "    cmip6_corrected_df : pd.DataFrame\n",
    "        CMIP6 corrected future data (columns: date, value)\n",
    "    scenario : str\n",
    "        Scenario name (e.g., 'ssp245', 'ssp585')\n",
    "    variable : str\n",
    "        Variable name\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    # Calculate annual means\n",
    "    cmip6_raw_df['year'] = cmip6_raw_df['date'].dt.year\n",
    "    cmip6_corrected_df['year'] = cmip6_corrected_df['date'].dt.year\n",
    "    \n",
    "    raw_annual = cmip6_raw_df.groupby('year')['value'].mean().reset_index()\n",
    "    corrected_annual = cmip6_corrected_df.groupby('year')['value'].mean().reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(raw_annual['year'], raw_annual['value'], 'o-', label=f'{scenario.upper()} raw', \n",
    "            linewidth=2, markersize=6, alpha=0.7)\n",
    "    ax.plot(corrected_annual['year'], corrected_annual['value'], 's-', label=f'{scenario.upper()} corrected', \n",
    "            linewidth=2, markersize=6, alpha=0.8)\n",
    "    ax.set_xlabel('Year', fontsize=12)\n",
    "    ax.set_ylabel(f'{variable.upper()} Annual Mean ({get_unit(variable)})', fontsize=12)\n",
    "    ax.set_title(f'{model} - {scenario.upper()} Trend Check: Raw vs Corrected\\n{variable.upper()}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"{model}_{lat:.2f}_{lon:.2f}_{scenario}_trend_raw_vs_corrected_{variable}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_scenario_separation(ssp245_corrected_df, ssp585_corrected_df, \n",
    "                             ssp245_raw_df=None, ssp585_raw_df=None,\n",
    "                             variable='vp', output_dir=None, model=None, lat=None, lon=None):\n",
    "    \"\"\"\n",
    "    Plot scenario separation check.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ssp245_corrected_df : pd.DataFrame\n",
    "        SSP245 corrected data\n",
    "    ssp585_corrected_df : pd.DataFrame\n",
    "        SSP585 corrected data\n",
    "    ssp245_raw_df : pd.DataFrame, optional\n",
    "        SSP245 raw data (for faint overlay)\n",
    "    ssp585_raw_df : pd.DataFrame, optional\n",
    "        SSP585 raw data (for faint overlay)\n",
    "    variable : str\n",
    "        Variable name\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    # Calculate annual means\n",
    "    ssp245_corrected_df['year'] = ssp245_corrected_df['date'].dt.year\n",
    "    ssp585_corrected_df['year'] = ssp585_corrected_df['date'].dt.year\n",
    "    \n",
    "    ssp245_corrected_annual = ssp245_corrected_df.groupby('year')['value'].mean().reset_index()\n",
    "    ssp585_corrected_annual = ssp585_corrected_df.groupby('year')['value'].mean().reset_index()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Optional: plot raw lines faintly\n",
    "    if ssp245_raw_df is not None:\n",
    "        ssp245_raw_df['year'] = ssp245_raw_df['date'].dt.year\n",
    "        ssp245_raw_annual = ssp245_raw_df.groupby('year')['value'].mean().reset_index()\n",
    "        ax.plot(ssp245_raw_annual['year'], ssp245_raw_annual['value'], '--', \n",
    "                label='SSP245 raw', linewidth=1, alpha=0.3, color='blue')\n",
    "    \n",
    "    if ssp585_raw_df is not None:\n",
    "        ssp585_raw_df['year'] = ssp585_raw_df['date'].dt.year\n",
    "        ssp585_raw_annual = ssp585_raw_df.groupby('year')['value'].mean().reset_index()\n",
    "        ax.plot(ssp585_raw_annual['year'], ssp585_raw_annual['value'], '--', \n",
    "                label='SSP585 raw', linewidth=1, alpha=0.3, color='red')\n",
    "    \n",
    "    # Plot corrected lines prominently\n",
    "    ax.plot(ssp245_corrected_annual['year'], ssp245_corrected_annual['value'], \n",
    "            'o-', label='SSP245 corrected', linewidth=2, markersize=6, color='blue', alpha=0.8)\n",
    "    ax.plot(ssp585_corrected_annual['year'], ssp585_corrected_annual['value'], \n",
    "            's-', label='SSP585 corrected', linewidth=2, markersize=6, color='red', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Year', fontsize=12)\n",
    "    ax.set_ylabel(f'{variable.upper()} Annual Mean ({get_unit(variable)})', fontsize=12)\n",
    "    ax.set_title(f'{model} - Scenario Separation Check\\n{variable.upper()}', fontsize=14)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        filename = f\"{model}_{lat:.2f}_{lon:.2f}_scenario_separation_{variable}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        plt.savefig(filepath, dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_future_variance_preservation(cmip6_raw_df, cmip6_corrected_df, scenario, variable='vp',\n",
    "                                     window_days=30, output_dir=None, model=None, lat=None, lon=None):\n",
    "    \"\"\"\n",
    "    Plot rolling standard deviation to check variance preservation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cmip6_raw_df : pd.DataFrame\n",
    "        CMIP6 raw future data\n",
    "    cmip6_corrected_df : pd.DataFrame\n",
    "        CMIP6 corrected future data\n",
    "    scenario : str\n",
    "        Scenario name\n",
    "    variable : str\n",
    "        Variable name\n",
    "    window_days : int\n",
    "        Rolling window size in days\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model : str\n",
    "        Model name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    \"\"\"\n",
    "    # Sort by date\n",
    "    cmip6_raw_df = cmip6_raw_df.sort_values('date').reset_index(drop=True)\n",
    "    cmip6_corrected_df = cmip6_corrected_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Check for missing data and constant values\n",
    "    raw_missing = cmip6_raw_df['value'].isna().sum()\n",
    "    corrected_missing = cmip6_corrected_df['value'].isna().sum()\n",
    "    \n",
    "    if raw_missing > 0 or corrected_missing > 0:\n",
    "        print(f\"    [WARNING] Missing data detected: raw={raw_missing}, corrected={corrected_missing}\")\n",
    "        # Fill NaN values with forward fill then backward fill to handle gaps\n",
    "        cmip6_raw_df['value'] = cmip6_raw_df['value'].fillna(method='ffill').fillna(method='bfill')\n",
    "        cmip6_corrected_df['value'] = cmip6_corrected_df['value'].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Calculate rolling std with min_periods to handle edge cases and missing data\n",
    "    # Use min_periods=window_days//2 to require at least half the window for valid std\n",
    "    min_periods = max(1, window_days // 2)\n",
    "    cmip6_raw_df['rolling_std'] = cmip6_raw_df['value'].rolling(\n",
    "        window=window_days, center=True, min_periods=min_periods\n",
    "    ).std()\n",
    "    cmip6_corrected_df['rolling_std'] = cmip6_corrected_df['value'].rolling(\n",
    "        window=window_days, center=True, min_periods=min_periods\n",
    "    ).std()\n",
    "    \n",
    "    # Check for NaN values in rolling_std (from edge effects or insufficient data)\n",
    "    raw_nan_std = cmip6_raw_df['rolling_std'].isna().sum()\n",
    "    corrected_nan_std = cmip6_corrected_df['rolling_std'].isna().sum()\n",
    "    \n",
    "    if raw_nan_std > 0 or corrected_nan_std > 0:\n",
    "        print(f\"    [INFO] NaN rolling std values: raw={raw_nan_std}, corrected={corrected_nan_std}\")\n",
    "        # Find date ranges with NaN values\n",
    "        if raw_nan_std > 0:\n",
    "            nan_dates_raw = cmip6_raw_df[cmip6_raw_df['rolling_std'].isna()]['date']\n",
    "            if len(nan_dates_raw) > 0:\n",
    "                print(f\"      Raw NaN period: {nan_dates_raw.min()} to {nan_dates_raw.max()}\")\n",
    "        if corrected_nan_std > 0:\n",
    "            nan_dates_corrected = cmip6_corrected_df[cmip6_corrected_df['rolling_std'].isna()]['date']\n",
    "            if len(nan_dates_corrected) > 0:\n",
    "                print(f\"      Corrected NaN period: {nan_dates_corrected.min()} to {nan_dates_corrected.max()}\")\n",
    "    \n",
    "    # Check for constant values (zero variance) that would result in flat lines\n",
    "    raw_constant = (cmip6_raw_df['rolling_std'] == 0).sum()\n",
    "    corrected_constant = (cmip6_corrected_df['rolling_std'] == 0).sum()\n",
    "    \n",
    "    # Also check for very low variance (near-zero, might appear flat)\n",
    "    raw_low_var = (cmip6_raw_df['rolling_std'] < 0.01).sum()\n",
    "    corrected_low_var = (cmip6_corrected_df['rolling_std'] < 0.01).sum()\n",
    "    \n",
    "    if raw_constant > 0 or corrected_constant > 0:\n",
    "        print(f\"    [INFO] Zero variance periods: raw={raw_constant}, corrected={corrected_constant}\")\n",
    "        # Find date ranges with constant values\n",
    "        if raw_constant > 0:\n",
    "            constant_dates_raw = cmip6_raw_df[cmip6_raw_df['rolling_std'] == 0]['date']\n",
    "            if len(constant_dates_raw) > 0:\n",
    "                print(f\"      Raw constant period: {constant_dates_raw.min()} to {constant_dates_raw.max()}\")\n",
    "                # Check if the actual values are constant\n",
    "                constant_period_raw = cmip6_raw_df[cmip6_raw_df['rolling_std'] == 0]\n",
    "                if len(constant_period_raw) > 0:\n",
    "                    unique_values = constant_period_raw['value'].nunique()\n",
    "                    print(f\"        Unique values in constant period: {unique_values}\")\n",
    "        if corrected_constant > 0:\n",
    "            constant_dates_corrected = cmip6_corrected_df[cmip6_corrected_df['rolling_std'] == 0]['date']\n",
    "            if len(constant_dates_corrected) > 0:\n",
    "                print(f\"      Corrected constant period: {constant_dates_corrected.min()} to {constant_dates_corrected.max()}\")\n",
    "                # Check if the actual values are constant\n",
    "                constant_period_corrected = cmip6_corrected_df[cmip6_corrected_df['rolling_std'] == 0]\n",
    "                if len(constant_period_corrected) > 0:\n",
    "                    unique_values = constant_period_corrected['value'].nunique()\n",
    "                    print(f\"        Unique values in constant period: {unique_values}\")\n",
    "    \n",
    "    if raw_low_var > raw_constant or corrected_low_var > corrected_constant:\n",
    "        print(f\"    [INFO] Very low variance periods (<0.01): raw={raw_low_var}, corrected={corrected_low_var}\")\n",
    "    \n",
    "    # Check data quality in early period (before 2044)\n",
    "    early_cutoff = pd.to_datetime('2044-01-01')\n",
    "    raw_early = cmip6_raw_df[cmip6_raw_df['date'] < early_cutoff]\n",
    "    corrected_early = cmip6_corrected_df[cmip6_corrected_df['date'] < early_cutoff]\n",
    "    \n",
    "    if len(raw_early) > 0:\n",
    "        raw_early_nan = raw_early['rolling_std'].isna().sum()\n",
    "        raw_early_zero = (raw_early['rolling_std'] == 0).sum()\n",
    "        raw_early_low = (raw_early['rolling_std'] < 0.01).sum()\n",
    "        raw_early_mean_std = raw_early['rolling_std'].mean()\n",
    "        print(f\"    [INFO] Early period (before 2044) - Raw: NaN={raw_early_nan}, Zero={raw_early_zero}, Low={raw_early_low}, Mean_std={raw_early_mean_std:.4f}\")\n",
    "    \n",
    "    if len(corrected_early) > 0:\n",
    "        corrected_early_nan = corrected_early['rolling_std'].isna().sum()\n",
    "        corrected_early_zero = (corrected_early['rolling_std'] == 0).sum()\n",
    "        corrected_early_low = (corrected_early['rolling_std'] < 0.01).sum()\n",
    "        corrected_early_mean_std = corrected_early['rolling_std'].mean()\n",
    "        print(f\"    [INFO] Early period (before 2044) - Corrected: NaN={corrected_early_nan}, Zero={corrected_early_zero}, Low={corrected_early_low}, Mean_std={corrected_early_mean_std:.4f}\")\n",
    "    \n",
    "    # Plot - filter out NaN values to avoid flat lines from missing data\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot only non-NaN values\n",
    "    raw_plot_data = cmip6_raw_df[cmip6_raw_df['rolling_std'].notna()]\n",
    "    corrected_plot_data = cmip6_corrected_df[cmip6_corrected_df['rolling_std'].notna()]\n",
    "    \n",
    "    ax.plot(raw_plot_data['date'], raw_plot_data['rolling_std'], \n",
    "            label=f'{scenario.upper()} raw', linewidth=1.5, alpha=0.7)\n",
    "    ax.plot(corrected_plot_data['date'], corrected_plot_data['rolling_std'], \n",
    "            label=f'{scenario.upper()} corrected', linewidth=1.5, alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Diagnostic Validation Function\n",
    "\n",
    "This function orchestrates all diagnostic checks for a single target (model/scenario/coordinate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_diagnostic_validation(silo_vp_df, silo_evap_df,\n",
    "                              vp_proxy_obs, evap_proxy_obs,\n",
    "                              vp_calibration, evap_calibration,\n",
    "                              vp_proxy_future, evap_proxy_future,\n",
    "                              original_met_data,\n",
    "                              baseline_start, baseline_end,\n",
    "                              model, scenario, lat, lon, output_dir):\n",
    "    \"\"\"\n",
    "    Run comprehensive diagnostic validation for baseline and future scenarios.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    silo_vp_df : pd.DataFrame\n",
    "        SILO VP baseline data\n",
    "    silo_evap_df : pd.DataFrame\n",
    "        SILO Evap baseline data\n",
    "    vp_proxy_obs : pd.DataFrame\n",
    "        CMIP6 obs VP raw proxy\n",
    "    evap_proxy_obs : pd.DataFrame\n",
    "        CMIP6 obs Evap raw proxy\n",
    "    vp_calibration : dict\n",
    "        VP calibration results (contains 'obs', 'future', 'silo_stats', 'cmip6_obs_stats')\n",
    "    evap_calibration : dict\n",
    "        Evap calibration results\n",
    "    vp_proxy_future : pd.DataFrame\n",
    "        CMIP6 future VP raw proxy\n",
    "    evap_proxy_future : pd.DataFrame\n",
    "        CMIP6 future Evap raw proxy\n",
    "    original_met_data : pd.DataFrame\n",
    "        Original .met file data\n",
    "    baseline_start : str\n",
    "        Baseline start date\n",
    "    baseline_end : str\n",
    "        Baseline end date\n",
    "    model : str\n",
    "        Model name\n",
    "    scenario : str\n",
    "        Scenario name\n",
    "    lat : float\n",
    "        Latitude\n",
    "    lon : float\n",
    "        Longitude\n",
    "    output_dir : str\n",
    "        Base output directory\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Summary of validation results\n",
    "    \"\"\"\n",
    "    # Create diagnostics output directory structure\n",
    "    diagnostics_dir = os.path.join(output_dir, 'diagnostics', model, f\"{lat:.2f}_{lon:.2f}\")\n",
    "    os.makedirs(diagnostics_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n[DIAGNOSTICS] Running validation for {model} {scenario} ({lat}, {lon})\")\n",
    "    print(f\"  Output directory: {diagnostics_dir}\")\n",
    "    \n",
    "    validation_summary = {}\n",
    "    \n",
    "    # A) Baseline Validation Diagnostics\n",
    "    print(f\"\\n  [A] Baseline Validation (1986-2014)...\")\n",
    "    \n",
    "    for variable in ['vp', 'evap']:\n",
    "        if variable == 'vp':\n",
    "            silo_df = silo_vp_df\n",
    "            cmip6_obs_raw = vp_proxy_obs\n",
    "            cmip6_obs_corrected = vp_calibration['obs']\n",
    "        else:\n",
    "            silo_df = silo_evap_df\n",
    "            cmip6_obs_raw = evap_proxy_obs\n",
    "            cmip6_obs_corrected = evap_calibration['obs']\n",
    "        \n",
    "        # A1) Monthly stats table\n",
    "        try:\n",
    "            stats_df, deltas_df = create_baseline_monthly_stats_table(\n",
    "                silo_df, cmip6_obs_raw, cmip6_obs_corrected,\n",
    "                baseline_start, baseline_end, variable=variable\n",
    "            )\n",
    "            \n",
    "            # Display in notebook\n",
    "            print(f\"\\n    {variable.upper()} - Monthly Statistics Table:\")\n",
    "            print(stats_df.to_string(index=False))\n",
    "            print(f\"\\n    {variable.upper()} - Post-Correction Deltas:\")\n",
    "            print(deltas_df.to_string(index=False))\n",
    "            \n",
    "            # Save CSV\n",
    "            stats_filename = f\"{model}_{lat:.2f}_{lon:.2f}_baseline_monthly_stats_{variable}.csv\"\n",
    "            stats_path = os.path.join(diagnostics_dir, stats_filename)\n",
    "            stats_df.to_csv(stats_path, index=False)\n",
    "            print(f\"    [OK] Saved: {stats_filename}\")\n",
    "            \n",
    "            validation_summary[f'{variable}_baseline_stats'] = stats_df\n",
    "            validation_summary[f'{variable}_baseline_deltas'] = deltas_df\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to create monthly stats table: {e}\")\n",
    "        \n",
    "        # A2) Monthly climatology plots\n",
    "        try:\n",
    "            plot_baseline_monthly_climatology(\n",
    "                silo_df, cmip6_obs_raw, cmip6_obs_corrected,\n",
    "                baseline_start, baseline_end, variable=variable,\n",
    "                output_dir=diagnostics_dir, model=model, lat=lat, lon=lon\n",
    "            )\n",
    "            print(f\"    [OK] Created monthly climatology plots for {variable}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to create monthly climatology plots: {e}\")\n",
    "        \n",
    "        # A3) Daily time-series overlay\n",
    "        try:\n",
    "            plot_baseline_daily_overlay(\n",
    "                silo_df, cmip6_obs_raw, cmip6_obs_corrected,\n",
    "                baseline_start, baseline_end, variable=variable,\n",
    "                output_dir=diagnostics_dir, model=model, lat=lat, lon=lon\n",
    "            )\n",
    "            print(f\"    [OK] Created daily overlay plot for {variable}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to create daily overlay plot: {e}\")\n",
    "        \n",
    "        # A4) Distribution check\n",
    "        try:\n",
    "            plot_baseline_distribution(\n",
    "                silo_df, cmip6_obs_raw, cmip6_obs_corrected,\n",
    "                baseline_start, baseline_end, variable=variable,\n",
    "                output_dir=diagnostics_dir, model=model, lat=lat, lon=lon\n",
    "            )\n",
    "            print(f\"    [OK] Created distribution plot for {variable}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to create distribution plot: {e}\")\n",
    "    \n",
    "    # B) Future Scenario Checks\n",
    "    print(f\"\\n  [B] Future Scenario Checks...\")\n",
    "    \n",
    "    for variable in ['vp', 'evap']:\n",
    "        if variable == 'vp':\n",
    "            cmip6_future_raw = vp_proxy_future\n",
    "            cmip6_future_corrected = vp_calibration['future']\n",
    "        else:\n",
    "            cmip6_future_raw = evap_proxy_future\n",
    "            cmip6_future_corrected = evap_calibration['future']\n",
    "        \n",
    "        # B1) Trend sanity check\n",
    "        try:\n",
    "            plot_future_trend_sanity_check(\n",
    "                cmip6_future_raw, cmip6_future_corrected, scenario,\n",
    "                variable=variable, output_dir=diagnostics_dir,\n",
    "                model=model, lat=lat, lon=lon\n",
    "            )\n",
    "            print(f\"    [OK] Created trend check plot for {variable} {scenario}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to create trend check plot: {e}\")\n",
    "        \n",
    "        # B3) Variance preservation\n",
    "        try:\n",
    "            plot_future_variance_preservation(\n",
    "                cmip6_future_raw, cmip6_future_corrected, scenario,\n",
    "                variable=variable, output_dir=diagnostics_dir,\n",
    "                model=model, lat=lat, lon=lon\n",
    "            )\n",
    "            print(f\"    [OK] Created variance preservation plot for {variable} {scenario}\")\n",
    "        except Exception as e:\n",
    "            print(f\"    [ERROR] Failed to create variance preservation plot: {e}\")\n",
    "    \n",
    "    # B2) Scenario separation (only if we have both SSP245 and SSP585)\n",
    "    # Note: This requires both scenarios to be processed. For now, we'll skip if only one scenario is available.\n",
    "    # This can be enhanced later to check for other scenario files in the output directory.\n",
    "    \n",
    "    # C) Physical bounds + data integrity checks\n",
    "    print(f\"\\n  [C] Physical Bounds + Data Integrity Checks...\")\n",
    "    try:\n",
    "        validation_result = validate_physical_bounds_and_integrity(\n",
    "            vp_calibration['future'],\n",
    "            evap_calibration['future'],\n",
    "            original_met_data,\n",
    "            model, scenario, lat, lon\n",
    "        )\n",
    "        print(f\"    [OK] All validation checks passed\")\n",
    "        validation_summary['integrity_check'] = validation_result\n",
    "    except ValueError as e:\n",
    "        print(f\"    [ERROR] Validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"    [ERROR] Validation check error: {e}\")\n",
    "    \n",
    "    print(f\"\\n  [OK] Diagnostic validation completed for {model} {scenario} ({lat}, {lon})\")\n",
    "    \n",
    "    return validation_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Main Processing Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Loading template .met file...\n",
      "======================================================================\n",
      "  [OK] Template loaded successfully\n",
      "\n",
      "======================================================================\n",
      "Processing 3 target(s)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Target 1/3: ACCESS_CM2 obs (-31.45, 117.55)\n",
      "======================================================================\n",
      "\n",
      "[STEP 1] Finding required files...\n",
      "  [ERROR] Missing required files:\n",
      "    - silo_met\n",
      "  [SKIP] Missing required files for ACCESS_CM2 obs (-31.45, 117.55)\n",
      "  [INFO] Please ensure all required files exist and run the corresponding calculation notebooks first\n",
      "\n",
      "======================================================================\n",
      "Target 2/3: ACCESS_CM2 ssp245 (-31.45, 117.55)\n",
      "======================================================================\n",
      "\n",
      "[STEP 1] Finding required files...\n",
      "  [ERROR] Missing required files:\n",
      "    - silo_met\n",
      "  [SKIP] Missing required files for ACCESS_CM2 ssp245 (-31.45, 117.55)\n",
      "  [INFO] Please ensure all required files exist and run the corresponding calculation notebooks first\n",
      "\n",
      "======================================================================\n",
      "Target 3/3: ACCESS_CM2 ssp585 (-31.45, 117.55)\n",
      "======================================================================\n",
      "\n",
      "[STEP 1] Finding required files...\n",
      "  [ERROR] Missing required files:\n",
      "    - silo_met\n",
      "  [SKIP] Missing required files for ACCESS_CM2 ssp585 (-31.45, 117.55)\n",
      "  [INFO] Please ensure all required files exist and run the corresponding calculation notebooks first\n",
      "\n",
      "======================================================================\n",
      "[COMPLETE] All targets processed!\n",
      "======================================================================\n",
      "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.45_117.55_Climate Files\n"
     ]
    }
   ],
   "source": [
    "# Main Processing Loop\n",
    "# ============================================================================\n",
    "\n",
    "# 1. Generate targets from configuration (one model, one scenario, multiple coordinates)\n",
    "targets = generate_targets(MODEL, SCENARIOS, COORDINATES)\n",
    "\n",
    "# 2. Load template\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Loading template .met file...\")\n",
    "print(f\"{'='*70}\")\n",
    "try:\n",
    "    template_header = parse_template_met(TEMPLATE_MET_FILE)\n",
    "    print(f\"  [OK] Template loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"  [ERROR] Failed to load template: {e}\")\n",
    "    raise\n",
    "\n",
    "# 3. Process each coordinate for the configured model/scenario\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Processing {len(targets)} target(s)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "for target_idx, target in enumerate(targets, 1):\n",
    "    model, scenario, lat, lon = target['model'], target['scenario'], target['lat'], target['lon']\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Target {target_idx}/{len(targets)}: {model} {scenario} ({lat}, {lon})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        # Find files (with error handling - skip if files missing)\n",
    "        print(f\"\\n[STEP 1] Finding required files...\")\n",
    "        print(f\"  [DEBUG] Searching in: {INPUT_MET_DIR}\")\n",
    "        print(f\"  [DEBUG] Looking for: {model} {scenario} at ({lat}, {lon})\")\n",
    "        files = find_files_for_target(model, scenario, lat, lon, INPUT_MET_DIR)\n",
    "        \n",
    "        # Debug: Show what was found\n",
    "        print(f\"  [DEBUG] Files found:\")\n",
    "        for key, path in files.items():\n",
    "            if path:\n",
    "                print(f\"    ✓ {key}: {os.path.basename(path)}\")\n",
    "            else:\n",
    "                print(f\"    ✗ {key}: NOT FOUND\")\n",
    "        \n",
    "        # Check if all required files exist\n",
    "        if not all_required_files_exist(files, scenario):\n",
    "            print(f\"  [SKIP] Missing required files for {model} {scenario} ({lat}, {lon})\")\n",
    "            print(f\"  [INFO] Please ensure all required files exist and run the corresponding calculation notebooks first\")\n",
    "            print(f\"  [DEBUG] Searched directory: {INPUT_MET_DIR}\")\n",
    "            # List some example files in the directory to help debug\n",
    "            try:\n",
    "                dir_files = os.listdir(INPUT_MET_DIR)\n",
    "                example_files = [f for f in dir_files if model.lower() in f.lower() or 'silo' in f.lower()][:5]\n",
    "                if example_files:\n",
    "                    print(f\"  [DEBUG] Example files in directory:\")\n",
    "                    for f in example_files:\n",
    "                        print(f\"    - {f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  [DEBUG] Could not list directory: {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  [OK] All required files found\")\n",
    "        \n",
    "        # Load data\n",
    "        print(f\"\\n[STEP 2] Loading data files...\")\n",
    "        try:\n",
    "            silo_header, silo_met = load_met_file(files['silo_met'])\n",
    "            print(f\"  [OK] SILO .met: {len(silo_met):,} rows\")\n",
    "            \n",
    "            cmip6_obs_header, cmip6_obs_met = load_met_file(files['cmip6_obs_met'])\n",
    "            print(f\"  [OK] CMIP6 obs .met: {len(cmip6_obs_met):,} rows\")\n",
    "            \n",
    "            cmip6_future_header, cmip6_future_met = load_met_file(files['cmip6_future_met'])\n",
    "            if len(cmip6_future_met) > 0:\n",
    "                future_date_min = cmip6_future_met['date'].min()\n",
    "                future_date_max = cmip6_future_met['date'].max()\n",
    "                print(f\"  [OK] CMIP6 future .met: {len(cmip6_future_met):,} rows\")\n",
    "                print(f\"      Date range: {future_date_min.date()} to {future_date_max.date()}\")\n",
    "            else:\n",
    "                print(f\"  [OK] CMIP6 future .met: {len(cmip6_future_met):,} rows\")\n",
    "            \n",
    "            vp_proxy_obs = load_proxy_csv(files['vp_obs'], variable='vp')\n",
    "            print(f\"  [OK] VP obs proxy: {len(vp_proxy_obs):,} rows\")\n",
    "            \n",
    "            vp_proxy_future = load_proxy_csv(files['vp_future'], variable='vp')\n",
    "            print(f\"  [OK] VP future proxy: {len(vp_proxy_future):,} rows\")\n",
    "            \n",
    "            evap_proxy_obs = load_proxy_csv(files['evap_obs'], variable='evap')\n",
    "            print(f\"  [OK] Evap obs proxy: {len(evap_proxy_obs):,} rows\")\n",
    "            \n",
    "            evap_proxy_future = load_proxy_csv(files['evap_future'], variable='evap')\n",
    "            print(f\"  [OK] Evap future proxy: {len(evap_proxy_future):,} rows\")\n",
    "            \n",
    "            # Check date range alignment between .met file and proxy CSV\n",
    "            if scenario.lower() != 'obs' and len(vp_proxy_future) > 0 and len(cmip6_future_met) > 0:\n",
    "                proxy_date_min = vp_proxy_future['date'].min()\n",
    "                proxy_date_max = vp_proxy_future['date'].max()\n",
    "                met_date_min = cmip6_future_met['date'].min()\n",
    "                met_date_max = cmip6_future_met['date'].max()\n",
    "                \n",
    "                print(f\"\\n  [INFO] Date range comparison:\")\n",
    "                print(f\"      Proxy CSV: {proxy_date_min.date()} to {proxy_date_max.date()}\")\n",
    "                print(f\"      .met file:  {met_date_min.date()} to {met_date_max.date()}\")\n",
    "                \n",
    "                # Filter .met file to match proxy CSV date range if they don't match\n",
    "                if met_date_min < proxy_date_min or met_date_max > proxy_date_max:\n",
    "                    print(f\"  [INFO] Filtering .met file to match proxy CSV date range...\")\n",
    "                    cmip6_future_met = cmip6_future_met[\n",
    "                        (cmip6_future_met['date'] >= proxy_date_min) & \n",
    "                        (cmip6_future_met['date'] <= proxy_date_max)\n",
    "                    ].copy()\n",
    "                    print(f\"  [OK] Filtered .met file: {len(cmip6_future_met):,} rows\")\n",
    "                    print(f\"      Filtered date range: {cmip6_future_met['date'].min().date()} to {cmip6_future_met['date'].max().date()}\")\n",
    "                elif met_date_min == proxy_date_min and met_date_max == proxy_date_max:\n",
    "                    print(f\"  [OK] Date ranges match perfectly\")\n",
    "                else:\n",
    "                    print(f\"  [WARNING] Date ranges partially overlap - may cause issues\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Failed to load data files: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract VP/Evap from .met files\n",
    "        print(f\"\\n[STEP 3] Extracting VP and Evap from .met files...\")\n",
    "        silo_vp = silo_met[['date', 'vp']].copy()\n",
    "        silo_vp = silo_vp.rename(columns={'vp': 'value'})\n",
    "        silo_evap = silo_met[['date', 'evap']].copy()\n",
    "        silo_evap = silo_evap.rename(columns={'evap': 'value'})\n",
    "        \n",
    "        # Filter to baseline period for SILO\n",
    "        baseline_start_dt = pd.to_datetime(BASELINE_START)\n",
    "        baseline_end_dt = pd.to_datetime(BASELINE_END)\n",
    "        silo_vp = silo_vp[(silo_vp['date'] >= baseline_start_dt) & (silo_vp['date'] <= baseline_end_dt)]\n",
    "        silo_evap = silo_evap[(silo_evap['date'] >= baseline_start_dt) & (silo_evap['date'] <= baseline_end_dt)]\n",
    "        \n",
    "        print(f\"  [OK] SILO VP: {len(silo_vp):,} baseline rows\")\n",
    "        print(f\"  [OK] SILO Evap: {len(silo_evap):,} baseline rows\")\n",
    "        \n",
    "        # Calibrate VP\n",
    "        print(f\"\\n[STEP 4] Calibrating VP...\")\n",
    "        try:\n",
    "            vp_calibration = calibrate_vp(\n",
    "                silo_vp, \n",
    "                vp_proxy_obs, \n",
    "                vp_proxy_future, \n",
    "                BASELINE_START, \n",
    "                BASELINE_END\n",
    "            )\n",
    "            print(f\"  [OK] VP calibration completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] VP calibration failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Calibrate Evap\n",
    "        print(f\"\\n[STEP 5] Calibrating Evap...\")\n",
    "        try:\n",
    "            evap_calibration = calibrate_evap(\n",
    "                silo_evap, \n",
    "                evap_proxy_obs, \n",
    "                evap_proxy_future, \n",
    "                BASELINE_START, \n",
    "                BASELINE_END\n",
    "            )\n",
    "            print(f\"  [OK] Evap calibration completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Evap calibration failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Write calibrated .met file\n",
    "        print(f\"\\n[STEP 6] Writing calibrated .met file...\")\n",
    "        try:\n",
    "            lat_str = f\"{lat:.2f}\".replace('-', 'neg')\n",
    "            lon_str = f\"{lon:.2f}\"\n",
    "            output_filename = f\"{model}_{scenario}_{lat_str}_{lon_str}_calibrated.met\"\n",
    "            output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "            \n",
    "            # Select correct met file and calibration results based on scenario\n",
    "            if scenario.lower() == \"obs\":\n",
    "                original_met = cmip6_obs_met\n",
    "                calibrated_vp_data = vp_calibration['obs']\n",
    "                calibrated_evap_data = evap_calibration['obs']\n",
    "            else:\n",
    "                original_met = cmip6_future_met\n",
    "                calibrated_vp_data = vp_calibration['future']\n",
    "                calibrated_evap_data = evap_calibration['future']\n",
    "            \n",
    "            write_calibrated_met(\n",
    "                original_met,\n",
    "                calibrated_vp_data,\n",
    "                calibrated_evap_data,\n",
    "                template_header,\n",
    "                output_path,\n",
    "                model,\n",
    "                scenario,\n",
    "                lat,\n",
    "                lon\n",
    "            )\n",
    "            print(f\"  [OK] Calibrated .met file created: {output_filename}\")\n",
    "            \n",
    "            # Also write raw (non-calibrated) .met file\n",
    "            raw_output_filename = f\"{model}_{scenario}_{lat_str}_{lon_str}.met\"\n",
    "            raw_output_path = os.path.join(OUTPUT_DIR, raw_output_filename)\n",
    "            \n",
    "            # Select correct raw proxy data based on scenario\n",
    "            if scenario.lower() == \"obs\":\n",
    "                raw_vp_data = vp_proxy_obs.copy()\n",
    "                raw_evap_data = evap_proxy_obs.copy()\n",
    "            else:\n",
    "                raw_vp_data = vp_proxy_future.copy()\n",
    "                raw_evap_data = evap_proxy_future.copy()\n",
    "            \n",
    "            # Ensure raw data has 'date' and 'value' columns\n",
    "            if 'date' not in raw_vp_data.columns or 'value' not in raw_vp_data.columns:\n",
    "                # If columns are different, try to rename\n",
    "                if len(raw_vp_data.columns) == 2:\n",
    "                    raw_vp_data.columns = ['date', 'value']\n",
    "            if 'date' not in raw_evap_data.columns or 'value' not in raw_evap_data.columns:\n",
    "                if len(raw_evap_data.columns) == 2:\n",
    "                    raw_evap_data.columns = ['date', 'value']\n",
    "            \n",
    "            write_calibrated_met(\n",
    "                original_met,\n",
    "                raw_vp_data,\n",
    "                raw_evap_data,\n",
    "                template_header,\n",
    "                raw_output_path,\n",
    "                model,\n",
    "                scenario,\n",
    "                lat,\n",
    "                lon\n",
    "            )\n",
    "            print(f\"  [OK] Raw .met file created: {raw_output_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Failed to write calibrated .met file: {e}\")\n",
    "            print(f\"  [SKIP] Skipping diagnostics - .met file creation failed\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        \n",
    "        # Generate diagnostics and report\n",
    "        print(f\"\\n[STEP 7] Generating diagnostics and report...\")\n",
    "        try:\n",
    "            diagnostics_vp = generate_diagnostics(\n",
    "                silo_vp,\n",
    "                vp_proxy_obs,\n",
    "                vp_calibration['obs'],\n",
    "                vp_calibration['future'],\n",
    "                vp_calibration['silo_stats'],\n",
    "                vp_calibration['cmip6_obs_stats'],\n",
    "                variable='vp',\n",
    "                baseline_start=BASELINE_START,\n",
    "                baseline_end=BASELINE_END\n",
    "            )\n",
    "            \n",
    "            diagnostics_evap = generate_diagnostics(\n",
    "                silo_evap,\n",
    "                evap_proxy_obs,\n",
    "                evap_calibration['obs'],\n",
    "                evap_calibration['future'],\n",
    "                evap_calibration['silo_stats'],\n",
    "                evap_calibration['cmip6_obs_stats'],\n",
    "                variable='evap',\n",
    "                baseline_start=BASELINE_START,\n",
    "                baseline_end=BASELINE_END\n",
    "            )\n",
    "            \n",
    "            write_calibration_report(\n",
    "                model, lat, lon,\n",
    "                diagnostics_vp, diagnostics_evap,\n",
    "                OUTPUT_DIR,\n",
    "                BASELINE_START, BASELINE_END\n",
    "            )\n",
    "            \n",
    "            print(f\"  [OK] Diagnostics and report generated\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [WARNING] Failed to generate diagnostics: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "        # Run comprehensive diagnostic validation\n",
    "        print(f\"\\n[STEP 8] Running diagnostic validation...\")\n",
    "        try:\n",
    "            validation_summary = run_diagnostic_validation(\n",
    "                silo_vp,\n",
    "                silo_evap,\n",
    "                vp_proxy_obs,\n",
    "                evap_proxy_obs,\n",
    "                vp_calibration,\n",
    "                evap_calibration,\n",
    "                vp_proxy_future,\n",
    "                evap_proxy_future,\n",
    "                cmip6_future_met,\n",
    "                BASELINE_START,\n",
    "                BASELINE_END,\n",
    "                model,\n",
    "                scenario,\n",
    "                lat,\n",
    "                lon,\n",
    "                OUTPUT_DIR\n",
    "            )\n",
    "            print(f\"  [OK] Diagnostic validation completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Diagnostic validation failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            # Don't continue - validation errors are critical\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"[SUCCESS] Completed processing: {model} {scenario} ({lat}, {lon})\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n  [ERROR] Unexpected error processing target: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[COMPLETE] All targets processed!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
