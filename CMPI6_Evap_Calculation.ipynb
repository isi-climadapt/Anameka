{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMIP6 ET₀ Calculation - Reference Evapotranspiration\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook calculates **daily reference evapotranspiration (ET₀)** from CMIP6 climate data using the **FAO-56 Penman-Monteith** method.\n",
    "\n",
    "## Purpose\n",
    "\n",
    "ET₀ represents the atmospheric evaporative demand and is a physically-based evaporation proxy calculated from CMIP6 meteorological variables. ET₀ can be used as input for further calibration to pan evaporation in a separate process.\n",
    "\n",
    "## Method\n",
    "\n",
    "From CMIP6 variables:\n",
    "- **tasmax, tasmin** → temperature (used to calculate mean temperature)\n",
    "- **hurs** → mean relative humidity (used to calculate vapor pressure)\n",
    "- **rsds** → incoming solar radiation (converted to MJ/m²/day)\n",
    "- **sfcWind** → wind speed at 10m (adjusted to 2m height)\n",
    "\n",
    "These variables are used to compute **reference evapotranspiration (ET₀)** using the **FAO-56 Penman-Monteith** formulation.\n",
    "\n",
    "**ET₀ Definition:**\n",
    "- ET₀ (mm/day) = physically based evaporation driven by radiation + temperature + humidity + wind\n",
    "- ET₀ represents the atmospheric evaporative demand\n",
    "- ET₀ is a clean, model-consistent evaporation signal\n",
    "\n",
    "## Input Variables (CMIP6)\n",
    "\n",
    "- **tasmax**: Daily maximum temperature (°C) - required\n",
    "- **tasmin**: Daily minimum temperature (°C) - required\n",
    "- **hurs**: Daily mean relative humidity (%) - required\n",
    "- **rsds**: Surface downwelling shortwave radiation (W/m²) - required\n",
    "- **sfcWind**: Wind speed at 10m (m/s) - required\n",
    "\n",
    "## Output\n",
    "\n",
    "- **eto**: Daily reference evapotranspiration (mm/day) - calculated using FAO-56 Penman-Monteith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5: Caching Functions\n",
    "\n",
    "To optimize performance, extracted NetCDF data is cached to CSV files. On subsequent runs, \n",
    "cached data is loaded automatically instead of re-extracting from NetCDF files.\n",
    "\n",
    "**Cache Location:** Cached files are saved in the output directory with naming convention:\n",
    "`{model_scenario}_{lat_str}_{lon_str}_{variable}.csv`\n",
    "\n",
    "**Cache Behavior:**\n",
    "- If cache exists and is valid → Load from cache (fast)\n",
    "- If cache doesn't exist or is invalid → Extract from NetCDF files and save to cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_variable_path(output_dir, model_scenario, lat_str, lon_str, variable):\n",
    "    \"\"\"\n",
    "    Generate the path for a cached variable CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    output_dir : str\n",
    "        Output directory\n",
    "    model_scenario : str\n",
    "        Model and scenario string (e.g., \"ACCESS_CM2_SSP245\")\n",
    "    lat_str : str\n",
    "        Latitude formatted as string (e.g., \"-31.75\")\n",
    "    lon_str : str\n",
    "        Longitude formatted as string (e.g., \"117.60\")\n",
    "    variable : str\n",
    "        Variable name (e.g., \"tasmax\", \"hurs\", \"sfcWind\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Path to cached CSV file\n",
    "    \"\"\"\n",
    "    cache_filename = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
    "    cache_path = os.path.join(output_dir, cache_filename)\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "def load_cached_variable(cache_path):\n",
    "    \"\"\"\n",
    "    Load cached variable data from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cache_path : str\n",
    "        Path to cached CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame or None\n",
    "        DataFrame with date and value columns if file exists and is valid, None otherwise\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cache_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(cache_path)\n",
    "        if 'date' not in df.columns or 'value' not in df.columns:\n",
    "            print(f\"  [WARNING] Cached file missing required columns, will re-extract\")\n",
    "            return None\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Basic validation - check if file has data\n",
    "        if len(df) == 0:\n",
    "            print(f\"  [WARNING] Cached file is empty, will re-extract\")\n",
    "            return None\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Error loading cached file: {e}, will re-extract\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_cached_variable(df, cache_path):\n",
    "    \"\"\"\n",
    "    Save extracted variable data to CSV cache file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with date and value columns\n",
    "    cache_path : str\n",
    "        Path to save cached CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df[['date', 'value']].to_csv(\n",
    "            cache_path,\n",
    "            index=False,\n",
    "            encoding='utf-8',\n",
    "            float_format='%.6f'\n",
    "        )\n",
    "        print(f\"  [INFO] Saved to cache: {os.path.basename(cache_path)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Failed to save cache: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: NetCDF Data Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_dir : str\n",
    "        Directory containing NetCDF files for the variable\n",
    "    variable : str\n",
    "        Variable name (tasmax, tasmin, hurs, rsds, sfcWind, etc.)\n",
    "    target_lat : float\n",
    "        Target latitude\n",
    "    target_lon : float\n",
    "        Target longitude\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all NetCDF files in the directory\n",
    "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
    "    \n",
    "    # Pattern 2: Files in subdirectories named {variable}_*\n",
    "    if len(nc_files) == 0:\n",
    "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
    "        for var_subdir in var_subdirs:\n",
    "            if os.path.isdir(var_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    # For rsds, also check rad_* folders\n",
    "    if len(nc_files) == 0 and variable == 'rsds':\n",
    "        rad_subdirs = glob.glob(os.path.join(netcdf_dir, \"rad_*\"))\n",
    "        for rad_subdir in rad_subdirs:\n",
    "            if os.path.isdir(rad_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*rsds*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    # For sfcWind, also check wind_* folders\n",
    "    if len(nc_files) == 0 and variable == 'sfcWind':\n",
    "        wind_subdirs = glob.glob(os.path.join(netcdf_dir, \"wind_*\"))\n",
    "        for wind_subdir in wind_subdirs:\n",
    "            if os.path.isdir(wind_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(wind_subdir, \"*sfcWind*.nc\")))\n",
    "                # If no sfcWind-specific files, try all .nc files in wind folder\n",
    "                if not found_files:\n",
    "                    found_files = sorted(glob.glob(os.path.join(wind_subdir, \"*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(wind_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    if len(nc_files) == 0:\n",
    "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
    "    \n",
    "    # Cache coordinate information from first file\n",
    "    lat_name = None\n",
    "    lon_name = None\n",
    "    time_name = None\n",
    "    lat_idx = None\n",
    "    lon_idx = None\n",
    "    var_name = None\n",
    "    \n",
    "    # List to store daily data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process first file to get coordinate structure\n",
    "    if len(nc_files) > 0:\n",
    "        try:\n",
    "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
    "            \n",
    "            # Get variable name\n",
    "            for v in ds_sample.data_vars:\n",
    "                if variable in v.lower() or v.lower() in variable.lower():\n",
    "                    var_name = v\n",
    "                    break\n",
    "            \n",
    "            if var_name is None:\n",
    "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
    "                if variable == 'rsds':\n",
    "                    possible_names.extend(['rad', 'RAD', 'rad_day'])\n",
    "                elif variable == 'sfcWind':\n",
    "                    possible_names.extend(['wind', 'WIND', 'wind_day', 'sfcwind', 'SFCWIND'])\n",
    "                for name in possible_names:\n",
    "                    if name in ds_sample.data_vars:\n",
    "                        var_name = name\n",
    "                        break\n",
    "            \n",
    "            # Get coordinate names\n",
    "            for coord in ds_sample.coords:\n",
    "                coord_lower = coord.lower()\n",
    "                if 'lat' in coord_lower:\n",
    "                    lat_name = coord\n",
    "                elif 'lon' in coord_lower:\n",
    "                    lon_name = coord\n",
    "                elif 'time' in coord_lower:\n",
    "                    time_name = coord\n",
    "            \n",
    "            if lat_name and lon_name:\n",
    "                # Find nearest grid point\n",
    "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
    "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
    "                \n",
    "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
    "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
    "                \n",
    "                # Check if within tolerance\n",
    "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
    "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
    "                else:\n",
    "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "            \n",
    "            ds_sample.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not read sample file: {e}\")\n",
    "    \n",
    "    if var_name is None or lat_idx is None or lon_idx is None:\n",
    "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
    "        return None\n",
    "    \n",
    "    # Process all files with progress bar\n",
    "    print(f\"  Processing files...\")\n",
    "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
    "        try:\n",
    "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "            \n",
    "            # Extract data using cached indices\n",
    "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            values = data.values\n",
    "            if values.ndim > 1:\n",
    "                values = values.flatten()\n",
    "            \n",
    "            # Get time values - extract year from filename\n",
    "            year = None\n",
    "            filename = os.path.basename(nc_file)\n",
    "            year_match = re.search(r'(\\d{4})', filename)\n",
    "            if year_match:\n",
    "                year = int(year_match.group(1))\n",
    "                # Create daily dates for the year (handles leap years automatically)\n",
    "                time_values = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='D')\n",
    "            else:\n",
    "                time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
    "            \n",
    "            # Ensure correct number of dates\n",
    "            if len(time_values) != len(values):\n",
    "                if len(time_values) > len(values):\n",
    "                    time_values = time_values[:len(values)]\n",
    "            \n",
    "            # Create DataFrame for this file\n",
    "            if len(values) > 0:\n",
    "                df_file = pd.DataFrame({\n",
    "                    'date': time_values[:len(values)],\n",
    "                    'value': values\n",
    "                })\n",
    "                all_data.append(df_file)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(f\"  ERROR: No data extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"  Combining data from {len(all_data)} files...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence)\n",
    "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    # Check for missing years and fill them\n",
    "    if len(combined_df) > 0:\n",
    "        min_date = combined_df['date'].min()\n",
    "        max_date = combined_df['date'].max()\n",
    "        \n",
    "        # Create complete date range from min to max\n",
    "        expected_dates = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "        expected_dates_df = pd.DataFrame({'date': expected_dates})\n",
    "        \n",
    "        # Find missing dates\n",
    "        combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
    "        merged_df = expected_dates_df.merge(combined_df, on='date', how='left')\n",
    "        missing_dates = merged_df[merged_df['value'].isna()]\n",
    "        \n",
    "        if len(missing_dates) > 0:\n",
    "            print(f\"  [WARNING] Found {len(missing_dates)} missing dates - attempting to fill...\")\n",
    "            \n",
    "            # Identify missing years\n",
    "            missing_dates['year'] = missing_dates['date'].dt.year\n",
    "            missing_years = sorted(missing_dates['year'].unique())\n",
    "            \n",
    "            for missing_year in missing_years:\n",
    "                year_dates = missing_dates[missing_dates['year'] == missing_year]\n",
    "                print(f\"  [INFO] Missing year {missing_year}: {len(year_dates)} days\")\n",
    "                \n",
    "                # Try to find a similar year to use as reference (use previous year if available, else next year)\n",
    "                ref_year = None\n",
    "                ref_data = None\n",
    "                \n",
    "                # Try previous year\n",
    "                prev_year = missing_year - 1\n",
    "                prev_year_data = combined_df[combined_df['date'].dt.year == prev_year]\n",
    "                if len(prev_year_data) >= 365:\n",
    "                    ref_year = prev_year\n",
    "                    ref_data = prev_year_data.copy()\n",
    "                    print(f\"    Using {prev_year} as reference year\")\n",
    "                \n",
    "                # If previous year not available, try next year\n",
    "                if ref_data is None:\n",
    "                    next_year = missing_year + 1\n",
    "                    next_year_data = combined_df[combined_df['date'].dt.year == next_year]\n",
    "                    if len(next_year_data) >= 365:\n",
    "                        ref_year = next_year\n",
    "                        ref_data = next_year_data.copy()\n",
    "                        print(f\"    Using {next_year} as reference year\")\n",
    "                \n",
    "                # Fill missing year data\n",
    "                if ref_data is not None:\n",
    "                    # Create data for missing year by adjusting day-of-year from reference year\n",
    "                    filled_data = []\n",
    "                    for missing_date in year_dates['date']:\n",
    "                        # Find corresponding day-of-year in reference year\n",
    "                        # Adjust for leap years\n",
    "                        ref_date = pd.Timestamp(year=ref_year, month=missing_date.month, day=missing_date.day)\n",
    "                        \n",
    "                        # If reference date doesn't exist (e.g., Feb 29 in non-leap year), use Feb 28\n",
    "                        if not ref_date.is_leap_year and missing_date.month == 2 and missing_date.day == 29:\n",
    "                            ref_date = pd.Timestamp(year=ref_year, month=2, day=28)\n",
    "                        \n",
    "                        # Find matching date in reference data\n",
    "                        ref_match = ref_data[ref_data['date'].dt.month == ref_date.month]\n",
    "                        ref_match = ref_match[ref_match['date'].dt.day == ref_date.day]\n",
    "                        \n",
    "                        if len(ref_match) > 0:\n",
    "                            filled_value = ref_match['value'].iloc[0]\n",
    "                            filled_data.append({\n",
    "                                'date': missing_date,\n",
    "                                'value': filled_value\n",
    "                            })\n",
    "                    \n",
    "                    if len(filled_data) > 0:\n",
    "                        filled_df = pd.DataFrame(filled_data)\n",
    "                        combined_df = pd.concat([combined_df, filled_df], ignore_index=True)\n",
    "                        print(f\"    Filled {len(filled_data)} days for year {missing_year}\")\n",
    "                    else:\n",
    "                        print(f\"    [WARNING] Could not fill data for year {missing_year}\")\n",
    "                else:\n",
    "                    print(f\"    [ERROR] No suitable reference year found for {missing_year}\")\n",
    "            \n",
    "            # Re-sort after filling\n",
    "            combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "            combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    # Check for constant values by year (indicates missing/corrupted data in source files)\n",
    "    combined_df['year'] = pd.to_datetime(combined_df['date']).dt.year\n",
    "    for year in combined_df['year'].unique():\n",
    "        year_data = combined_df[combined_df['year'] == year]['value']\n",
    "        non_null_data = year_data.dropna()\n",
    "        if len(non_null_data) > 10:  # Only check if we have enough data points\n",
    "            if non_null_data.nunique() == 1:\n",
    "                constant_value = non_null_data.iloc[0]\n",
    "                num_days = len(non_null_data)\n",
    "                print(f\"  [WARNING] Year {year} has constant {variable} value: {constant_value:.2f} for {num_days} days (likely missing/corrupted source data)\")\n",
    "    combined_df = combined_df.drop(columns=['year'])\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
    "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [INFO] Using manual output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\n",
      "======================================================================\n",
      "CONFIGURATION\n",
      "======================================================================\n",
      "  Model: ACCESS CM2\n",
      "  Scenario: SSP585\n",
      "  Coordinates: (-31.750000, 117.599998)\n",
      "  CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
      "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\n",
      "  Required Variables: tasmax, tasmin, hurs, rsds, sfcWind\n",
      "======================================================================\n",
      "\n",
      "All paths and filenames will automatically use the above settings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION - CHANGE VALUES BELOW AS NEEDED\n",
    "# ============================================================================\n",
    "# All other settings will automatically adjust based on these values\n",
    "\n",
    "# Output Directory - OPTIONAL: Set to None to auto-generate, or specify a custom path\n",
    "OUTPUT_DIR_MANUAL = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\"  # Set to None for auto-generation\n",
    "\n",
    "# Model (usually doesn't need to change)\n",
    "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
    "\n",
    "# Scenario - CHANGE THIS\n",
    "SCENARIO = \"SSP585\"   # Options: \"SSP245\", \"SSP585\", etc.\n",
    "\n",
    "# Coordinates - CHANGE THESE\n",
    "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
    "LONGITUDE = 117.5999984741211  # Target longitude in decimal degrees (-180 to 180)\n",
    "\n",
    "# ============================================================================\n",
    "# AUTOMATIC SETTINGS (derived from above - no need to change)\n",
    "# ============================================================================\n",
    "\n",
    "# Base directories\n",
    "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
    "base_output_dir = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\"\n",
    "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
    "\n",
    "# Auto-generate output directory and filename components based on scenario and coordinates\n",
    "# For directory names, use underscore format (filesystem-friendly)\n",
    "lat_str_dir = f\"{LATITUDE:.2f}\".replace('.', '_').replace('-', 'neg')\n",
    "lon_str_dir = f\"{LONGITUDE:.2f}\".replace('.', '_').replace('-', 'neg')\n",
    "# For output filenames, use decimal format (keep dots and minus signs)\n",
    "lat_str = f\"{LATITUDE:.2f}\"\n",
    "lon_str = f\"{LONGITUDE:.2f}\"\n",
    "model_scenario = f\"{MODEL.replace(' ', '_')}_{SCENARIO}\"\n",
    "model_scenario_dir = f\"{model_scenario}_{lat_str_dir}_{lon_str_dir}\"\n",
    "\n",
    "# Use manual output directory if specified, otherwise auto-generate\n",
    "if OUTPUT_DIR_MANUAL is not None and OUTPUT_DIR_MANUAL != \"\":\n",
    "    OUTPUT_DIR = OUTPUT_DIR_MANUAL\n",
    "    print(f\"  [INFO] Using manual output directory: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    OUTPUT_DIR = os.path.join(base_output_dir, model_scenario_dir)\n",
    "    print(f\"  [INFO] Auto-generated output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Variables required for ET₀ calculation (FAO-56 Penman-Monteith)\n",
    "REQUIRED_VARIABLES = ['tasmax', 'tasmin', 'hurs', 'rsds', 'sfcWind']\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Scenario: {SCENARIO}\")\n",
    "print(f\"  Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"  CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Required Variables: {', '.join(REQUIRED_VARIABLES)}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAll paths and filenames will automatically use the above settings.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: FAO-56 Penman-Monteith Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_saturation_vapor_pressure(temperature_c):\n",
    "    \"\"\"\n",
    "    Calculate saturation vapor pressure (kPa) at temperature T (°C).\n",
    "    Using SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
    "    \"\"\"\n",
    "    return 0.611 * np.exp(17.27 * temperature_c / (temperature_c + 237.3))\n",
    "\n",
    "\n",
    "def calculate_actual_vapor_pressure(hurs_df, tasmax_df, tasmin_df):\n",
    "    \"\"\"\n",
    "    Calculate actual vapor pressure (kPa) from mean relative humidity.\n",
    "    Using mean humidity: ea = es(Tmean) × hurs / 100\n",
    "    \"\"\"\n",
    "    # Normalize dates to date-only (remove time component) to handle different time stamps\n",
    "    # Make copies to avoid modifying original dataframes\n",
    "    tasmax_df_norm = tasmax_df.copy()\n",
    "    tasmin_df_norm = tasmin_df.copy()\n",
    "    hurs_df_norm = hurs_df.copy()\n",
    "    \n",
    "    # Normalize dates to date-only\n",
    "    tasmax_df_norm['date'] = pd.to_datetime(tasmax_df_norm['date']).dt.date\n",
    "    tasmin_df_norm['date'] = pd.to_datetime(tasmin_df_norm['date']).dt.date\n",
    "    hurs_df_norm['date'] = pd.to_datetime(hurs_df_norm['date']).dt.date\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged = tasmax_df_norm.merge(tasmin_df_norm, on='date', suffixes=('_max', '_min'))\n",
    "    merged = merged.rename(columns={'value_max': 'tasmax', 'value_min': 'tasmin'})\n",
    "    \n",
    "    # Calculate mean temperature\n",
    "    merged['tmean'] = (merged['tasmax'] + merged['tasmin']) / 2.0\n",
    "    \n",
    "    # Merge with hurs (mean relative humidity)\n",
    "    merged = merged.merge(hurs_df_norm[['date', 'value']], on='date')\n",
    "    merged = merged.rename(columns={'value': 'hurs'})\n",
    "    \n",
    "    # Calculate saturation vapor pressure at mean temperature\n",
    "    merged['es'] = calculate_saturation_vapor_pressure(merged['tmean'])\n",
    "    \n",
    "    # Calculate actual vapor pressure using mean relative humidity\n",
    "    merged['ea'] = merged['es'] * merged['hurs'] / 100.0\n",
    "    \n",
    "    # Return DataFrame with date and ea columns\n",
    "    ea_df = merged[['date', 'ea']].copy()\n",
    "    ea_df = ea_df.rename(columns={'ea': 'value'})\n",
    "    \n",
    "    # Convert date back to datetime for proper CSV export (from date object to datetime)\n",
    "    ea_df['date'] = pd.to_datetime(ea_df['date'])\n",
    "    \n",
    "    return ea_df\n",
    "\n",
    "\n",
    "def convert_wind_10m_to_2m(wind_10m):\n",
    "    \"\"\"\n",
    "    Convert wind speed from 10m height to 2m height.\n",
    "    Using logarithmic wind profile: u2 = u10 × ln(2/0.0002) / ln(10/0.0002)\n",
    "    \"\"\"\n",
    "    z0 = 0.0002  # Roughness length (m) - typical for open water/pan\n",
    "    u2 = wind_10m * np.log(2.0 / z0) / np.log(10.0 / z0)\n",
    "    return u2\n",
    "\n",
    "\n",
    "def calculate_eto_fao56_penman_monteith(tasmax_df, tasmin_df, hurs_df, \n",
    "                                        rsds_df, sfcWind_df, latitude):\n",
    "    \"\"\"\n",
    "    Calculate reference evapotranspiration (ET₀) using FAO-56 Penman-Monteith method.\n",
    "    \n",
    "    Formula: ET₀ = (0.408 × Δ × (Rn - G) + γ × (900/(T+273)) × u₂ × (es - ea)) / (Δ + γ × (1 + 0.34 × u₂))\n",
    "    \"\"\"\n",
    "    # Normalize dates to date-only (remove time component) to handle different time stamps\n",
    "    # Make copies to avoid modifying original dataframes\n",
    "    tasmax_df_norm = tasmax_df.copy()\n",
    "    tasmin_df_norm = tasmin_df.copy()\n",
    "    rsds_df_norm = rsds_df.copy()\n",
    "    sfcWind_df_norm = sfcWind_df.copy()\n",
    "    \n",
    "    # Normalize dates to date-only\n",
    "    tasmax_df_norm['date'] = pd.to_datetime(tasmax_df_norm['date']).dt.date\n",
    "    tasmin_df_norm['date'] = pd.to_datetime(tasmin_df_norm['date']).dt.date\n",
    "    rsds_df_norm['date'] = pd.to_datetime(rsds_df_norm['date']).dt.date\n",
    "    sfcWind_df_norm['date'] = pd.to_datetime(sfcWind_df_norm['date']).dt.date\n",
    "    \n",
    "    # Merge all dataframes\n",
    "    merged = tasmax_df_norm.merge(tasmin_df_norm, on='date', suffixes=('_max', '_min'))\n",
    "    merged = merged.rename(columns={'value_max': 'tasmax', 'value_min': 'tasmin'})\n",
    "    \n",
    "    # Calculate mean temperature\n",
    "    merged['tmean'] = (merged['tasmax'] + merged['tasmin']) / 2.0\n",
    "    \n",
    "    # Calculate actual vapor pressure (this function also normalizes dates internally)\n",
    "    ea_df = calculate_actual_vapor_pressure(hurs_df, tasmax_df, tasmin_df)\n",
    "    # Normalize ea_df date as well\n",
    "    ea_df_norm = ea_df.copy()\n",
    "    ea_df_norm['date'] = pd.to_datetime(ea_df_norm['date']).dt.date\n",
    "    merged = merged.merge(ea_df_norm, on='date')\n",
    "    merged = merged.rename(columns={'value': 'ea'})\n",
    "    \n",
    "    # Calculate saturation vapor pressure at mean temperature\n",
    "    merged['es'] = calculate_saturation_vapor_pressure(merged['tmean'])\n",
    "    \n",
    "    # Calculate slope of vapor pressure curve (Δ) in kPa/°C\n",
    "    merged['delta'] = 4098 * merged['es'] / ((merged['tmean'] + 237.3) ** 2)\n",
    "    \n",
    "    # Psychrometric constant (γ) in kPa/°C (at sea level, 101.3 kPa)\n",
    "    gamma = 0.665e-3 * 101.3  # 0.0675 kPa/°C\n",
    "    \n",
    "    # Convert rsds from W/m² to MJ/m²/day\n",
    "    merged = merged.merge(rsds_df_norm[['date', 'value']], on='date')\n",
    "    merged = merged.rename(columns={'value': 'rsds'})\n",
    "    merged['rsds_mj'] = merged['rsds'] * 0.0864  # W/m² to MJ/m²/day\n",
    "    \n",
    "    # Calculate net radiation (Rn) - simplified: Rn ≈ 0.77 × Rs (assuming albedo = 0.23)\n",
    "    # For daily calculations, soil heat flux (G) is assumed to be 0\n",
    "    merged['rn'] = 0.77 * merged['rsds_mj']  # MJ/m²/day\n",
    "    merged['g'] = 0.0  # Soil heat flux assumed 0 for daily calculations\n",
    "    \n",
    "    # Convert wind speed from 10m to 2m\n",
    "    merged = merged.merge(sfcWind_df_norm[['date', 'value']], on='date')\n",
    "    merged = merged.rename(columns={'value': 'wind_10m'})\n",
    "    merged['wind_2m'] = convert_wind_10m_to_2m(merged['wind_10m'])\n",
    "    \n",
    "    # Calculate vapor pressure deficit (es - ea) in kPa\n",
    "    merged['vpd'] = merged['es'] - merged['ea']\n",
    "    \n",
    "    # FAO-56 Penman-Monteith formula\n",
    "    numerator = (0.408 * merged['delta'] * (merged['rn'] - merged['g']) + \n",
    "                 gamma * (900.0 / (merged['tmean'] + 273.0)) * merged['wind_2m'] * merged['vpd'])\n",
    "    denominator = merged['delta'] + gamma * (1.0 + 0.34 * merged['wind_2m'])\n",
    "    \n",
    "    merged['eto'] = numerator / denominator\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    merged['eto'] = merged['eto'].clip(lower=0.0)\n",
    "    \n",
    "    # Return DataFrame with date and eto columns\n",
    "    eto_df = merged[['date', 'eto']].copy()\n",
    "    eto_df = eto_df.rename(columns={'eto': 'value'})\n",
    "    \n",
    "    # Convert date back to datetime for proper CSV export (from date object to datetime)\n",
    "    eto_df['date'] = pd.to_datetime(eto_df['date'])\n",
    "    \n",
    "    return eto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Main Processing - Extract CMIP6 Data and Calculate ET₀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Processing Coordinate: (-31.750000, 117.599998)\n",
      "Model: ACCESS CM2, Scenario: SSP585\n",
      "======================================================================\n",
      "\n",
      "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP585\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Processing variable: tasmax\n",
      "======================================================================\n",
      "  [DEBUG] Cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\ACCESS_CM2_SSP585_-31.75_117.60_tasmax.csv\n",
      "  [OK] Loaded from cache: 10,957 records for tasmax\n",
      "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Processing variable: tasmin\n",
      "======================================================================\n",
      "  [DEBUG] Cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\ACCESS_CM2_SSP585_-31.75_117.60_tasmin.csv\n",
      "  [OK] Loaded from cache: 10,957 records for tasmin\n",
      "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Processing variable: hurs\n",
      "======================================================================\n",
      "  [DEBUG] Cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\ACCESS_CM2_SSP585_-31.75_117.60_hurs.csv\n",
      "  [OK] Loaded from cache: 10,957 records for hurs\n",
      "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Processing variable: rsds\n",
      "======================================================================\n",
      "  [DEBUG] Cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\ACCESS_CM2_SSP585_-31.75_117.60_rsds.csv\n",
      "  [OK] Loaded from cache: 10,957 records for rsds\n",
      "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Processing variable: sfcWind\n",
      "======================================================================\n",
      "  [DEBUG] Cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\\ACCESS_CM2_SSP585_-31.75_117.60_sfcWind.csv\n",
      "  [OK] Loaded from cache: 10,957 records for sfcWind\n",
      "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "\n",
      "======================================================================\n",
      "Calculating ET₀ using FAO-56 Penman-Monteith method...\n",
      "======================================================================\n",
      "\n",
      "  [OK] Calculated ET₀ for 10,957 days\n",
      "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "  ET₀ range: 0.32 to 15.38 mm/day\n",
      "  ET₀ mean: 6.03 mm/day\n",
      "\n",
      "  [OK] Saved ET₀ data to: ACCESS_CM2_SSP585_neg31.75_117.60_eto.csv\n",
      "\n",
      "======================================================================\n",
      "[SUCCESS] ET₀ CALCULATION COMPLETED!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Construct data directory path\n",
    "data_dir = os.path.join(CMIP6_BASE_DIR, f\"{MODEL} {SCENARIO}\")\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Processing Coordinate: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"Model: {MODEL}, Scenario: {SCENARIO}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData directory: {data_dir}\\n\")\n",
    "\n",
    "# Extract data for all required variables\n",
    "extracted_data = {}\n",
    "\n",
    "for variable in REQUIRED_VARIABLES:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing variable: {variable}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Check for cached data first\n",
    "    cache_path = get_cached_variable_path(OUTPUT_DIR, model_scenario, lat_str, lon_str, variable)\n",
    "    print(f\"  [DEBUG] Cache path: {cache_path}\")\n",
    "    df = load_cached_variable(cache_path)\n",
    "    \n",
    "    if df is not None:\n",
    "        # Use cached data\n",
    "        extracted_data[variable] = df\n",
    "        print(f\"  [OK] Loaded from cache: {len(df):,} records for {variable}\")\n",
    "        print(f\"  [INFO] Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    else:\n",
    "        # Extract from NetCDF files\n",
    "        df = extract_daily_data_from_netcdf(\n",
    "            data_dir, \n",
    "            variable, \n",
    "            LATITUDE, \n",
    "            LONGITUDE, \n",
    "            tolerance=COORD_TOLERANCE\n",
    "        )\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            extracted_data[variable] = df\n",
    "            print(f\"  [OK] Extracted {len(df):,} records for {variable}\")\n",
    "            # Save to cache for future runs\n",
    "            save_cached_variable(df, cache_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Failed to extract data for required variable: {variable}\")\n",
    "\n",
    "# Check if all required variables are available\n",
    "missing_vars = [v for v in REQUIRED_VARIABLES if v not in extracted_data]\n",
    "\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required variables: {missing_vars}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Calculating ET₀ using FAO-56 Penman-Monteith method...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Calculate ET₀ using FAO-56 Penman-Monteith\n",
    "eto_df = calculate_eto_fao56_penman_monteith(\n",
    "    extracted_data['tasmax'],\n",
    "    extracted_data['tasmin'],\n",
    "    extracted_data['hurs'],\n",
    "    extracted_data['rsds'],\n",
    "    extracted_data['sfcWind'],\n",
    "    LATITUDE\n",
    ")\n",
    "\n",
    "print(f\"  [OK] Calculated ET₀ for {len(eto_df):,} days\")\n",
    "print(f\"  Date range: {eto_df['date'].min()} to {eto_df['date'].max()}\")\n",
    "print(f\"  ET₀ range: {eto_df['value'].min():.2f} to {eto_df['value'].max():.2f} mm/day\")\n",
    "print(f\"  ET₀ mean: {eto_df['value'].mean():.2f} mm/day\")\n",
    "\n",
    "# Save ET₀ to CSV (using auto-generated filename components from configuration)\n",
    "# Format lat_str to use 'neg' prefix instead of '-' for filenames\n",
    "lat_str_filename = lat_str.replace('-', 'neg')\n",
    "output_filename = f\"{model_scenario}_{lat_str_filename}_{lon_str}_eto.csv\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "eto_df.to_csv(output_path, index=False, encoding='utf-8', float_format='%.2f')\n",
    "print(f\"\\n  [OK] Saved ET₀ data to: {output_filename}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[SUCCESS] ET₀ CALCULATION COMPLETED!\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
