{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMIP6 Evaporation (Evap) Calculation\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook calculates evaporation (evapotranspiration) from CMIP6 climate data. Evaporation can be calculated using various methods depending on available variables.\n",
    "\n",
    "## Input Variables\n",
    "\n",
    "- **tasmax**: Daily maximum temperature (°C) - required\n",
    "- **tasmin**: Daily minimum temperature (°C) - required\n",
    "- **rsds**: Surface downwelling shortwave radiation (W/m²) - required\n",
    "- **hurs**: Relative humidity (%) - optional but recommended\n",
    "- **pr**: Precipitation (mm) - optional\n",
    "\n",
    "## Output\n",
    "\n",
    "- **evap**: Evaporation/Evapotranspiration (mm/day)\n",
    "\n",
    "## Calculation Methods\n",
    "\n",
    "### Method 1: Simplified Penman-Monteith (Recommended)\n",
    "\n",
    "Uses temperature, radiation, and humidity to estimate evapotranspiration:\n",
    "\n",
    "```\n",
    "ET = (Δ × Rn + γ × (900/(T+273)) × u2 × (es - ea)) / (Δ + γ × (1 + 0.34 × u2))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Δ` = slope of vapor pressure curve\n",
    "- `Rn` = net radiation (MJ/m²/day)\n",
    "- `γ` = psychrometric constant\n",
    "- `T` = mean temperature (°C)\n",
    "- `u2` = wind speed at 2m (m/s) - estimated if not available\n",
    "- `es` = saturation vapor pressure (hPa)\n",
    "- `ea` = actual vapor pressure (hPa)\n",
    "\n",
    "### Method 2: Temperature-based (Hargreaves-Samani)\n",
    "\n",
    "Simpler method using only temperature and radiation:\n",
    "\n",
    "```\n",
    "ET = 0.0023 × Ra × (Tmean + 17.8) × √(Tmax - Tmin)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Ra` = extraterrestrial radiation (MJ/m²/day)\n",
    "- `Tmean` = mean temperature (°C)\n",
    "- `Tmax` = maximum temperature (°C)\n",
    "- `Tmin` = minimum temperature (°C)\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Set configuration parameters (Model, Scenario, coordinates)\n",
    "2. Extract required climate variables from NetCDF files\n",
    "3. Calculate evaporation using selected method\n",
    "4. Save results to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\"  # Output directory\n",
    "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
    "\n",
    "# Model and Scenario\n",
    "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
    "SCENARIO = \"SSP585\"   # e.g., \"SSP245\" or \"SSP585\"\n",
    "\n",
    "# Coordinates\n",
    "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
    "LONGITUDE = 117.5999984741211  # Target longitude in decimal degrees (-180 to 180)\n",
    "\n",
    "# Calculation method: 'penman' or 'hargreaves'\n",
    "EVAP_METHOD = 'hargreaves'  # Default to Hargreaves (simpler, fewer variables needed)\n",
    "\n",
    "# Variables required for evaporation calculation\n",
    "REQUIRED_VARIABLES = ['tasmax', 'tasmin', 'rsds']  # Minimum required\n",
    "OPTIONAL_VARIABLES = ['hurs', 'pr']  # Optional but improve accuracy\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model: {MODEL}\")\n",
    "print(f\"  - Scenario: {SCENARIO}\")\n",
    "print(f\"  - Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"  - Calculation Method: {EVAP_METHOD}\")\n",
    "print(f\"  - Required Variables: {', '.join(REQUIRED_VARIABLES)}\")\n",
    "print(f\"  - Optional Variables: {', '.join(OPTIONAL_VARIABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: NetCDF Data Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_dir : str\n",
    "        Directory containing NetCDF files for the variable\n",
    "    variable : str\n",
    "        Variable name (tasmax, tasmin, rsds, hurs, pr)\n",
    "    target_lat : float\n",
    "        Target latitude\n",
    "    target_lon : float\n",
    "        Target longitude\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all NetCDF files in the directory\n",
    "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
    "    \n",
    "    # Pattern 2: Files in subdirectories named {variable}_*\n",
    "    if len(nc_files) == 0:\n",
    "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
    "        for var_subdir in var_subdirs:\n",
    "            if os.path.isdir(var_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    # For rsds, also check rad_* folders\n",
    "    if len(nc_files) == 0 and variable == 'rsds':\n",
    "        rad_subdirs = glob.glob(os.path.join(netcdf_dir, \"rad_*\"))\n",
    "        for rad_subdir in rad_subdirs:\n",
    "            if os.path.isdir(rad_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*rsds*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    if len(nc_files) == 0:\n",
    "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
    "    \n",
    "    # Cache coordinate information from first file\n",
    "    lat_name = None\n",
    "    lon_name = None\n",
    "    time_name = None\n",
    "    lat_idx = None\n",
    "    lon_idx = None\n",
    "    var_name = None\n",
    "    \n",
    "    # List to store daily data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process first file to get coordinate structure\n",
    "    if len(nc_files) > 0:\n",
    "        try:\n",
    "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
    "            \n",
    "            # Get variable name\n",
    "            for v in ds_sample.data_vars:\n",
    "                if variable in v.lower() or v.lower() in variable.lower():\n",
    "                    var_name = v\n",
    "                    break\n",
    "            \n",
    "            if var_name is None:\n",
    "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
    "                if variable == 'rsds':\n",
    "                    possible_names.extend(['rad', 'RAD', 'rad_day'])\n",
    "                for name in possible_names:\n",
    "                    if name in ds_sample.data_vars:\n",
    "                        var_name = name\n",
    "                        break\n",
    "            \n",
    "            # Get coordinate names\n",
    "            for coord in ds_sample.coords:\n",
    "                coord_lower = coord.lower()\n",
    "                if 'lat' in coord_lower:\n",
    "                    lat_name = coord\n",
    "                elif 'lon' in coord_lower:\n",
    "                    lon_name = coord\n",
    "                elif 'time' in coord_lower:\n",
    "                    time_name = coord\n",
    "            \n",
    "            if lat_name and lon_name:\n",
    "                # Find nearest grid point\n",
    "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
    "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
    "                \n",
    "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
    "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
    "                \n",
    "                # Check if within tolerance\n",
    "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
    "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
    "                else:\n",
    "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "            \n",
    "            ds_sample.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not read sample file: {e}\")\n",
    "    \n",
    "    if var_name is None or lat_idx is None or lon_idx is None:\n",
    "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
    "        return None\n",
    "    \n",
    "    # Process all files with progress bar\n",
    "    print(f\"  Processing files...\")\n",
    "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
    "        try:\n",
    "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "            \n",
    "            # Extract data using cached indices\n",
    "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            values = data.values\n",
    "            if values.ndim > 1:\n",
    "                values = values.flatten()\n",
    "            \n",
    "            # Get time values\n",
    "            time_values = None\n",
    "            \n",
    "            # Method 1: Try to use time coordinate from NetCDF file\n",
    "            if time_name and time_name in ds.coords:\n",
    "                try:\n",
    "                    time_coord = ds[time_name]\n",
    "                    if len(time_coord) == len(values):\n",
    "                        try:\n",
    "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
    "                            time_values = pd.to_datetime(time_decoded.values)\n",
    "                        except:\n",
    "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
    "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
    "                                base_date = pd.to_datetime(base_date_str)\n",
    "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "            # Method 2: Extract year from filename\n",
    "            if time_values is None:\n",
    "                year = None\n",
    "                filename = os.path.basename(nc_file)\n",
    "                all_years = re.findall(r'\\d{4}', filename)\n",
    "                for year_str in all_years:\n",
    "                    year_candidate = int(year_str)\n",
    "                    if 2000 <= year_candidate <= 2100:\n",
    "                        year = year_candidate\n",
    "                        break\n",
    "                \n",
    "                if year:\n",
    "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
    "                else:\n",
    "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
    "            \n",
    "            # Ensure correct number of dates\n",
    "            if len(time_values) != len(values):\n",
    "                if len(time_values) > len(values):\n",
    "                    time_values = time_values[:len(values)]\n",
    "            \n",
    "            # Create DataFrame for this file\n",
    "            if len(values) > 0:\n",
    "                df_file = pd.DataFrame({\n",
    "                    'date': time_values[:len(values)],\n",
    "                    'value': values\n",
    "                })\n",
    "                all_data.append(df_file)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(f\"  ERROR: No data extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"  Combining data from {len(all_data)} files...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence)\n",
    "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
    "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Evaporation Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_evaporation_hargreaves(tasmax_df, tasmin_df, rsds_df, latitude):\n",
    "    \"\"\"\n",
    "    Calculate evapotranspiration using Hargreaves-Samani method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tasmax_df : pd.DataFrame\n",
    "        DataFrame with date and value (maximum temperature °C) columns\n",
    "    tasmin_df : pd.DataFrame\n",
    "        DataFrame with date and value (minimum temperature °C) columns\n",
    "    rsds_df : pd.DataFrame\n",
    "        DataFrame with date and value (solar radiation W/m²) columns\n",
    "    latitude : float\n",
    "        Latitude in decimal degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with date and value (evaporation mm/day) columns\n",
    "    \"\"\"\n",
    "    # Merge all dataframes\n",
    "    merged = tasmax_df.merge(tasmin_df, on='date', suffixes=('_max', '_min'))\n",
    "    merged = merged.merge(rsds_df, on='date')\n",
    "    merged = merged.rename(columns={'value': 'rsds'})\n",
    "    \n",
    "    # Calculate mean temperature\n",
    "    merged['tmean'] = (merged['value_max'] + merged['value_min']) / 2.0\n",
    "    merged['trange'] = merged['value_max'] - merged['value_min']\n",
    "    \n",
    "    # Convert rsds from W/m² to MJ/m²/day\n",
    "    merged['rsds_mj'] = merged['rsds'] * 0.0864  # W/m² to MJ/m²/day\n",
    "    \n",
    "    # Calculate day of year\n",
    "    merged['doy'] = merged['date'].dt.dayofyear\n",
    "    \n",
    "    # Calculate extraterrestrial radiation (Ra) in MJ/m²/day\n",
    "    # Simplified formula for daily Ra\n",
    "    lat_rad = math.radians(latitude)\n",
    "    \n",
    "    def calculate_ra(doy):\n",
    "        # Solar declination (radians)\n",
    "        delta = 0.409 * math.sin(2 * math.pi * doy / 365 - 1.39)\n",
    "        # Sunset hour angle (radians)\n",
    "        ws = math.acos(-math.tan(lat_rad) * math.tan(delta))\n",
    "        # Extraterrestrial radiation (MJ/m²/day)\n",
    "        dr = 1 + 0.033 * math.cos(2 * math.pi * doy / 365)\n",
    "        ra = 37.6 * dr * (ws * math.sin(lat_rad) * math.sin(delta) + math.cos(lat_rad) * math.cos(delta) * math.sin(ws))\n",
    "        return ra\n",
    "    \n",
    "    merged['ra'] = merged['doy'].apply(calculate_ra)\n",
    "    \n",
    "    # Hargreaves-Samani formula: ET = 0.0023 × Ra × (Tmean + 17.8) × √(Tmax - Tmin)\n",
    "    # Using actual solar radiation instead of Ra for better accuracy\n",
    "    merged['evap'] = 0.0023 * merged['rsds_mj'] * (merged['tmean'] + 17.8) * np.sqrt(merged['trange'])\n",
    "    \n",
    "    # Ensure non-negative values\n",
    "    merged['evap'] = merged['evap'].clip(lower=0.0)\n",
    "    \n",
    "    # Return DataFrame with date and evap columns\n",
    "    evap_df = merged[['date', 'evap']].copy()\n",
    "    evap_df = evap_df.rename(columns={'evap': 'value'})\n",
    "    \n",
    "    return evap_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Main Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct data directory path\n",
    "data_dir = os.path.join(CMIP6_BASE_DIR, f\"{MODEL} {SCENARIO}\")\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Processing Coordinate: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"Model: {MODEL}, Scenario: {SCENARIO}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData directory: {data_dir}\\n\")\n",
    "\n",
    "# Extract data for all required variables\n",
    "extracted_data = {}\n",
    "\n",
    "for variable in REQUIRED_VARIABLES:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing variable: {variable}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    df = extract_daily_data_from_netcdf(\n",
    "        data_dir, \n",
    "        variable, \n",
    "        LATITUDE, \n",
    "        LONGITUDE, \n",
    "        tolerance=COORD_TOLERANCE\n",
    "    )\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        extracted_data[variable] = df\n",
    "        print(f\"  [OK] Extracted {len(df):,} records for {variable}\")\n",
    "    else:\n",
    "        print(f\"  [ERROR] Failed to extract data for {variable}\")\n",
    "\n",
    "# Try to extract optional variables\n",
    "for variable in OPTIONAL_VARIABLES:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing optional variable: {variable}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    df = extract_daily_data_from_netcdf(\n",
    "        data_dir, \n",
    "        variable, \n",
    "        LATITUDE, \n",
    "        LONGITUDE, \n",
    "        tolerance=COORD_TOLERANCE\n",
    "    )\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        extracted_data[variable] = df\n",
    "        print(f\"  [OK] Extracted {len(df):,} records for {variable}\")\n",
    "    else:\n",
    "        print(f\"  [WARNING] Could not extract data for optional variable {variable}\")\n",
    "\n",
    "# Check if all required variables are available\n",
    "missing_vars = [v for v in REQUIRED_VARIABLES if v not in extracted_data]\n",
    "\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required variables: {missing_vars}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Calculating Evaporation using {EVAP_METHOD} method...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Calculate evaporation based on selected method\n",
    "if EVAP_METHOD == 'hargreaves':\n",
    "    evap_df = calculate_evaporation_hargreaves(\n",
    "        extracted_data['tasmax'],\n",
    "        extracted_data['tasmin'],\n",
    "        extracted_data['rsds'],\n",
    "        LATITUDE\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown evaporation method: {EVAP_METHOD}\")\n",
    "\n",
    "print(f\"  [OK] Calculated evaporation for {len(evap_df):,} days\")\n",
    "print(f\"  Date range: {evap_df['date'].min()} to {evap_df['date'].max()}\")\n",
    "print(f\"  Evap range: {evap_df['value'].min():.2f} to {evap_df['value'].max():.2f} mm/day\")\n",
    "print(f\"  Evap mean: {evap_df['value'].mean():.2f} mm/day\")\n",
    "\n",
    "# Save to CSV\n",
    "lat_str = f\"{LATITUDE:.2f}\"\n",
    "lon_str = f\"{LONGITUDE:.2f}\"\n",
    "model_scenario = f\"{MODEL.replace(' ', '_')}_{SCENARIO}\"\n",
    "output_filename = f\"{model_scenario}_{lat_str}_{lon_str}_evap.csv\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "evap_df.to_csv(output_path, index=False, encoding='utf-8', float_format='%.2f')\n",
    "print(f\"\\n  [OK] Saved evaporation data to: {output_filename}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[SUCCESS] EVAPORATION CALCULATION COMPLETED!\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
