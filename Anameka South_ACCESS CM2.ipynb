{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anameka South - ACCESS CM2 Daily Data Extraction\n",
        "\n",
        "This notebook extracts daily time series data from NetCDF files for a specific coordinate.\n",
        "\n",
        "## Variables: tasmax, tasmin, pr\n",
        "## Scenarios: SSP585, SSP245\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import os\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Configuration\n",
        "SSP585_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP585\"\n",
        "SSP245_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP245\"\n",
        "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\"\n",
        "\n",
        "# Variables to process\n",
        "VARIABLES = ['tasmax', 'tasmin', 'pr']\n",
        "SCENARIOS = ['SSP585', 'SSP245']\n",
        "\n",
        "# Coordinate matching tolerance (degrees)\n",
        "COORD_TOLERANCE = 0.01  # Approximately 1.1 km\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  - SSP585 directory: {SSP585_DIR}\")\n",
        "print(f\"  - SSP245 directory: {SSP245_DIR}\")\n",
        "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"  - Variables: {', '.join(VARIABLES)}\")\n",
        "print(f\"  - Scenarios: {', '.join(SCENARIOS)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Specify Target Coordinate\n",
        "\n",
        "**Enter the latitude and longitude for the grid point you want to extract:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TARGET COORDINATE\n",
        "TARGET_LAT = -31.75  # Latitude (degrees)\n",
        "TARGET_LON = 117.5999984741211   # Longitude (degrees)\n",
        "\n",
        "print(f\"Target coordinate:\")\n",
        "print(f\"  Latitude: {TARGET_LAT}\")\n",
        "print(f\"  Longitude: {TARGET_LON}\")\n",
        "print(f\"  Tolerance: {COORD_TOLERANCE} degrees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
        "    Optimized version with progress reporting and cached coordinate indices.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    netcdf_dir : str\n",
        "        Directory containing NetCDF files for the variable\n",
        "    variable : str\n",
        "        Variable name (tasmax, tasmin, or pr)\n",
        "    target_lat : float\n",
        "        Target latitude\n",
        "    target_lon : float\n",
        "        Target longitude\n",
        "    tolerance : float\n",
        "        Coordinate matching tolerance in degrees\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with columns: date, value\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Find all NetCDF files in the directory\n",
        "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
        "    \n",
        "    if len(nc_files) == 0:\n",
        "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
        "    \n",
        "    # Cache coordinate information from first file\n",
        "    lat_name = None\n",
        "    lon_name = None\n",
        "    time_name = None\n",
        "    lat_idx = None\n",
        "    lon_idx = None\n",
        "    actual_lat = None\n",
        "    actual_lon = None\n",
        "    var_name = None\n",
        "    \n",
        "    # List to store daily data\n",
        "    all_data = []\n",
        "    \n",
        "    # Process first file to get coordinate structure\n",
        "    if len(nc_files) > 0:\n",
        "        try:\n",
        "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
        "            \n",
        "            # Get variable name\n",
        "            for v in ds_sample.data_vars:\n",
        "                if variable in v.lower() or v.lower() in variable.lower():\n",
        "                    var_name = v\n",
        "                    break\n",
        "            \n",
        "            if var_name is None:\n",
        "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
        "                for name in possible_names:\n",
        "                    if name in ds_sample.data_vars:\n",
        "                        var_name = name\n",
        "                        break\n",
        "            \n",
        "            # Get coordinate names\n",
        "            for coord in ds_sample.coords:\n",
        "                coord_lower = coord.lower()\n",
        "                if 'lat' in coord_lower:\n",
        "                    lat_name = coord\n",
        "                elif 'lon' in coord_lower:\n",
        "                    lon_name = coord\n",
        "                elif 'time' in coord_lower:\n",
        "                    time_name = coord\n",
        "            \n",
        "            if lat_name and lon_name:\n",
        "                # Find nearest grid point (cache indices)\n",
        "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
        "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
        "                \n",
        "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
        "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
        "                \n",
        "                # Check if within tolerance\n",
        "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
        "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
        "                else:\n",
        "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
        "            \n",
        "            ds_sample.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not read sample file: {e}\")\n",
        "    \n",
        "    if var_name is None or lat_idx is None or lon_idx is None:\n",
        "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
        "        return None\n",
        "    \n",
        "    # Process all files with progress bar\n",
        "    print(f\"  Processing files...\")\n",
        "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
        "        try:\n",
        "            # Open NetCDF file with minimal decoding for speed\n",
        "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
        "            \n",
        "            # Extract data using cached indices\n",
        "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
        "            \n",
        "            # Get time values - extract year from filename for simplicity and speed\n",
        "            import re\n",
        "            year_match = re.search(r'(\\d{4})', os.path.basename(nc_file))\n",
        "            if year_match:\n",
        "                year = int(year_match.group(1))\n",
        "                # Create daily dates for the year (handles leap years automatically)\n",
        "                time_values = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='D')\n",
        "            else:\n",
        "                # Fallback: use index if year not found\n",
        "                time_values = pd.date_range(start='2000-01-01', periods=len(data.values), freq='D')\n",
        "            \n",
        "            # Convert to numpy array (load into memory)\n",
        "            values = data.values\n",
        "            if values.ndim > 1:\n",
        "                values = values.flatten()\n",
        "            \n",
        "            # Create DataFrame for this file\n",
        "            if len(values) == len(time_values):\n",
        "                df_file = pd.DataFrame({\n",
        "                    'date': time_values[:len(values)],\n",
        "                    'value': values\n",
        "                })\n",
        "                all_data.append(df_file)\n",
        "            \n",
        "            ds.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(all_data) == 0:\n",
        "        print(f\"  ERROR: No data extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Combine all data\n",
        "    print(f\"  Combining data from {len(all_data)} files...\")\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Remove duplicate dates (keep first occurrence)\n",
        "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
        "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
        "    \n",
        "    return combined_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process each scenario and variable\n",
        "results_summary = {}\n",
        "total_start_time = time.time()\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"STARTING EXTRACTION PROCESS\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total tasks: {len(SCENARIOS) * len(VARIABLES)} (2 scenarios × 3 variables)\")\n",
        "print(f\"Target coordinate: ({TARGET_LAT}, {TARGET_LON})\")\n",
        "print(f\"{'='*70}\\n\")\n",
        "\n",
        "task_num = 0\n",
        "total_tasks = len(SCENARIOS) * len(VARIABLES)\n",
        "\n",
        "for scenario in SCENARIOS:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing Scenario: {scenario}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Select directory based on scenario\n",
        "    if scenario == 'SSP585':\n",
        "        base_dir = SSP585_DIR\n",
        "    elif scenario == 'SSP245':\n",
        "        base_dir = SSP245_DIR\n",
        "    else:\n",
        "        print(f\"  ERROR: Unknown scenario {scenario}\")\n",
        "        continue\n",
        "    \n",
        "    for variable in VARIABLES:\n",
        "        task_num += 1\n",
        "        print(f\"\\n{'-'*70}\")\n",
        "        print(f\"Task {task_num}/{total_tasks}: Processing {variable} ({scenario})\")\n",
        "        print(f\"{'-'*70}\")\n",
        "        \n",
        "        task_start_time = time.time()\n",
        "        \n",
        "        # Construct directory path for this variable\n",
        "        var_dir = os.path.join(base_dir, f\"{variable}_ACCESS CM2 {scenario}\")\n",
        "        \n",
        "        if not os.path.exists(var_dir):\n",
        "            print(f\"  ERROR: Directory not found: {var_dir}\")\n",
        "            continue\n",
        "        \n",
        "        # Extract daily data\n",
        "        daily_data = extract_daily_data_from_netcdf(\n",
        "            var_dir,\n",
        "            variable,\n",
        "            TARGET_LAT,\n",
        "            TARGET_LON,\n",
        "            tolerance=COORD_TOLERANCE\n",
        "        )\n",
        "        \n",
        "        if daily_data is None or len(daily_data) == 0:\n",
        "            print(f\"  WARNING: No data extracted for {variable} ({scenario})\")\n",
        "            continue\n",
        "        \n",
        "        # Prepare output filename\n",
        "        output_filename = f\"Anameka South_ACCESS CM2_{variable}_{scenario}.csv\"\n",
        "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "        \n",
        "        # Export to CSV (keep only date and value columns for tidy format)\n",
        "        print(f\"  Exporting to CSV...\")\n",
        "        output_df = daily_data[['date', 'value']].copy()\n",
        "        output_df.to_csv(\n",
        "            output_path,\n",
        "            index=False,\n",
        "            encoding='utf-8',\n",
        "            float_format='%.6f'\n",
        "        )\n",
        "        \n",
        "        task_elapsed = time.time() - task_start_time\n",
        "        print(f\"  ✓ Exported to CSV: {os.path.basename(output_path)}\")\n",
        "        print(f\"  ✓ Rows: {len(output_df):,} | Time: {task_elapsed:.1f}s\")\n",
        "        \n",
        "        # Store summary\n",
        "        key = f\"{variable}_{scenario}\"\n",
        "        results_summary[key] = {\n",
        "            'rows': len(output_df),\n",
        "            'date_range': f\"{output_df['date'].min()} to {output_df['date'].max()}\",\n",
        "            'output_file': output_filename,\n",
        "            'time_seconds': task_elapsed\n",
        "        }\n",
        "\n",
        "total_elapsed = time.time() - total_start_time\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"ALL TASKS COMPLETED\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total processing time: {total_elapsed:.1f} seconds ({total_elapsed/60:.1f} minutes)\")\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXTRACTION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTarget coordinate: ({TARGET_LAT}, {TARGET_LON})\")\n",
        "print(f\"\\nFiles processed:\")\n",
        "for key, summary in results_summary.items():\n",
        "    var, scen = key.split('_', 1)\n",
        "    print(f\"\\n  {var.upper()} ({scen}):\")\n",
        "    print(f\"      Rows: {summary['rows']}\")\n",
        "    print(f\"      Date range: {summary['date_range']}\")\n",
        "    print(f\"      Output file: {summary['output_file']}\")\n",
        "\n",
        "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
        "print(\"\\nAll CSV files exported successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
