{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anameka South - ACCESS CM2 Daily Data Extraction\n",
    "\n",
    "This notebook extracts daily time series data from NetCDF files for a specific coordinate.\n",
    "\n",
    "## Variables: tasmax, tasmin, pr\n",
    "## Scenarios: SSP585, SSP245\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T02:12:44.200314Z",
     "iopub.status.busy": "2025-12-15T02:12:44.199725Z",
     "iopub.status.idle": "2025-12-15T02:12:45.904796Z",
     "shell.execute_reply": "2025-12-15T02:12:45.903272Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  - SSP585 directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP585\n",
      "  - SSP245 directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP245\n",
      "  - Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\n",
      "  - Variables: tasmax, tasmin, pr\n",
      "  - Scenarios: SSP585, SSP245\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "SSP585_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP585\"\n",
    "SSP245_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP245\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\"\n",
    "\n",
    "# Variables to process\n",
    "VARIABLES = ['tasmax', 'tasmin', 'pr']\n",
    "SCENARIOS = ['SSP585', 'SSP245']\n",
    "\n",
    "# Coordinate matching tolerance (degrees)\n",
    "COORD_TOLERANCE = 0.01  # Approximately 1.1 km\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  - SSP585 directory: {SSP585_DIR}\")\n",
    "print(f\"  - SSP245 directory: {SSP245_DIR}\")\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Variables: {', '.join(VARIABLES)}\")\n",
    "print(f\"  - Scenarios: {', '.join(SCENARIOS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Specify Target Coordinate\n",
    "\n",
    "**Enter the latitude and longitude for the grid point you want to extract:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T02:12:45.964024Z",
     "iopub.status.busy": "2025-12-15T02:12:45.963380Z",
     "iopub.status.idle": "2025-12-15T02:12:45.970776Z",
     "shell.execute_reply": "2025-12-15T02:12:45.969547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target coordinate:\n",
      "  Latitude: -31.75\n",
      "  Longitude: 117.5999984741211\n",
      "  Tolerance: 0.01 degrees\n"
     ]
    }
   ],
   "source": [
    "# TARGET COORDINATE\n",
    "TARGET_LAT = -31.75  # Latitude (degrees)\n",
    "TARGET_LON = 117.5999984741211   # Longitude (degrees)\n",
    "\n",
    "print(f\"Target coordinate:\")\n",
    "print(f\"  Latitude: {TARGET_LAT}\")\n",
    "print(f\"  Longitude: {TARGET_LON}\")\n",
    "print(f\"  Tolerance: {COORD_TOLERANCE} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T02:12:45.975304Z",
     "iopub.status.busy": "2025-12-15T02:12:45.974815Z",
     "iopub.status.idle": "2025-12-15T02:12:45.995912Z",
     "shell.execute_reply": "2025-12-15T02:12:45.994697Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
    "    Optimized version with progress reporting and cached coordinate indices.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_dir : str\n",
    "        Directory containing NetCDF files for the variable\n",
    "    variable : str\n",
    "        Variable name (tasmax, tasmin, or pr)\n",
    "    target_lat : float\n",
    "        Target latitude\n",
    "    target_lon : float\n",
    "        Target longitude\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all NetCDF files in the directory\n",
    "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
    "    \n",
    "    if len(nc_files) == 0:\n",
    "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
    "    \n",
    "    # Cache coordinate information from first file\n",
    "    lat_name = None\n",
    "    lon_name = None\n",
    "    time_name = None\n",
    "    lat_idx = None\n",
    "    lon_idx = None\n",
    "    actual_lat = None\n",
    "    actual_lon = None\n",
    "    var_name = None\n",
    "    \n",
    "    # List to store daily data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process first file to get coordinate structure\n",
    "    if len(nc_files) > 0:\n",
    "        try:\n",
    "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
    "            \n",
    "            # Get variable name\n",
    "            for v in ds_sample.data_vars:\n",
    "                if variable in v.lower() or v.lower() in variable.lower():\n",
    "                    var_name = v\n",
    "                    break\n",
    "            \n",
    "            if var_name is None:\n",
    "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
    "                for name in possible_names:\n",
    "                    if name in ds_sample.data_vars:\n",
    "                        var_name = name\n",
    "                        break\n",
    "            \n",
    "            # Get coordinate names\n",
    "            for coord in ds_sample.coords:\n",
    "                coord_lower = coord.lower()\n",
    "                if 'lat' in coord_lower:\n",
    "                    lat_name = coord\n",
    "                elif 'lon' in coord_lower:\n",
    "                    lon_name = coord\n",
    "                elif 'time' in coord_lower:\n",
    "                    time_name = coord\n",
    "            \n",
    "            if lat_name and lon_name:\n",
    "                # Find nearest grid point (cache indices)\n",
    "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
    "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
    "                \n",
    "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
    "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
    "                \n",
    "                # Check if within tolerance\n",
    "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
    "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
    "                else:\n",
    "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "            \n",
    "            ds_sample.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not read sample file: {e}\")\n",
    "    \n",
    "    if var_name is None or lat_idx is None or lon_idx is None:\n",
    "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
    "        return None\n",
    "    \n",
    "    # Process all files with progress bar\n",
    "    print(f\"  Processing files...\")\n",
    "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
    "        try:\n",
    "            # Open NetCDF file with minimal decoding for speed\n",
    "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "            \n",
    "            # Extract data using cached indices\n",
    "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
    "            \n",
    "            # Get time values - extract year from filename for simplicity and speed\n",
    "            import re\n",
    "            year_match = re.search(r'(\\d{4})', os.path.basename(nc_file))\n",
    "            if year_match:\n",
    "                year = int(year_match.group(1))\n",
    "                # Create daily dates for the year (handles leap years automatically)\n",
    "                time_values = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='D')\n",
    "            else:\n",
    "                # Fallback: use index if year not found\n",
    "                time_values = pd.date_range(start='2000-01-01', periods=len(data.values), freq='D')\n",
    "            \n",
    "            # Convert to numpy array (load into memory)\n",
    "            values = data.values\n",
    "            if values.ndim > 1:\n",
    "                values = values.flatten()\n",
    "            \n",
    "            # Create DataFrame for this file\n",
    "            if len(values) == len(time_values):\n",
    "                df_file = pd.DataFrame({\n",
    "                    'date': time_values[:len(values)],\n",
    "                    'value': values\n",
    "                })\n",
    "                all_data.append(df_file)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(f\"  ERROR: No data extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"  Combining data from {len(all_data)} files...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence)\n",
    "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
    "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "    \n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T02:12:46.000806Z",
     "iopub.status.busy": "2025-12-15T02:12:46.000124Z",
     "iopub.status.idle": "2025-12-15T02:24:20.569060Z",
     "shell.execute_reply": "2025-12-15T02:24:20.567917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING EXTRACTION PROCESS\n",
      "======================================================================\n",
      "Total tasks: 6 (2 scenarios × 3 variables)\n",
      "Target coordinate: (-31.75, 117.5999984741211)\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Processing Scenario: SSP585\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Task 1/6: Processing tasmax (SSP585)\n",
      "----------------------------------------------------------------------\n",
      "  Found 30 NetCDF files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   3%|▎         | 1/30 [00:04<02:24,  4.97s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   7%|▋         | 2/30 [00:09<02:14,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  10%|█         | 3/30 [00:14<02:08,  4.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  13%|█▎        | 4/30 [00:19<02:02,  4.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  17%|█▋        | 5/30 [00:23<01:54,  4.57s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  20%|██        | 6/30 [00:25<01:28,  3.67s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  23%|██▎       | 7/30 [00:29<01:28,  3.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  27%|██▋       | 8/30 [00:34<01:29,  4.08s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  30%|███       | 9/30 [00:38<01:29,  4.24s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  33%|███▎      | 10/30 [00:43<01:26,  4.33s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  37%|███▋      | 11/30 [00:47<01:24,  4.46s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  40%|████      | 12/30 [00:52<01:21,  4.54s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  43%|████▎     | 13/30 [00:57<01:18,  4.65s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  47%|████▋     | 14/30 [01:02<01:14,  4.67s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  50%|█████     | 15/30 [01:07<01:10,  4.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  53%|█████▎    | 16/30 [01:11<01:06,  4.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  57%|█████▋    | 17/30 [01:16<01:01,  4.76s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  60%|██████    | 18/30 [01:21<00:57,  4.80s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  63%|██████▎   | 19/30 [01:26<00:52,  4.81s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  67%|██████▋   | 20/30 [01:31<00:48,  4.83s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  70%|███████   | 21/30 [01:36<00:43,  4.83s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  73%|███████▎  | 22/30 [01:41<00:38,  4.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  77%|███████▋  | 23/30 [01:45<00:33,  4.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  80%|████████  | 24/30 [01:50<00:29,  4.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  83%|████████▎ | 25/30 [01:55<00:24,  4.84s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  87%|████████▋ | 26/30 [02:00<00:18,  4.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  90%|█████████ | 27/30 [02:04<00:14,  4.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  93%|█████████▎| 28/30 [02:09<00:09,  4.70s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  97%|█████████▋| 29/30 [02:13<00:04,  4.66s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax: 100%|██████████| 30/30 [02:18<00:00,  4.67s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax: 100%|██████████| 30/30 [02:18<00:00,  4.62s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 29 files...\n",
      "  ✓ Extracted 10,592 daily records in 140.2 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "  Exporting to CSV...\n",
      "  ✓ Exported to CSV: Anameka South_ACCESS CM2_tasmax_SSP585.csv\n",
      "  ✓ Rows: 10,592 | Time: 140.3s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Task 2/6: Processing tasmin (SSP585)\n",
      "----------------------------------------------------------------------\n",
      "  Found 30 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   3%|▎         | 1/30 [00:04<02:16,  4.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   7%|▋         | 2/30 [00:09<02:15,  4.83s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  10%|█         | 3/30 [00:14<02:10,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  13%|█▎        | 4/30 [00:18<02:02,  4.70s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  17%|█▋        | 5/30 [00:20<01:32,  3.71s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  20%|██        | 6/30 [00:22<01:14,  3.12s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  23%|██▎       | 7/30 [00:24<01:02,  2.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  27%|██▋       | 8/30 [00:26<00:54,  2.49s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  30%|███       | 9/30 [00:28<00:49,  2.33s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  33%|███▎      | 10/30 [00:30<00:44,  2.22s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  37%|███▋      | 11/30 [00:32<00:40,  2.14s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  40%|████      | 12/30 [00:34<00:37,  2.08s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  43%|████▎     | 13/30 [00:36<00:34,  2.05s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  47%|████▋     | 14/30 [00:38<00:32,  2.04s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  50%|█████     | 15/30 [00:40<00:30,  2.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  53%|█████▎    | 16/30 [00:42<00:28,  2.02s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  57%|█████▋    | 17/30 [00:44<00:25,  2.00s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  60%|██████    | 18/30 [00:46<00:24,  2.01s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  63%|██████▎   | 19/30 [00:48<00:22,  2.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  67%|██████▋   | 20/30 [00:50<00:20,  2.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  70%|███████   | 21/30 [00:52<00:18,  2.04s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  73%|███████▎  | 22/30 [00:54<00:16,  2.04s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  77%|███████▋  | 23/30 [00:56<00:14,  2.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  80%|████████  | 24/30 [00:58<00:12,  2.01s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  83%|████████▎ | 25/30 [01:00<00:10,  2.01s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  87%|████████▋ | 26/30 [01:02<00:08,  2.00s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  90%|█████████ | 27/30 [01:04<00:06,  2.01s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  97%|█████████▋| 29/30 [01:04<00:01,  1.11s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin: 100%|██████████| 30/30 [01:05<00:00,  2.17s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 26 files...\n",
      "  ✓ Extracted 9,497 daily records in 65.1 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "  Exporting to CSV...\n",
      "  ✓ Exported to CSV: Anameka South_ACCESS CM2_tasmin_SSP585.csv\n",
      "  ✓ Rows: 9,497 | Time: 65.1s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Task 3/6: Processing pr (SSP585)\n",
      "----------------------------------------------------------------------\n",
      "  Found 28 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:   0%|          | 0/28 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:   4%|▎         | 1/28 [00:02<00:55,  2.05s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:   7%|▋         | 2/28 [00:04<00:53,  2.07s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  11%|█         | 3/28 [00:06<00:51,  2.07s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  14%|█▍        | 4/28 [00:08<00:49,  2.08s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  18%|█▊        | 5/28 [00:10<00:48,  2.10s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  21%|██▏       | 6/28 [00:12<00:46,  2.11s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  25%|██▌       | 7/28 [00:14<00:44,  2.10s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  29%|██▊       | 8/28 [00:16<00:42,  2.13s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  32%|███▏      | 9/28 [00:19<00:40,  2.15s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  36%|███▌      | 10/28 [00:21<00:38,  2.11s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  39%|███▉      | 11/28 [00:23<00:36,  2.12s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  43%|████▎     | 12/28 [00:25<00:33,  2.11s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  46%|████▋     | 13/28 [00:27<00:31,  2.12s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  50%|█████     | 14/28 [00:29<00:29,  2.13s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  54%|█████▎    | 15/28 [00:31<00:27,  2.12s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  57%|█████▋    | 16/28 [00:33<00:25,  2.14s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  61%|██████    | 17/28 [00:35<00:23,  2.11s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  64%|██████▍   | 18/28 [00:38<00:23,  2.34s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  68%|██████▊   | 19/28 [00:43<00:28,  3.16s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  71%|███████▏  | 20/28 [00:48<00:29,  3.75s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  75%|███████▌  | 21/28 [00:54<00:29,  4.15s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  79%|███████▊  | 22/28 [00:59<00:26,  4.49s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  82%|████████▏ | 23/28 [01:04<00:23,  4.65s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  86%|████████▌ | 24/28 [01:09<00:19,  4.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  89%|████████▉ | 25/28 [01:14<00:14,  4.99s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  93%|█████████▎| 26/28 [01:20<00:10,  5.02s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  96%|█████████▋| 27/28 [01:25<00:05,  5.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr: 100%|██████████| 28/28 [01:30<00:00,  5.09s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr: 100%|██████████| 28/28 [01:30<00:00,  3.23s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 27 files...\n",
      "  ✓ Extracted 9,862 daily records in 90.4 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "  Exporting to CSV...\n",
      "  ✓ Exported to CSV: Anameka South_ACCESS CM2_pr_SSP585.csv\n",
      "  ✓ Rows: 9,862 | Time: 90.4s\n",
      "\n",
      "======================================================================\n",
      "Processing Scenario: SSP245\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Task 4/6: Processing tasmax (SSP245)\n",
      "----------------------------------------------------------------------\n",
      "  Found 30 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   3%|▎         | 1/30 [00:04<02:19,  4.81s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   7%|▋         | 2/30 [00:09<02:14,  4.80s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  10%|█         | 3/30 [00:14<02:09,  4.81s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  13%|█▎        | 4/30 [00:19<02:05,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  17%|█▋        | 5/30 [00:24<02:02,  4.89s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  20%|██        | 6/30 [00:28<01:55,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  23%|██▎       | 7/30 [00:33<01:51,  4.86s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  27%|██▋       | 8/30 [00:38<01:48,  4.91s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  30%|███       | 9/30 [00:43<01:43,  4.92s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  33%|███▎      | 10/30 [00:48<01:38,  4.93s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  37%|███▋      | 11/30 [00:53<01:34,  4.95s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  40%|████      | 12/30 [00:58<01:30,  5.02s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  43%|████▎     | 13/30 [01:03<01:23,  4.94s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  47%|████▋     | 14/30 [01:08<01:18,  4.91s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  50%|█████     | 15/30 [01:13<01:12,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  53%|█████▎    | 16/30 [01:17<01:06,  4.78s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  57%|█████▋    | 17/30 [01:22<01:01,  4.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  60%|██████    | 18/30 [01:27<00:56,  4.75s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  63%|██████▎   | 19/30 [01:32<00:52,  4.76s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  67%|██████▋   | 20/30 [01:36<00:47,  4.79s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  70%|███████   | 21/30 [01:42<00:43,  4.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  73%|███████▎  | 22/30 [01:46<00:39,  4.89s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  77%|███████▋  | 23/30 [01:51<00:33,  4.83s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  80%|████████  | 24/30 [01:56<00:29,  4.84s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  83%|████████▎ | 25/30 [02:01<00:24,  4.83s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  87%|████████▋ | 26/30 [02:06<00:19,  4.81s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  90%|█████████ | 27/30 [02:10<00:14,  4.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  93%|█████████▎| 28/30 [02:15<00:09,  4.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  97%|█████████▋| 29/30 [02:19<00:04,  4.67s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax: 100%|██████████| 30/30 [02:24<00:00,  4.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax: 100%|██████████| 30/30 [02:24<00:00,  4.83s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 29 files...\n",
      "  ✓ Extracted 10,592 daily records in 144.8 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "  Exporting to CSV...\n",
      "  ✓ Exported to CSV: Anameka South_ACCESS CM2_tasmax_SSP245.csv\n",
      "  ✓ Rows: 10,592 | Time: 144.9s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Task 5/6: Processing tasmin (SSP245)\n",
      "----------------------------------------------------------------------\n",
      "  Found 30 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   3%|▎         | 1/30 [00:04<02:21,  4.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   7%|▋         | 2/30 [00:09<02:15,  4.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  10%|█         | 3/30 [00:14<02:10,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  13%|█▎        | 4/30 [00:19<02:04,  4.81s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  17%|█▋        | 5/30 [00:24<02:01,  4.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  20%|██        | 6/30 [00:29<01:58,  4.95s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  23%|██▎       | 7/30 [00:34<01:54,  4.99s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  27%|██▋       | 8/30 [00:39<01:49,  4.98s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  30%|███       | 9/30 [00:44<01:43,  4.94s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  33%|███▎      | 10/30 [00:49<01:38,  4.92s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  37%|███▋      | 11/30 [00:53<01:33,  4.90s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  40%|████      | 12/30 [00:58<01:27,  4.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  43%|████▎     | 13/30 [01:03<01:22,  4.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  47%|████▋     | 14/30 [01:08<01:17,  4.82s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  50%|█████     | 15/30 [01:12<01:10,  4.70s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  53%|█████▎    | 16/30 [01:17<01:06,  4.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  57%|█████▋    | 17/30 [01:22<01:03,  4.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  60%|██████    | 18/30 [01:27<00:58,  4.92s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  63%|██████▎   | 19/30 [01:32<00:54,  4.95s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  67%|██████▋   | 20/30 [01:37<00:49,  4.99s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  70%|███████   | 21/30 [01:42<00:45,  5.00s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  73%|███████▎  | 22/30 [01:48<00:40,  5.05s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  77%|███████▋  | 23/30 [01:53<00:35,  5.05s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  80%|████████  | 24/30 [01:58<00:30,  5.08s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  83%|████████▎ | 25/30 [02:03<00:25,  5.08s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  87%|████████▋ | 26/30 [02:08<00:20,  5.10s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  90%|█████████ | 27/30 [02:13<00:15,  5.09s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  93%|█████████▎| 28/30 [02:18<00:10,  5.10s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  97%|█████████▋| 29/30 [02:23<00:05,  5.10s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin: 100%|██████████| 30/30 [02:28<00:00,  5.10s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin: 100%|██████████| 30/30 [02:28<00:00,  4.96s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 29 files...\n",
      "  ✓ Extracted 10,592 daily records in 148.9 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "  Exporting to CSV...\n",
      "  ✓ Exported to CSV: Anameka South_ACCESS CM2_tasmin_SSP245.csv\n",
      "  ✓ Rows: 10,592 | Time: 148.9s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Task 6/6: Processing pr (SSP245)\n",
      "----------------------------------------------------------------------\n",
      "  Found 30 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:   3%|▎         | 1/30 [00:04<02:20,  4.84s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:   7%|▋         | 2/30 [00:08<01:59,  4.26s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  10%|█         | 3/30 [00:10<01:26,  3.19s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  13%|█▎        | 4/30 [00:12<01:09,  2.69s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  17%|█▋        | 5/30 [00:14<01:01,  2.45s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  20%|██        | 6/30 [00:16<00:54,  2.27s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  23%|██▎       | 7/30 [00:18<00:49,  2.14s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  27%|██▋       | 8/30 [00:20<00:45,  2.09s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  30%|███       | 9/30 [00:22<00:42,  2.04s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  33%|███▎      | 10/30 [00:24<00:39,  2.00s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  37%|███▋      | 11/30 [00:26<00:37,  1.99s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  40%|████      | 12/30 [00:28<00:35,  1.98s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  43%|████▎     | 13/30 [00:30<00:33,  1.96s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  47%|████▋     | 14/30 [00:32<00:31,  1.97s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  50%|█████     | 15/30 [00:34<00:29,  1.99s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  53%|█████▎    | 16/30 [00:36<00:28,  2.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  57%|█████▋    | 17/30 [00:39<00:33,  2.57s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  60%|██████    | 18/30 [00:44<00:39,  3.27s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  63%|██████▎   | 19/30 [00:49<00:40,  3.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  67%|██████▋   | 20/30 [00:54<00:40,  4.01s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  70%|███████   | 21/30 [00:59<00:39,  4.35s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  73%|███████▎  | 22/30 [01:04<00:36,  4.59s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  77%|███████▋  | 23/30 [01:09<00:32,  4.71s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  80%|████████  | 24/30 [01:14<00:28,  4.80s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  83%|████████▎ | 25/30 [01:19<00:24,  4.90s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  87%|████████▋ | 26/30 [01:25<00:20,  5.03s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  90%|█████████ | 27/30 [01:30<00:15,  5.16s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  93%|█████████▎| 28/30 [01:34<00:09,  4.84s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr:  97%|█████████▋| 29/30 [01:39<00:04,  4.98s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr: 100%|██████████| 30/30 [01:44<00:00,  4.97s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  pr: 100%|██████████| 30/30 [01:44<00:00,  3.50s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 29 files...\n",
      "  ✓ Extracted 10,592 daily records in 104.9 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "  Exporting to CSV...\n",
      "  ✓ Exported to CSV: Anameka South_ACCESS CM2_pr_SSP245.csv\n",
      "  ✓ Rows: 10,592 | Time: 105.0s\n",
      "\n",
      "======================================================================\n",
      "ALL TASKS COMPLETED\n",
      "======================================================================\n",
      "Total processing time: 694.6 seconds (11.6 minutes)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each scenario and variable\n",
    "results_summary = {}\n",
    "total_start_time = time.time()\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"STARTING EXTRACTION PROCESS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total tasks: {len(SCENARIOS) * len(VARIABLES)} (2 scenarios × 3 variables)\")\n",
    "print(f\"Target coordinate: ({TARGET_LAT}, {TARGET_LON})\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "task_num = 0\n",
    "total_tasks = len(SCENARIOS) * len(VARIABLES)\n",
    "\n",
    "for scenario in SCENARIOS:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing Scenario: {scenario}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Select directory based on scenario\n",
    "    if scenario == 'SSP585':\n",
    "        base_dir = SSP585_DIR\n",
    "    elif scenario == 'SSP245':\n",
    "        base_dir = SSP245_DIR\n",
    "    else:\n",
    "        print(f\"  ERROR: Unknown scenario {scenario}\")\n",
    "        continue\n",
    "    \n",
    "    for variable in VARIABLES:\n",
    "        task_num += 1\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        print(f\"Task {task_num}/{total_tasks}: Processing {variable} ({scenario})\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        \n",
    "        task_start_time = time.time()\n",
    "        \n",
    "        # Construct directory path for this variable\n",
    "        var_dir = os.path.join(base_dir, f\"{variable}_ACCESS CM2 {scenario}\")\n",
    "        \n",
    "        if not os.path.exists(var_dir):\n",
    "            print(f\"  ERROR: Directory not found: {var_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Extract daily data\n",
    "        daily_data = extract_daily_data_from_netcdf(\n",
    "            var_dir,\n",
    "            variable,\n",
    "            TARGET_LAT,\n",
    "            TARGET_LON,\n",
    "            tolerance=COORD_TOLERANCE\n",
    "        )\n",
    "        \n",
    "        if daily_data is None or len(daily_data) == 0:\n",
    "            print(f\"  WARNING: No data extracted for {variable} ({scenario})\")\n",
    "            continue\n",
    "        \n",
    "        # Prepare output filename\n",
    "        output_filename = f\"Anameka South_ACCESS CM2_{variable}_{scenario}.csv\"\n",
    "        output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "        \n",
    "        # Export to CSV (keep only date and value columns for tidy format)\n",
    "        print(f\"  Exporting to CSV...\")\n",
    "        output_df = daily_data[['date', 'value']].copy()\n",
    "        output_df.to_csv(\n",
    "            output_path,\n",
    "            index=False,\n",
    "            encoding='utf-8',\n",
    "            float_format='%.6f'\n",
    "        )\n",
    "        \n",
    "        task_elapsed = time.time() - task_start_time\n",
    "        print(f\"  ✓ Exported to CSV: {os.path.basename(output_path)}\")\n",
    "        print(f\"  ✓ Rows: {len(output_df):,} | Time: {task_elapsed:.1f}s\")\n",
    "        \n",
    "        # Store summary\n",
    "        key = f\"{variable}_{scenario}\"\n",
    "        results_summary[key] = {\n",
    "            'rows': len(output_df),\n",
    "            'date_range': f\"{output_df['date'].min()} to {output_df['date'].max()}\",\n",
    "            'output_file': output_filename,\n",
    "            'time_seconds': task_elapsed\n",
    "        }\n",
    "\n",
    "total_elapsed = time.time() - total_start_time\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"ALL TASKS COMPLETED\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total processing time: {total_elapsed:.1f} seconds ({total_elapsed/60:.1f} minutes)\")\n",
    "print(f\"{'='*70}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-15T02:24:20.571526Z",
     "iopub.status.busy": "2025-12-15T02:24:20.571316Z",
     "iopub.status.idle": "2025-12-15T02:24:20.580561Z",
     "shell.execute_reply": "2025-12-15T02:24:20.579214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXTRACTION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Target coordinate: (-31.75, 117.5999984741211)\n",
      "\n",
      "Files processed:\n",
      "\n",
      "  TASMAX (SSP585):\n",
      "      Rows: 10592\n",
      "      Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "      Output file: Anameka South_ACCESS CM2_tasmax_SSP585.csv\n",
      "\n",
      "  TASMIN (SSP585):\n",
      "      Rows: 9497\n",
      "      Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "      Output file: Anameka South_ACCESS CM2_tasmin_SSP585.csv\n",
      "\n",
      "  PR (SSP585):\n",
      "      Rows: 9862\n",
      "      Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "      Output file: Anameka South_ACCESS CM2_pr_SSP585.csv\n",
      "\n",
      "  TASMAX (SSP245):\n",
      "      Rows: 10592\n",
      "      Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "      Output file: Anameka South_ACCESS CM2_tasmax_SSP245.csv\n",
      "\n",
      "  TASMIN (SSP245):\n",
      "      Rows: 10592\n",
      "      Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "      Output file: Anameka South_ACCESS CM2_tasmin_SSP245.csv\n",
      "\n",
      "  PR (SSP245):\n",
      "      Rows: 10592\n",
      "      Date range: 2035-01-01 00:00:00 to 2063-12-31 00:00:00\n",
      "      Output file: Anameka South_ACCESS CM2_pr_SSP245.csv\n",
      "\n",
      "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\n",
      "\n",
      "All CSV files exported successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXTRACTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTarget coordinate: ({TARGET_LAT}, {TARGET_LON})\")\n",
    "print(f\"\\nFiles processed:\")\n",
    "for key, summary in results_summary.items():\n",
    "    var, scen = key.split('_', 1)\n",
    "    print(f\"\\n  {var.upper()} ({scen}):\")\n",
    "    print(f\"      Rows: {summary['rows']}\")\n",
    "    print(f\"      Date range: {summary['date_range']}\")\n",
    "    print(f\"      Output file: {summary['output_file']}\")\n",
    "\n",
    "print(f\"\\nOutput directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nAll CSV files exported successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
