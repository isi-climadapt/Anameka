{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACCESS MET File Creation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook creates an ACCESS APSIM MET file by merging data from:\n",
        "\n",
        "1. **Base variables** (tasmax, tasmin, pr, rsds) - extracted from CMIP6 NetCDF files\n",
        "   - **Caching:** Extracted data is automatically cached to CSV files for faster subsequent runs\n",
        "2. **Vapor Pressure (VP)** - loaded from CSV file generated by `CMPI6_VP_Calculation.ipynb`\n",
        "3. **Evaporation (Evap)** - loaded from CSV file generated by `CMPI6_Evap_Calculation.ipynb` (which already incorporates sfcWind)\n",
        "\n",
        "## Output\n",
        "\n",
        "ACCESS MET file with all required variables:\n",
        "- **maxt**: Maximum temperature (°C)\n",
        "- **mint**: Minimum temperature (°C)\n",
        "- **rain**: Precipitation (mm)\n",
        "- **radn**: Radiation (MJ/m²)\n",
        "- **evap**: Class A pan evaporation (mm/day)\n",
        "- **vp**: Vapor pressure (hPa)\n",
        "- **code**: Hardcoded to '222222'\n",
        "\n",
        "## Configuration\n",
        "\n",
        "Only change scenario, coordinates, and output directory at the top. All other settings adjust automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import math\n",
        "import calendar\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [INFO] Using manual output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\n",
            "======================================================================\n",
            "CONFIGURATION\n",
            "======================================================================\n",
            "  Model: ACCESS CM2\n",
            "  Scenario: SSP585\n",
            "  Coordinates: (-31.750000, 117.600000)\n",
            "  Date Range: 1986 to 2014\n",
            "  CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
            "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\n",
            "  Base Variables: tasmax, tasmin, pr, rsds\n",
            "  VP Variables: hurs\n",
            "  Evap Variables: hurs\n",
            "======================================================================\n",
            "\n",
            "All paths and filenames will automatically use the above settings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CONFIGURATION - CHANGE VALUES BELOW AS NEEDED\n",
        "# ============================================================================\n",
        "# All other settings will automatically adjust based on these values\n",
        "\n",
        "# Model (usually doesn't need to change)\n",
        "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
        "\n",
        "# Scenario - CHANGE THIS\n",
        "SCENARIO = \"SSP585\"   # Options: \"obs\", \"SSP245\", \"SSP585\", etc.\n",
        "\n",
        "# Coordinates - CHANGE THESE\n",
        "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
        "LONGITUDE = 117.60  # Target longitude in decimal degrees (-180 to 180)\n",
        "\n",
        "# Date Range - CHANGE THESE\n",
        "START_YEAR = 1986   # Start year for MET file (e.g., 1985 for obs, 2035 for future scenarios)\n",
        "END_YEAR = 2014     # End year for MET file (e.g., 2014 for obs, 2064 for future, or None for all available data)\n",
        "\n",
        "# Output Directory - OPTIONAL: Set to None to auto-generate, or specify a custom path\n",
        "OUTPUT_DIR_MANUAL = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\"  # Set to None for auto-generation\n",
        "\n",
        "# ============================================================================\n",
        "# AUTOMATIC SETTINGS (derived from above - no need to change)\n",
        "# ============================================================================\n",
        "\n",
        "# Base directories\n",
        "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
        "base_output_dir = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\"\n",
        "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
        "\n",
        "# Auto-generate output directory and filename components based on scenario and coordinates\n",
        "lat_str = f\"{LATITUDE:.2f}\".replace('.', '_').replace('-', 'neg')\n",
        "lon_str = f\"{LONGITUDE:.2f}\".replace('.', '_').replace('-', 'neg')\n",
        "model_scenario = f\"{MODEL.replace(' ', '_')}_{SCENARIO}\"\n",
        "model_scenario_dir = f\"{model_scenario}_{lat_str}_{lon_str}\"\n",
        "\n",
        "# Use manual output directory if specified, otherwise auto-generate\n",
        "if OUTPUT_DIR_MANUAL is not None and OUTPUT_DIR_MANUAL != \"\":\n",
        "    OUTPUT_DIR = OUTPUT_DIR_MANUAL\n",
        "    print(f\"  [INFO] Using manual output directory: {OUTPUT_DIR}\")\n",
        "else:\n",
        "    OUTPUT_DIR = os.path.join(base_output_dir, model_scenario_dir)\n",
        "    print(f\"  [INFO] Auto-generated output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Variables required for ACCESS MET file\n",
        "# Base variables (from CMIP6)\n",
        "BASE_VARIABLES = ['tasmax', 'tasmin', 'pr', 'rsds']\n",
        "# Variables for VP calculation\n",
        "VP_VARIABLES = ['hurs']  # VP calculated from hurs, tasmax, tasmin\n",
        "# Variables for Evap calculation\n",
        "# Note: sfcWind is not extracted here - it's already used in CMPI6_Evap_Calculation.ipynb\n",
        "EVAP_VARIABLES = ['hurs']  # hurs used for evap calculation (sfcWind already incorporated in evap CSV)\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Model: {MODEL}\")\n",
        "print(f\"  Scenario: {SCENARIO}\")\n",
        "print(f\"  Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
        "print(f\"  Date Range: {START_YEAR} to {END_YEAR if END_YEAR is not None else 'end of data'}\")\n",
        "print(f\"  CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Base Variables: {', '.join(BASE_VARIABLES)}\")\n",
        "print(f\"  VP Variables: {', '.join(VP_VARIABLES)}\")\n",
        "print(f\"  Evap Variables: {', '.join(EVAP_VARIABLES)}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAll paths and filenames will automatically use the above settings.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: NetCDF Data Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Find all NetCDF files in the directory\n",
        "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
        "    \n",
        "    # Pattern 2: Files in subdirectories named {variable}_*\n",
        "    if len(nc_files) == 0:\n",
        "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
        "        for var_subdir in var_subdirs:\n",
        "            if os.path.isdir(var_subdir):\n",
        "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
        "                    break\n",
        "    \n",
        "    # For rsds, also check rad_* folders\n",
        "    if len(nc_files) == 0 and variable == 'rsds':\n",
        "        rad_subdirs = glob.glob(os.path.join(netcdf_dir, \"rad_*\"))\n",
        "        for rad_subdir in rad_subdirs:\n",
        "            if os.path.isdir(rad_subdir):\n",
        "                found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*rsds*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
        "                    break\n",
        "    \n",
        "    # For sfcWind, also check wind_* folders\n",
        "    if len(nc_files) == 0 and variable == 'sfcWind':\n",
        "        wind_subdirs = glob.glob(os.path.join(netcdf_dir, \"wind_*\"))\n",
        "        for wind_subdir in wind_subdirs:\n",
        "            if os.path.isdir(wind_subdir):\n",
        "                found_files = sorted(glob.glob(os.path.join(wind_subdir, \"*sfcWind*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(wind_subdir)}/\")\n",
        "                    break\n",
        "        # If no files found with sfcWind in name, try any .nc files in wind_* directory\n",
        "        if len(nc_files) == 0:\n",
        "            for wind_subdir in wind_subdirs:\n",
        "                if os.path.isdir(wind_subdir):\n",
        "                    found_files = sorted(glob.glob(os.path.join(wind_subdir, \"*.nc\")))\n",
        "                    if found_files:\n",
        "                        nc_files.extend(found_files)\n",
        "                        print(f\"  Found files in subdirectory: {os.path.basename(wind_subdir)}/\")\n",
        "                        break\n",
        "    \n",
        "    if len(nc_files) == 0:\n",
        "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
        "    \n",
        "    # Cache coordinate information from first file\n",
        "    lat_name = None\n",
        "    lon_name = None\n",
        "    time_name = None\n",
        "    lat_idx = None\n",
        "    lon_idx = None\n",
        "    var_name = None\n",
        "    \n",
        "    # List to store daily data\n",
        "    all_data = []\n",
        "    \n",
        "    # Process first file to get coordinate structure\n",
        "    if len(nc_files) > 0:\n",
        "        try:\n",
        "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
        "            \n",
        "            # Get variable name\n",
        "            for v in ds_sample.data_vars:\n",
        "                if variable in v.lower() or v.lower() in variable.lower():\n",
        "                    var_name = v\n",
        "                    break\n",
        "            \n",
        "            if var_name is None:\n",
        "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
        "                if variable == 'rsds':\n",
        "                    possible_names.extend(['rad', 'RAD', 'rad_day'])\n",
        "                for name in possible_names:\n",
        "                    if name in ds_sample.data_vars:\n",
        "                        var_name = name\n",
        "                        break\n",
        "            \n",
        "            # Get coordinate names\n",
        "            for coord in ds_sample.coords:\n",
        "                coord_lower = coord.lower()\n",
        "                if 'lat' in coord_lower:\n",
        "                    lat_name = coord\n",
        "                elif 'lon' in coord_lower:\n",
        "                    lon_name = coord\n",
        "                elif 'time' in coord_lower:\n",
        "                    time_name = coord\n",
        "            \n",
        "            if lat_name and lon_name:\n",
        "                # Find nearest grid point\n",
        "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
        "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
        "                \n",
        "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
        "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
        "                \n",
        "                # Check if within tolerance\n",
        "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
        "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
        "                else:\n",
        "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
        "            \n",
        "            ds_sample.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not read sample file: {e}\")\n",
        "    \n",
        "    if var_name is None or lat_idx is None or lon_idx is None:\n",
        "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
        "        return None\n",
        "    \n",
        "    # Process all files with progress bar\n",
        "    print(f\"  Processing files...\")\n",
        "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
        "        try:\n",
        "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
        "            \n",
        "            # Extract data using cached indices\n",
        "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
        "            \n",
        "            # Convert to numpy array\n",
        "            values = data.values\n",
        "            if values.ndim > 1:\n",
        "                values = values.flatten()\n",
        "            \n",
        "            # Get time values - extract year from filename\n",
        "            year = None\n",
        "            filename = os.path.basename(nc_file)\n",
        "            year_match = re.search(r'(\\d{4})', filename)\n",
        "            if year_match:\n",
        "                year = int(year_match.group(1))\n",
        "                # Create daily dates for the year (handles leap years automatically)\n",
        "                time_values = pd.date_range(start=f'{year}-01-01', end=f'{year}-12-31', freq='D')\n",
        "            else:\n",
        "                time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
        "            \n",
        "            # Ensure correct number of dates\n",
        "            if len(time_values) != len(values):\n",
        "                if len(time_values) > len(values):\n",
        "                    time_values = time_values[:len(values)]\n",
        "            \n",
        "            # Create DataFrame for this file\n",
        "            if len(values) > 0:\n",
        "                df_file = pd.DataFrame({\n",
        "                    'date': time_values[:len(values)],\n",
        "                    'value': values\n",
        "                })\n",
        "                all_data.append(df_file)\n",
        "            \n",
        "            ds.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(all_data) == 0:\n",
        "        print(f\"  ERROR: No data extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Combine all data\n",
        "    print(f\"  Combining data from {len(all_data)} files...\")\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "        # Remove duplicate dates (keep first occurrence)\n",
        "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
        "    \n",
        "    # Check for constant values by year (indicates missing/corrupted data in source files)\n",
        "    combined_df['year'] = pd.to_datetime(combined_df['date']).dt.year\n",
        "    for year in combined_df['year'].unique():\n",
        "        year_data = combined_df[combined_df['year'] == year]['value']\n",
        "        non_null_data = year_data.dropna()\n",
        "        if len(non_null_data) > 10:  # Only check if we have enough data points\n",
        "            if non_null_data.nunique() == 1:\n",
        "                constant_value = non_null_data.iloc[0]\n",
        "                num_days = len(non_null_data)\n",
        "                print(f\"  [WARNING] Year {year} has constant {variable} value: {constant_value:.2f} for {num_days} days (likely missing/corrupted source data)\")\n",
        "    combined_df = combined_df.drop(columns=['year'])\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
        "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
        "    \n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Caching Functions\n",
        "\n",
        "To optimize performance, extracted NetCDF data is cached to CSV files. On subsequent runs, \n",
        "cached data is loaded automatically instead of re-extracting from NetCDF files.\n",
        "\n",
        "**Cache Location:** Cached files are saved in the output directory with naming convention:\n",
        "`{model_scenario}_{lat_str}_{lon_str}_{variable}.csv`\n",
        "\n",
        "**Cache Behavior:**\n",
        "- If cache exists and is valid → Load from cache (fast)\n",
        "- If cache doesn't exist or is invalid → Extract from NetCDF files and save to cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Load VP and Evap Data from Calculation Notebooks\n",
        "\n",
        "This notebook loads Vapor Pressure (VP) and Evaporation (Evap) data from the CSV files generated by:\n",
        "- **`CMPI6_VP_Calculation.ipynb`** - Calculates VP using SILO method\n",
        "- **`CMPI6_Evap_Calculation.ipynb`** - Calculates Evap using FAO-56 Penman-Monteith with SILO calibration\n",
        "\n",
        "**Important:** Before running this notebook, ensure that:\n",
        "1. `CMPI6_VP_Calculation.ipynb` has been executed and generated the VP CSV file\n",
        "2. `CMPI6_Evap_Calculation.ipynb` has been executed and generated the Evap CSV file\n",
        "\n",
        "The CSV files are expected to be in the same output directory and follow the naming convention:\n",
        "- VP: `{model_scenario}_{lat_str}_{lon_str}_vp.csv`\n",
        "- Evap: `{model_scenario}_{lat_str}_{lon_str}_evap.csv`\n",
        "\n",
        "Where:\n",
        "- `model_scenario` = `{MODEL.replace(' ', '_')}_{SCENARIO}` (e.g., \"ACCESS_CM2_SSP245\")\n",
        "- `lat_str` = latitude formatted as string (e.g., \"neg31_75\")\n",
        "- `lon_str` = longitude formatted as string (e.g., \"117_60\")\n",
        "\n",
        "### Vapor Pressure (VP) Calculation - SILO Method\n",
        "\n",
        "**Input Variables:**\n",
        "- **hurs**: Mean relative humidity (%) - required\n",
        "- **tasmax**: Daily maximum temperature (°C) - required\n",
        "- **tasmin**: Daily minimum temperature (°C) - required\n",
        "\n",
        "**Output:**\n",
        "- **vp**: Vapor pressure (hPa) matching SILO units\n",
        "\n",
        "**Calculation Method:**\n",
        "\n",
        "#### Saturation Vapor Pressure (kPa)\n",
        "\n",
        "For mean temperature T_mean in °C:\n",
        "\n",
        "```\n",
        "e_s(T_mean) = 0.611 × exp(17.27 × T_mean / (T_mean + 237.3))\n",
        "```\n",
        "\n",
        "Where: T_mean = (tasmax + tasmin) / 2\n",
        "\n",
        "#### Actual Vapor Pressure (hPa)\n",
        "\n",
        "Using mean relative humidity (hurs) and saturation vapor pressure:\n",
        "\n",
        "```\n",
        "VP(hPa) = 10 × (hurs/100) × e_s(T_mean)\n",
        "```\n",
        "\n",
        "Or directly in hPa:\n",
        "\n",
        "```\n",
        "VP(hPa) = (hurs/100) × 0.611 × exp(17.27 × T_mean / (T_mean + 237.3)) × 10\n",
        "```\n",
        "\n",
        "This gives a daily VP proxy. SILO VP is \"9am-like\"; to replicate SILO closely, you would bias-correct this VP proxy to SILO over a historical overlap period.\n",
        "\n",
        "---\n",
        "\n",
        "### Evaporation (Evap) Calculation - FAO-56 Penman-Monteith\n",
        "\n",
        "**Key Principle:**\n",
        "\n",
        "Class A pan evaporation cannot be directly calculated from CMIP6. Instead, we:\n",
        "- Compute a physically based evaporation proxy (ET₀) from CMIP6 meteorology\n",
        "- Calibrate (bias-correct) it to SILO pan evaporation over a historical overlap period\n",
        "\n",
        "This is the standard and accepted approach in Australian crop modelling.\n",
        "\n",
        "**Input Variables (CMIP6):**\n",
        "- **tasmax**: Daily maximum temperature (°C) - required\n",
        "- **tasmin**: Daily minimum temperature (°C) - required\n",
        "- **hurs**: Daily mean relative humidity (%) - required\n",
        "- **rsds**: Surface downwelling shortwave radiation (W/m²) - required\n",
        "- **sfcWind**: Wind speed at 10m (m/s) - used in `CMPI6_Evap_Calculation.ipynb` (already incorporated in evap CSV)\n",
        "\n",
        "**Output:**\n",
        "- **evap**: Class A pan evaporation (mm/day) - SILO-scaled and APSIM-compatible\n",
        "\n",
        "**Calculation Method:**\n",
        "\n",
        "#### Step 1: Calculate ET₀ (FAO-56 Penman-Monteith)\n",
        "\n",
        "ET₀ (mm/day) = physically based evaporation driven by radiation + temperature + humidity + wind\n",
        "\n",
        "Formula:\n",
        "```\n",
        "ET₀ = (0.408 × Δ × (Rn - G) + γ × (900/(T+273)) × u₂ × (es - ea)) / (Δ + γ × (1 + 0.34 × u₂))\n",
        "```\n",
        "\n",
        "Where:\n",
        "- `Δ` = slope of vapor pressure curve (kPa/°C) = 4098 × es / (T + 237.3)²\n",
        "- `Rn` = net radiation (MJ/m²/day) ≈ 0.77 × Rs (where Rs is solar radiation converted from W/m² to MJ/m²/day)\n",
        "- `G` = soil heat flux (MJ/m²/day) - assumed 0 for daily calculations\n",
        "- `γ` = psychrometric constant (kPa/°C) = 0.665e-3 × 101.3 ≈ 0.0675 kPa/°C\n",
        "- `T` = mean air temperature (°C) = (tasmax + tasmin) / 2\n",
        "- `u₂` = wind speed at 2m height (m/s) - converted from 10m using logarithmic wind profile\n",
        "- `es` = saturation vapor pressure (kPa) at mean temperature\n",
        "- `ea` = actual vapor pressure (kPa) calculated from hurs (mean relative humidity), tasmax, tasmin\n",
        "\n",
        "**Wind Speed Conversion (10m to 2m):**\n",
        "```\n",
        "u₂ = u₁₀ × ln(2/z₀) / ln(10/z₀)\n",
        "```\n",
        "Where z₀ = 0.0002 m (roughness length for open water/pan)\n",
        "\n",
        "**Actual Vapor Pressure (for Evap):**\n",
        "```\n",
        "ea = es(T_mean) × hurs / 100\n",
        "```\n",
        "Where T_mean = (tasmax + tasmin) / 2\n",
        "\n",
        "#### Step 2: Monthly Calibration to SILO Pan Evaporation\n",
        "\n",
        "For each calendar month:\n",
        "```\n",
        "pan_evap = a_m × ET₀ + b_m\n",
        "```\n",
        "\n",
        "Where `a_m` and `b_m` are fitted using SILO evap_pan over the historical overlap period (1981-2010 or 1991-2020).\n",
        "\n",
        "#### Step 3: Apply Calibration to Future Projections\n",
        "\n",
        "Compute ET₀ using the same formulas, then apply the monthly calibration coefficients.\n",
        "\n",
        "**Justification:**\n",
        "\n",
        "\"Daily pan evaporation was derived from CMIP6 meteorological variables by first calculating FAO-56 reference evapotranspiration and then bias-correcting to SILO Class A pan evaporation over a historical overlap period, ensuring consistency with APSIM soil-water and crop-growth parameterisation.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cached_variable_path(output_dir, model_scenario, lat_str, lon_str, variable):\n",
        "    \"\"\"\n",
        "    Generate the path for a cached variable CSV file.\n",
        "    Tries multiple naming formats to match existing cache files.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_dir : str\n",
        "        Output directory\n",
        "    model_scenario : str\n",
        "        Model and scenario string (e.g., \"ACCESS_CM2_SSP245\")\n",
        "    lat_str : str\n",
        "        Latitude formatted as string (e.g., \"neg31_45\")\n",
        "    lon_str : str\n",
        "        Longitude formatted as string (e.g., \"117_55\")\n",
        "    variable : str\n",
        "        Variable name (e.g., \"tasmax\", \"hurs\")\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Path to cached CSV file (tries decimal format first, then underscore format)\n",
        "    \"\"\"\n",
        "    # Try decimal format first (matches existing files: -31.45_117.55)\n",
        "    # Convert from underscore format to decimal format\n",
        "    if 'neg' in lat_str:\n",
        "        lat_decimal = lat_str.replace('neg', '-').replace('_', '.')\n",
        "    elif '_' in lat_str:\n",
        "        lat_decimal = lat_str.replace('_', '.')\n",
        "    else:\n",
        "        lat_decimal = lat_str\n",
        "    \n",
        "    if '_' in lon_str:\n",
        "        lon_decimal = lon_str.replace('_', '.')\n",
        "    else:\n",
        "        lon_decimal = lon_str\n",
        "    \n",
        "    # First try: decimal format (e.g., ACCESS_CM2_obs_-31.45_117.55_tasmax.csv)\n",
        "    cache_filename_decimal = f\"{model_scenario}_{lat_decimal}_{lon_decimal}_{variable}.csv\"\n",
        "    cache_path_decimal = os.path.join(output_dir, cache_filename_decimal)\n",
        "    \n",
        "    # Check if decimal format exists\n",
        "    if os.path.exists(cache_path_decimal):\n",
        "        return cache_path_decimal\n",
        "    \n",
        "    # Second try: underscore format (e.g., ACCESS_CM2_obs_neg31_45_117_55_tasmax.csv)\n",
        "    cache_filename_underscore = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
        "    cache_path_underscore = os.path.join(output_dir, cache_filename_underscore)\n",
        "    \n",
        "    # Return the decimal format path (even if it doesn't exist yet) as it matches existing files\n",
        "    # The load_cached_variable function will handle the file not found case\n",
        "    return cache_path_decimal\n",
        "\n",
        "\n",
        "def load_cached_variable(cache_path):\n",
        "    \"\"\"\n",
        "    Load cached variable data from CSV file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    cache_path : str\n",
        "        Path to cached CSV file\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame or None\n",
        "        DataFrame with date and value columns if file exists and is valid, None otherwise\n",
        "    \"\"\"\n",
        "    if not os.path.exists(cache_path):\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(cache_path)\n",
        "        if 'date' not in df.columns or 'value' not in df.columns:\n",
        "            print(f\"  [WARNING] Cached file missing required columns, will re-extract\")\n",
        "            return None\n",
        "        \n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        # Normalize dates to date-only (remove time component) to ensure proper merging\n",
        "        df['date'] = df['date'].dt.normalize()\n",
        "        \n",
        "        # Basic validation - check if file has data\n",
        "        if len(df) == 0:\n",
        "            print(f\"  [WARNING] Cached file is empty, will re-extract\")\n",
        "            return None\n",
        "        \n",
        "        # Check if values are all zero (might indicate a problem)\n",
        "        if 'value' in df.columns:\n",
        "            non_zero_count = (df['value'] != 0).sum()\n",
        "            if non_zero_count == 0 and len(df) > 0:\n",
        "                print(f\"  [WARNING] All values in cache are zero! This may indicate a problem with the cache file.\")\n",
        "            else:\n",
        "                print(f\"  [INFO] Cache loaded: {len(df):,} records, {non_zero_count:,} non-zero values\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"  [WARNING] Error loading cached file: {e}, will re-extract\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def save_cached_variable(df, cache_path):\n",
        "    \"\"\"\n",
        "    Save extracted variable data to CSV cache file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with date and value columns\n",
        "    cache_path : str\n",
        "        Path to save cached CSV file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df[['date', 'value']].to_csv(\n",
        "            cache_path,\n",
        "            index=False,\n",
        "            encoding='utf-8',\n",
        "            float_format='%.6f'\n",
        "        )\n",
        "        print(f\"  [INFO] Saved to cache: {os.path.basename(cache_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [WARNING] Failed to save cache: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_vp_from_csv(output_dir, model_scenario, lat_str, lon_str):\n",
        "    \"\"\"\n",
        "    Load Vapor Pressure (VP) data from CSV file generated by CMPI6_VP_Calculation.ipynb.\n",
        "    \n",
        "    This function searches for VP CSV files with flexible naming patterns to handle\n",
        "    different coordinate formatting conventions.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_dir : str\n",
        "        Directory containing the VP CSV file\n",
        "    model_scenario : str\n",
        "        Model and scenario string (e.g., \"ACCESS_CM2_SSP245\")\n",
        "    lat_str : str\n",
        "        Latitude formatted as string (e.g., \"neg31_75\")\n",
        "    lon_str : str\n",
        "        Longitude formatted as string (e.g., \"117_60\")\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with date and value (vapor pressure hPa) columns, or None if file not found\n",
        "    \"\"\"\n",
        "    # Try new format first (decimal coordinates with neg prefix: neg31.75_117.60)\n",
        "    # Convert from underscore format to decimal format, keeping 'neg' prefix\n",
        "    # If lat_str uses underscores (neg31_75), convert to neg31.75\n",
        "    if '_' in lat_str and 'neg' in lat_str:\n",
        "        lat_str_decimal = lat_str.replace('_', '.').replace('neg', 'neg')\n",
        "    elif '_' in lat_str:\n",
        "        lat_str_decimal = lat_str.replace('_', '.').replace('-', 'neg')\n",
        "    else:\n",
        "        lat_str_decimal = lat_str.replace('-', 'neg')\n",
        "    lon_str_decimal = lon_str.replace('_', '.')\n",
        "    vp_filename_new = f\"{model_scenario}_{lat_str_decimal}_{lon_str_decimal}_vp.csv\"\n",
        "    vp_path = os.path.join(output_dir, vp_filename_new)\n",
        "    \n",
        "    # If new format doesn't exist, try old format (neg31_75_117_60)\n",
        "    if not os.path.exists(vp_path):\n",
        "        vp_filename_old = f\"{model_scenario}_{lat_str}_{lon_str}_vp.csv\"\n",
        "        vp_path = os.path.join(output_dir, vp_filename_old)\n",
        "        \n",
        "        # If still not found, search for any matching VP file\n",
        "        if not os.path.exists(vp_path):\n",
        "            matching_files = glob.glob(os.path.join(output_dir, f\"{model_scenario}*_vp.csv\"))\n",
        "            if matching_files:\n",
        "                vp_path = matching_files[0]\n",
        "                print(f\"  [INFO] Found VP file with alternative naming: {os.path.basename(vp_path)}\")\n",
        "            else:\n",
        "                print(f\"  [WARNING] VP CSV file not found\")\n",
        "                print(f\"  [INFO] Expected formats:\")\n",
        "                print(f\"    - New format: {vp_filename_new}\")\n",
        "                print(f\"    - Old format: {vp_filename_old}\")\n",
        "                print(f\"  [INFO] Searched in: {output_dir}\")\n",
        "                print(f\"  [INFO] Please run CMPI6_VP_Calculation.ipynb first to generate the VP data\")\n",
        "                return None\n",
        "    \n",
        "    try:\n",
        "        vp_df = pd.read_csv(vp_path)\n",
        "        vp_df['date'] = pd.to_datetime(vp_df['date'])\n",
        "        print(f\"  [OK] Loaded VP data from: {os.path.basename(vp_path)}\")\n",
        "        print(f\"  [INFO] VP records: {len(vp_df):,} days\")\n",
        "        print(f\"  [INFO] Date range: {vp_df['date'].min()} to {vp_df['date'].max()}\")\n",
        "        print(f\"  [INFO] VP range: {vp_df['value'].min():.2f} to {vp_df['value'].max():.2f} hPa\")\n",
        "        print(f\"  [SUCCESS] VP data loaded successfully!\")\n",
        "        return vp_df\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] Failed to load VP CSV file: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def load_evap_from_csv(output_dir, model_scenario, lat_str, lon_str):\n",
        "    \"\"\"\n",
        "    Load Evaporation (Evap) data from CSV file generated by CMPI6_Evap_Calculation.ipynb.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_dir : str\n",
        "        Directory containing the Evap CSV file\n",
        "    model_scenario : str\n",
        "        Model and scenario string (e.g., \"ACCESS_CM2_SSP245\")\n",
        "    lat_str : str\n",
        "        Latitude formatted as string (e.g., \"neg31_75\")\n",
        "    lon_str : str\n",
        "        Longitude formatted as string (e.g., \"117_60\")\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with date and value (evaporation mm/day) columns, or None if file not found\n",
        "    \"\"\"\n",
        "    # Try new format first (decimal coordinates with neg prefix: neg31.75_117.60)\n",
        "    # Convert from underscore format to decimal format, keeping 'neg' prefix\n",
        "    # If lat_str uses underscores (neg31_75), convert to neg31.75\n",
        "    if '_' in lat_str and 'neg' in lat_str:\n",
        "        lat_str_decimal = lat_str.replace('_', '.').replace('neg', 'neg')\n",
        "    elif '_' in lat_str:\n",
        "        lat_str_decimal = lat_str.replace('_', '.').replace('-', 'neg')\n",
        "    else:\n",
        "        lat_str_decimal = lat_str.replace('-', 'neg')\n",
        "    lon_str_decimal = lon_str.replace('_', '.')\n",
        "    \n",
        "    # Try _eto.csv first (actual filename from CMPI6_Evap_Calculation.ipynb)\n",
        "    evap_filename_eto = f\"{model_scenario}_{lat_str_decimal}_{lon_str_decimal}_eto.csv\"\n",
        "    evap_path = os.path.join(output_dir, evap_filename_eto)\n",
        "    \n",
        "    # If _eto.csv doesn't exist, try _evap.csv (alternative naming)\n",
        "    if not os.path.exists(evap_path):\n",
        "        evap_filename_evap = f\"{model_scenario}_{lat_str_decimal}_{lon_str_decimal}_evap.csv\"\n",
        "        evap_path = os.path.join(output_dir, evap_filename_evap)\n",
        "    \n",
        "    # If still not found, try old format with underscores (neg31_75_117_60)\n",
        "    if not os.path.exists(evap_path):\n",
        "        evap_filename_old_eto = f\"{model_scenario}_{lat_str}_{lon_str}_eto.csv\"\n",
        "        evap_path = os.path.join(output_dir, evap_filename_old_eto)\n",
        "    \n",
        "    if not os.path.exists(evap_path):\n",
        "        evap_filename_old_evap = f\"{model_scenario}_{lat_str}_{lon_str}_evap.csv\"\n",
        "        evap_path = os.path.join(output_dir, evap_filename_old_evap)\n",
        "    \n",
        "    # If still not found, search for any matching Evap/ET₀ file\n",
        "    if not os.path.exists(evap_path):\n",
        "        matching_files = glob.glob(os.path.join(output_dir, f\"{model_scenario}*_evap.csv\"))\n",
        "        matching_files.extend(glob.glob(os.path.join(output_dir, f\"{model_scenario}*_eto.csv\")))\n",
        "        if matching_files:\n",
        "            evap_path = matching_files[0]\n",
        "            print(f\"  [INFO] Found Evap/ET₀ file with alternative naming: {os.path.basename(evap_path)}\")\n",
        "        else:\n",
        "            print(f\"  [WARNING] Evap CSV file not found\")\n",
        "            print(f\"  [INFO] Expected formats:\")\n",
        "            print(f\"    - ET₀ format: {evap_filename_eto}\")\n",
        "            print(f\"    - Evap format: {evap_filename_evap}\")\n",
        "            print(f\"    - Old ET₀ format: {evap_filename_old_eto}\")\n",
        "            print(f\"    - Old Evap format: {evap_filename_old_evap}\")\n",
        "            print(f\"  [INFO] Searched in: {output_dir}\")\n",
        "            print(f\"  [INFO] Please run CMPI6_Evap_Calculation.ipynb first to generate the ET₀/Evap data\")\n",
        "            return None\n",
        "    \n",
        "    try:\n",
        "        evap_df = pd.read_csv(evap_path)\n",
        "        evap_df['date'] = pd.to_datetime(evap_df['date'])\n",
        "        print(f\"  [OK] Loaded Evap data from: {os.path.basename(evap_path)}\")\n",
        "        print(f\"  [INFO] Evap records: {len(evap_df):,} days\")\n",
        "        print(f\"  [INFO] Date range: {evap_df['date'].min()} to {evap_df['date'].max()}\")\n",
        "        print(f\"  [INFO] Evap range: {evap_df['value'].min():.2f} to {evap_df['value'].max():.2f} mm/day\")\n",
        "        print(f\"  [SUCCESS] Evap data loaded successfully!\")\n",
        "        return evap_df\n",
        "    except Exception as e:\n",
        "        print(f\"  [ERROR] Failed to load Evap CSV file: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: MET File Creation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_tav_amp(df):\n",
        "    \"\"\"\n",
        "    Calculate annual average temperature (tav) and annual amplitude (amp).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with date index and maxt, mint columns\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (tav, amp)\n",
        "    \"\"\"\n",
        "    # Calculate mean temperature for each day\n",
        "    df['tmean'] = (df['maxt'] + df['mint']) / 2.0\n",
        "    \n",
        "    # Group by month and calculate monthly means\n",
        "    df['month'] = df.index.month\n",
        "    monthly_means = df.groupby('month')['tmean'].mean()\n",
        "    \n",
        "    # Calculate annual average (tav)\n",
        "    tav = monthly_means.mean()\n",
        "    \n",
        "    # Calculate annual amplitude (amp)\n",
        "    # amp = (max monthly mean - min monthly mean) / 2\n",
        "    amp = (monthly_means.max() - monthly_means.min()) / 2.0\n",
        "    \n",
        "    return tav, amp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_complete_met_file(tasmax_df, tasmin_df, pr_df, rsds_df, vp_df, evap_df, \n",
        "                            scenario=None, output_dir=None, latitude=None, \n",
        "                            longitude=None, model=None, start_year=None, end_year=None):\n",
        "    \"\"\"\n",
        "    Create ACCESS MET format file with all variables including VP and Evap.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    tasmax_df : pd.DataFrame\n",
        "        DataFrame with date and value (maximum temperature °C) columns\n",
        "    tasmin_df : pd.DataFrame\n",
        "        DataFrame with date and value (minimum temperature °C) columns\n",
        "    pr_df : pd.DataFrame\n",
        "        DataFrame with date and value (precipitation mm) columns\n",
        "    rsds_df : pd.DataFrame\n",
        "        DataFrame with date and value (solar radiation W/m²) columns\n",
        "    vp_df : pd.DataFrame\n",
        "        DataFrame with date and value (vapor pressure hPa) columns\n",
        "    evap_df : pd.DataFrame\n",
        "        DataFrame with date and value (evaporation mm/day) columns\n",
        "    scenario : str\n",
        "        Scenario name (e.g., \"SSP245\", \"SSP585\")\n",
        "    output_dir : str\n",
        "        Output directory path\n",
        "    latitude : float\n",
        "        Latitude in decimal degrees\n",
        "    longitude : float\n",
        "        Longitude in decimal degrees\n",
        "    model : str\n",
        "        Model name (e.g., \"ACCESS CM2\")\n",
        "    start_year : int, optional\n",
        "        Start year for MET file. Only data from this year onwards will be included.\n",
        "        If None, uses the minimum date from the data.\n",
        "    end_year : int, optional\n",
        "        End year for MET file. Only data up to this year will be included.\n",
        "        If None, uses the maximum date from the data.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (tav, amp, num_rows)\n",
        "    \"\"\"\n",
        "    # Merge all dataframes on date\n",
        "    # Normalize all dates to date-only (remove time component) to ensure proper merging\n",
        "    merged = tasmax_df.copy()\n",
        "    merged = merged.rename(columns={'value': 'maxt'})\n",
        "    merged['date'] = pd.to_datetime(merged['date']).dt.normalize()\n",
        "    print(f\"  [DEBUG] tasmax: {len(merged):,} records, maxt range: {merged['maxt'].min():.2f} to {merged['maxt'].max():.2f}\")\n",
        "    \n",
        "    # Merge tasmin\n",
        "    tasmin_df['date'] = pd.to_datetime(tasmin_df['date']).dt.normalize()\n",
        "    merged = merged.merge(tasmin_df[['date', 'value']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value': 'mint'})\n",
        "    print(f\"  [DEBUG] After tasmin merge: {len(merged):,} records, mint range: {merged['mint'].min():.2f} to {merged['mint'].max():.2f}\")\n",
        "    \n",
        "    # Merge pr (precipitation/rain)\n",
        "    pr_df['date'] = pd.to_datetime(pr_df['date']).dt.normalize()\n",
        "    merged = merged.merge(pr_df[['date', 'value']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value': 'rain'})\n",
        "    print(f\"  [DEBUG] After pr merge: {len(merged):,} records, rain range: {merged['rain'].min():.2f} to {merged['rain'].max():.2f}\")\n",
        "    \n",
        "    # Merge rsds (radiation) - REQUIRED\n",
        "    rsds_df['date'] = pd.to_datetime(rsds_df['date']).dt.normalize()\n",
        "    print(f\"  [DEBUG] rsds before merge: {len(rsds_df):,} records, value range: {rsds_df['value'].min():.2f} to {rsds_df['value'].max():.2f}\")\n",
        "    \n",
        "    # Check if rsds data has any non-zero values (diagnostic)\n",
        "    rsds_nonzero = (rsds_df['value'] != 0).sum()\n",
        "    rsds_total = len(rsds_df)\n",
        "    if rsds_nonzero == 0 and rsds_total > 0:\n",
        "        print(f\"  [WARNING] All rsds values are zero in source data! This may indicate missing/corrupted rsds NetCDF files.\")\n",
        "    elif rsds_nonzero < rsds_total * 0.1:\n",
        "        print(f\"  [WARNING] Only {rsds_nonzero:,} out of {rsds_total:,} rsds values are non-zero ({100*rsds_nonzero/rsds_total:.1f}%)\")\n",
        "    \n",
        "    # Convert W/m2 to MJ/m2 by multiplying by 0.0864 (seconds per day / 1e6)\n",
        "    rsds_df['value_mj'] = rsds_df['value'] * 0.0864\n",
        "    merged = merged.merge(rsds_df[['date', 'value_mj']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value_mj': 'radn'})\n",
        "    print(f\"  [DEBUG] After rsds merge: {len(merged):,} records, radn range: {merged['radn'].min():.2f} to {merged['radn'].max():.2f}\")\n",
        "    print(f\"  [DEBUG] Non-null counts - maxt: {merged['maxt'].notna().sum()}, mint: {merged['mint'].notna().sum()}, rain: {merged['rain'].notna().sum()}, radn: {merged['radn'].notna().sum()}\")\n",
        "    \n",
        "    # Diagnostic: Check radn values after merge\n",
        "    radn_nonzero_after_merge = (merged['radn'] != 0).sum() if 'radn' in merged.columns else 0\n",
        "    radn_total_after_merge = merged['radn'].notna().sum() if 'radn' in merged.columns else 0\n",
        "    if radn_total_after_merge > 0:\n",
        "        print(f\"  [INFO] After merge: {radn_nonzero_after_merge:,} non-zero radn values out of {radn_total_after_merge:,} total ({100*radn_nonzero_after_merge/radn_total_after_merge:.1f}%)\")\n",
        "    \n",
        "    # Merge VP (vapor pressure)\n",
        "    vp_df['date'] = pd.to_datetime(vp_df['date']).dt.normalize()\n",
        "    merged = merged.merge(vp_df[['date', 'value']], on='date', how='left')\n",
        "    merged = merged.rename(columns={'value': 'vp'})\n",
        "    merged['vp'] = pd.to_numeric(merged['vp'], errors='coerce')\n",
        "    \n",
        "    # Merge Evap (evaporation)\n",
        "    evap_df['date'] = pd.to_datetime(evap_df['date']).dt.normalize()\n",
        "    merged = merged.merge(evap_df[['date', 'value']], on='date', how='left')\n",
        "    merged = merged.rename(columns={'value': 'evap'})\n",
        "    merged['evap'] = pd.to_numeric(merged['evap'], errors='coerce')\n",
        "    \n",
        "    # Sort by date\n",
        "    merged = merged.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Diagnostic: Check radn values before filtering\n",
        "    if 'radn' in merged.columns and len(merged) > 0:\n",
        "        radn_nonzero_before = (merged['radn'] != 0).sum()\n",
        "        radn_total_before = merged['radn'].notna().sum()\n",
        "        radn_min = merged['radn'].min() if radn_total_before > 0 else None\n",
        "        radn_max = merged['radn'].max() if radn_total_before > 0 else None\n",
        "        if radn_total_before > 0:\n",
        "            print(f\"  [INFO] Before filtering: {radn_nonzero_before:,} non-zero radn values out of {radn_total_before:,} total\")\n",
        "            if radn_min is not None and radn_max is not None:\n",
        "                print(f\"  [INFO] Radn range before filtering: {radn_min:.2f} to {radn_max:.2f} MJ/m²\")\n",
        "        else:\n",
        "            print(f\"  [WARNING] No radn data available before filtering!\")\n",
        "    \n",
        "    # Check actual data date range before filtering and create backup\n",
        "    merged_backup = merged.copy()  # Backup in case filtering removes all data\n",
        "    original_count_before_filter = len(merged)\n",
        "    data_min_date = merged['date'].min() if len(merged) > 0 else None\n",
        "    data_max_date = merged['date'].max() if len(merged) > 0 else None\n",
        "    original_date_range = f\"{data_min_date} to {data_max_date}\" if data_min_date is not None else \"no data\"\n",
        "    \n",
        "    # Check if specified date range overlaps with actual data\n",
        "    use_actual_date_range = False\n",
        "    if start_year is not None and data_max_date is not None:\n",
        "        specified_start = pd.Timestamp(year=start_year, month=1, day=1)\n",
        "        if specified_start > data_max_date:\n",
        "            print(f\"  [WARNING] Specified START_YEAR={start_year} is after data end date ({data_max_date.year})\")\n",
        "            print(f\"  [WARNING] Data date range: {original_date_range}\")\n",
        "            print(f\"  [INFO] Using actual data date range instead of specified START_YEAR/END_YEAR\")\n",
        "            use_actual_date_range = True\n",
        "    if end_year is not None and data_min_date is not None:\n",
        "        specified_end = pd.Timestamp(year=end_year, month=12, day=31)\n",
        "        if specified_end < data_min_date:\n",
        "            print(f\"  [WARNING] Specified END_YEAR={end_year} is before data start date ({data_min_date.year})\")\n",
        "            print(f\"  [WARNING] Data date range: {original_date_range}\")\n",
        "            print(f\"  [INFO] Using actual data date range instead of specified START_YEAR/END_YEAR\")\n",
        "            use_actual_date_range = True\n",
        "    \n",
        "    # Filter data to only include dates from start_year onwards and up to end_year\n",
        "    # But skip filtering if date range doesn't overlap with data\n",
        "    if not use_actual_date_range:\n",
        "        if start_year is not None:\n",
        "            start_date = pd.Timestamp(year=start_year, month=1, day=1)\n",
        "            merged = merged[merged['date'] >= start_date].copy()\n",
        "            removed_count = original_count_before_filter - len(merged)\n",
        "            if removed_count > 0:\n",
        "                print(f\"  [INFO] Filtered data to start from {start_year}: removed {removed_count:,} earlier records\")\n",
        "                if len(merged) == 0:\n",
        "                    print(f\"  [WARNING] All data was filtered out by START_YEAR={start_year}!\")\n",
        "                    print(f\"  [WARNING] Restoring data and using actual data date range instead\")\n",
        "                    merged = merged_backup.copy()  # Restore from backup\n",
        "                    use_actual_date_range = True\n",
        "        \n",
        "        if end_year is not None and len(merged) > 0:\n",
        "            end_date = pd.Timestamp(year=end_year, month=12, day=31)\n",
        "            count_before_end_filter = len(merged)\n",
        "            merged = merged[merged['date'] <= end_date].copy()\n",
        "            removed_count = count_before_end_filter - len(merged)\n",
        "            if removed_count > 0:\n",
        "                print(f\"  [INFO] Filtered data to end at {end_year}: removed {removed_count:,} later records\")\n",
        "                if len(merged) == 0:\n",
        "                    print(f\"  [WARNING] All data was filtered out by END_YEAR={end_year}!\")\n",
        "                    print(f\"  [WARNING] Restoring data and using actual data date range instead\")\n",
        "                    merged = merged_backup.copy()  # Restore from backup\n",
        "                    use_actual_date_range = True\n",
        "    \n",
        "    # Diagnostic: Check radn values after filtering\n",
        "    if 'radn' in merged.columns and len(merged) > 0:\n",
        "        radn_nonzero_after_filter = (merged['radn'] != 0).sum()\n",
        "        radn_total_after_filter = merged['radn'].notna().sum()\n",
        "        if radn_total_after_filter > 0:\n",
        "            print(f\"  [INFO] After filtering: {radn_nonzero_after_filter:,} non-zero radn values out of {radn_total_after_filter:,} total\")\n",
        "        else:\n",
        "            print(f\"  [WARNING] No radn data remaining after filtering!\")\n",
        "    \n",
        "    # CRITICAL: Create a complete date range to ensure all days are included\n",
        "    # Use actual data range if filtering removed all data or date range doesn't overlap\n",
        "    if use_actual_date_range:\n",
        "        # Update dates from restored/current merged data\n",
        "        if len(merged) > 0:\n",
        "            data_min_date = merged['date'].min()\n",
        "            data_max_date = merged['date'].max()\n",
        "        if data_min_date is not None and data_max_date is not None:\n",
        "            # Use actual data date range\n",
        "            min_date = pd.Timestamp(year=data_min_date.year, month=1, day=1)\n",
        "            max_date = pd.Timestamp(year=data_max_date.year, month=12, day=31)\n",
        "            print(f\"  [INFO] Using actual data date range: {min_date.year} to {max_date.year}\")\n",
        "        else:\n",
        "            # Fallback\n",
        "            min_date = pd.Timestamp(year=1985, month=1, day=1)\n",
        "            max_date = pd.Timestamp(year=2014, month=12, day=31)\n",
        "    elif start_year is not None and len(merged) > 0:\n",
        "        min_date = pd.Timestamp(year=start_year, month=1, day=1)\n",
        "        if end_year is not None:\n",
        "            max_date = pd.Timestamp(year=end_year, month=12, day=31)\n",
        "        else:\n",
        "            max_date = merged['date'].max() if len(merged) > 0 else pd.Timestamp(year=2014, month=12, day=31)\n",
        "            max_date = pd.Timestamp(year=max_date.year, month=12, day=31)\n",
        "    else:\n",
        "        # Use data range\n",
        "        if len(merged) > 0:\n",
        "            min_date = merged['date'].min()\n",
        "            min_date = pd.Timestamp(year=min_date.year, month=1, day=1)\n",
        "            max_date = merged['date'].max()\n",
        "            max_date = pd.Timestamp(year=max_date.year, month=12, day=31)\n",
        "        else:\n",
        "            # Fallback if no data\n",
        "            min_date = pd.Timestamp(year=1985, month=1, day=1)\n",
        "            max_date = pd.Timestamp(year=2014, month=12, day=31)\n",
        "    \n",
        "    original_count = len(merged)\n",
        "    \n",
        "    # Create complete date range (includes all days, including day 366 for leap years)\n",
        "    complete_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "    # Ensure the date range index has a name so reset_index() works correctly\n",
        "    complete_date_range.name = 'date'\n",
        "    \n",
        "    # Set date as index for reindexing\n",
        "    merged = merged.set_index('date')\n",
        "    \n",
        "    # Reindex to include all days in the complete range\n",
        "    merged = merged.reindex(complete_date_range)\n",
        "    # Ensure index name is preserved after reindex (critical for reset_index to work)\n",
        "    merged.index.name = 'date'\n",
        "    # Verify the name was set correctly\n",
        "    if merged.index.name != 'date':\n",
        "        # Force set it again\n",
        "        merged.index = merged.index.rename('date')\n",
        "    \n",
        "    # Report if days were added\n",
        "    new_count = len(merged)\n",
        "    if new_count > original_count:\n",
        "        print(f\"  [INFO] Added {new_count - original_count} missing days to ensure complete date range\")\n",
        "        # Check if radn is all NaN after reindexing (indicates no rsds data for the date range)\n",
        "        if 'radn' in merged.columns:\n",
        "            radn_nan_count = merged['radn'].isna().sum()\n",
        "            if radn_nan_count == new_count - original_count:\n",
        "                print(f\"  [WARNING] All {radn_nan_count} added days have NaN radn values - no rsds data available for this date range!\")\n",
        "                print(f\"  [WARNING] This will result in radn=0.0 after filling. Check if rsds data exists for the specified date range.\")\n",
        "    \n",
        "    # Fill missing values for numeric columns using forward fill then backward fill\n",
        "    # But detect and warn about constant values (indicates missing/corrupted data)\n",
        "    numeric_cols = ['maxt', 'mint', 'rain', 'radn']\n",
        "    for col in numeric_cols:\n",
        "        if col in merged.columns:\n",
        "            # Check for constant values AFTER reindex but BEFORE filling\n",
        "            # This helps identify years that were completely missing and got filled with constant values\n",
        "            merged['year_temp'] = merged.index.year\n",
        "            for year in merged['year_temp'].unique():\n",
        "                year_mask = merged['year_temp'] == year\n",
        "                year_data = merged.loc[year_mask, col]\n",
        "                # Check if all non-NaN values are the same (constant value)\n",
        "                non_null_data = year_data.dropna()\n",
        "                if len(non_null_data) > 10:  # Only warn if we have enough data points\n",
        "                    if non_null_data.nunique() == 1:\n",
        "                        constant_value = non_null_data.iloc[0]\n",
        "                        # Count how many days have this value\n",
        "                        num_constant_days = (year_data == constant_value).sum()\n",
        "                        if num_constant_days >= 300:  # Most of the year is constant\n",
        "                            print(f\"  [WARNING] Year {year} has constant {col} value ({constant_value:.2f}) for {num_constant_days} days - likely missing/corrupted source data\")\n",
        "            merged = merged.drop(columns=['year_temp'])\n",
        "            \n",
        "            # Fill missing values (but this won't help if the source data itself is constant)\n",
        "            merged[col] = merged[col].ffill().bfill()\n",
        "            merged[col] = merged[col].fillna(0.0)\n",
        "    \n",
        "    # Handle VP separately - copy from day 365 to day 366 for leap years\n",
        "    if 'vp' in merged.columns:\n",
        "        # Reset index to get date back as a column\n",
        "        original_index_name = merged.index.name  # Capture BEFORE reset_index\n",
        "        merged_temp = merged.reset_index()\n",
        "        \n",
        "        # Ensure 'date' column exists - handle all possible cases\n",
        "        if 'date' not in merged_temp.columns:\n",
        "            # Check if 'index' column exists (created when index had no name)\n",
        "            if 'index' in merged_temp.columns:\n",
        "                merged_temp = merged_temp.rename(columns={'index': 'date'})\n",
        "            # Check if original index name exists as a column\n",
        "            elif original_index_name and original_index_name in merged_temp.columns:\n",
        "                merged_temp = merged_temp.rename(columns={original_index_name: 'date'})\n",
        "            else:\n",
        "                # Find the first datetime column (should be the index that was reset)\n",
        "                for col in merged_temp.columns:\n",
        "                    if pd.api.types.is_datetime64_any_dtype(merged_temp[col]):\n",
        "                        merged_temp = merged_temp.rename(columns={col: 'date'})\n",
        "                        break\n",
        "        \n",
        "        # Final safety check - if still no 'date' column, find the column created by reset_index\n",
        "        if 'date' not in merged_temp.columns:\n",
        "            # Find any datetime column (should be the one created by reset_index)\n",
        "            datetime_cols = [col for col in merged_temp.columns if pd.api.types.is_datetime64_any_dtype(merged_temp[col])]\n",
        "            if datetime_cols:\n",
        "                # Use the first datetime column (should be the index that was reset)\n",
        "                merged_temp = merged_temp.rename(columns={datetime_cols[0]: 'date'})\n",
        "            else:\n",
        "                # Absolute last resort: create from the original merged index\n",
        "                if len(merged.index) == len(merged_temp):\n",
        "                    merged_temp['date'] = pd.to_datetime(merged.index.values)\n",
        "                else:\n",
        "                    raise ValueError(f\"Cannot create 'date' column. Original index name: {original_index_name}, \"\n",
        "                                   f\"merged_temp columns: {list(merged_temp.columns)}, \"\n",
        "                                   f\"merged index type: {type(merged.index)}, \"\n",
        "                                   f\"Lengths - merged: {len(merged)}, merged_temp: {len(merged_temp)}\")\n",
        "        \n",
        "        # Verify 'date' column exists before using it\n",
        "        if 'date' not in merged_temp.columns:\n",
        "            raise ValueError(f\"Could not create 'date' column in merged_temp. Available columns: {list(merged_temp.columns)}\")\n",
        "        \n",
        "        merged_temp['year'] = merged_temp['date'].dt.year\n",
        "        merged_temp['day'] = merged_temp['date'].dt.dayofyear\n",
        "        \n",
        "        # Find all day 366 rows (leap year days)\n",
        "        day366_mask = merged_temp['day'] == 366\n",
        "        day366_rows = merged_temp[day366_mask].copy()\n",
        "        \n",
        "        if len(day366_rows) > 0:\n",
        "            # For each day 366, copy VP from day 365 of the same year\n",
        "            for idx in day366_rows.index:\n",
        "                year = merged_temp.loc[idx, 'year']\n",
        "                # Find day 365 of the same year\n",
        "                day365_mask = (merged_temp['year'] == year) & (merged_temp['day'] == 365)\n",
        "                day365_rows = merged_temp[day365_mask]\n",
        "                \n",
        "                if len(day365_rows) > 0:\n",
        "                    # Copy VP value from day 365 to day 366\n",
        "                    vp_day365 = merged_temp.loc[day365_rows.index[0], 'vp']\n",
        "                    vp_day366 = merged_temp.loc[idx, 'vp']\n",
        "                    if pd.isna(vp_day366) or vp_day366 == '':\n",
        "                        merged_temp.loc[idx, 'vp'] = vp_day365\n",
        "                        print(f\"  [INFO] Copied VP value from day 365 to day 366 for year {year}: {vp_day365:.2f} hPa\")\n",
        "        \n",
        "        # Set date back as index\n",
        "        merged = merged_temp.set_index('date')\n",
        "        merged = merged.drop(columns=['year', 'day'])\n",
        "    \n",
        "    # Fill missing VP and Evap values\n",
        "    # IMPORTANT: Only forward fill to avoid propagating values backward to earlier dates\n",
        "    # This prevents constant values in the first years when VP/Evap data starts later than base variables\n",
        "    # If VP/Evap data is missing at the start, leave it as NaN (don't backward fill)\n",
        "    if 'vp' in merged.columns:\n",
        "        # Forward fill only - fills gaps within the data range but doesn't fill backward\n",
        "        merged['vp'] = merged['vp'].ffill()\n",
        "    if 'evap' in merged.columns:\n",
        "        # Forward fill only - fills gaps within the data range but doesn't fill backward\n",
        "        merged['evap'] = merged['evap'].ffill()\n",
        "    \n",
        "    # Reset index to get date back as a column\n",
        "    original_index_name = merged.index.name  # Capture BEFORE reset_index\n",
        "    merged = merged.reset_index()\n",
        "    \n",
        "    # Ensure 'date' column exists - handle all possible cases\n",
        "    if 'date' not in merged.columns:\n",
        "        # Check if 'index' column exists (created when index had no name)\n",
        "        if 'index' in merged.columns:\n",
        "            merged = merged.rename(columns={'index': 'date'})\n",
        "        # Check if original index name exists as a column\n",
        "        elif original_index_name and original_index_name in merged.columns:\n",
        "            merged = merged.rename(columns={original_index_name: 'date'})\n",
        "        else:\n",
        "            # Find the first datetime column (should be the index that was reset)\n",
        "            for col in merged.columns:\n",
        "                if pd.api.types.is_datetime64_any_dtype(merged[col]):\n",
        "                    merged = merged.rename(columns={col: 'date'})\n",
        "                    break\n",
        "    \n",
        "    # Final safety check - if still no 'date' column, find datetime column or create from index\n",
        "    if 'date' not in merged.columns:\n",
        "        # Find any datetime column\n",
        "        datetime_cols = [col for col in merged.columns if pd.api.types.is_datetime64_any_dtype(merged[col])]\n",
        "        if datetime_cols:\n",
        "            merged = merged.rename(columns={datetime_cols[0]: 'date'})\n",
        "        else:\n",
        "            # Last resort: use the index itself (should be datetime after reset_index)\n",
        "            if isinstance(merged.index, pd.DatetimeIndex):\n",
        "                merged['date'] = merged.index\n",
        "            else:\n",
        "                # Convert index to datetime if possible\n",
        "                merged['date'] = pd.to_datetime(merged.index, errors='coerce')\n",
        "    \n",
        "    # Calculate tav and amp (use only non-NaN values for calculation)\n",
        "    merged_temp = merged[['date', 'maxt', 'mint']].copy()\n",
        "    merged_temp = merged_temp.set_index('date')\n",
        "    merged_temp.index = pd.to_datetime(merged_temp.index)\n",
        "    tav, amp = calculate_tav_amp(merged_temp)\n",
        "    \n",
        "    # Verify 'date' column exists before using it\n",
        "    if 'date' not in merged.columns:\n",
        "        raise ValueError(f\"Could not create 'date' column in merged. Available columns: {list(merged.columns)}\")\n",
        "    \n",
        "    # Create year and day columns\n",
        "    merged['year'] = merged['date'].dt.year\n",
        "    merged['day'] = merged['date'].dt.dayofyear\n",
        "    \n",
        "    # Add code column (hardcoded to '222222')\n",
        "    merged['code'] = '222222'\n",
        "    \n",
        "    # Create met_data\n",
        "    met_data = merged[['year', 'day', 'radn', 'maxt', 'mint', 'rain', 'evap', 'vp', 'code']].copy()\n",
        "    \n",
        "    # #region agent log\n",
        "    with open(r\"c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log\", \"a\") as f:\n",
        "        import json\n",
        "        log_entry = {\"location\":\"create_complete_met_file:904\",\"message\":\"met_data created\",\"data\":{\"columns\":list(met_data.columns),\"index_name\":str(met_data.index.name),\"has_date_col\":\"date\" in met_data.columns},\"timestamp\":int(time.time()*1000),\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\"}\n",
        "        f.write(json.dumps(log_entry) + \"\\n\")\n",
        "    # #endregion agent log\n",
        "    \n",
        "    # Ensure numeric columns are properly formatted\n",
        "    for col in ['radn', 'maxt', 'mint', 'rain', 'evap', 'vp']:\n",
        "        if col in met_data.columns:\n",
        "            met_data[col] = pd.to_numeric(met_data[col], errors='coerce')\n",
        "    \n",
        "    # Prepare header\n",
        "    current_date = datetime.now().strftime('%Y%m%d')\n",
        "    model_scenario = f\"{model.replace(' ', '_')}_{scenario}\" if model and scenario else \"CMIP6\"\n",
        "    \n",
        "    header = f\"\"\"[weather.met.weather]\n",
        "!Your Ref:  \"\n",
        "latitude = {latitude:.2f}  (DECIMAL DEGREES)\n",
        "longitude =  {longitude:.2f}  (DECIMAL DEGREES)\n",
        "tav = {tav:.2f} (oC) ! Annual average ambient temperature.\n",
        "amp = {amp:.2f} (oC) ! Annual amplitude in mean monthly temperature.\n",
        "!Data Extracted from CMIP6 {model} {scenario} dataset on {current_date} for APSIM\n",
        "!As evaporation is read at 9am, it has been shifted to day before\n",
        "!ie The evaporation measured on 20 April is in row for 19 April\n",
        "!The 6 digit code indicates the source of the 6 data columns\n",
        "!0 actual observation, 1 actual observation composite station\n",
        "!2 interpolated from daily observations\n",
        "!3 interpolated from daily observations using anomaly interpolation method for CLIMARC data\n",
        "!6 synthetic pan\n",
        "!7 interpolated long term averages\n",
        "!more detailed two digit codes are available in SILO's 'Standard' format files\n",
        "!\n",
        "!For further information see the documentation on the datadrill\n",
        "!  http://www.longpaddock.qld.gov.au/silo\n",
        "!\n",
        "year  day radn  maxt   mint  rain  evap    vp   code\n",
        " ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\n",
        "\"\"\"\n",
        "    \n",
        "    # Create output filename with coordinate-based naming\n",
        "    # Format lat_str to use 'neg' prefix instead of '-' for filenames\n",
        "    lat_str = f\"{latitude:.2f}\".replace('-', 'neg')\n",
        "    lon_str = f\"{longitude:.2f}\"\n",
        "    output_filename = f\"{model_scenario}_{lat_str}_{lon_str}.met\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    \n",
        "    # Write MET file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(header)\n",
        "        # Write data rows\n",
        "        for _, row in met_data.iterrows():\n",
        "            # Format the row with proper spacing\n",
        "            radn_val = row['radn'] if pd.notna(row['radn']) else 0.0\n",
        "            evap_val = row['evap'] if pd.notna(row['evap']) else 0.0\n",
        "            vp_val = row['vp'] if pd.notna(row['vp']) else 0.0\n",
        "            \n",
        "            # Format numbers with proper spacing\n",
        "            radn_str = f\"{float(radn_val):6.1f}\"\n",
        "            evap_str = f\"{float(evap_val):6.1f}\"\n",
        "            vp_str = f\"{float(vp_val):6.1f}\"\n",
        "            \n",
        "            # Code is hardcoded to '222222'\n",
        "            code_str = \"222222\"\n",
        "            \n",
        "            # Handle NaN values for maxt, mint, rain - use 0.0 as default\n",
        "            maxt_val = row['maxt'] if pd.notna(row['maxt']) else 0.0\n",
        "            mint_val = row['mint'] if pd.notna(row['mint']) else 0.0\n",
        "            rain_val = row['rain'] if pd.notna(row['rain']) else 0.0\n",
        "            \n",
        "            # Format with proper column widths\n",
        "            line = f\"{int(row['year']):4d} {int(row['day']):4d} {radn_str} {maxt_val:6.1f} {mint_val:6.1f} {rain_val:6.1f} {evap_str} {vp_str} {code_str}\\n\"\n",
        "            f.write(line)\n",
        "    \n",
        "    # Also create CSV version (using same lat_str format with 'neg')\n",
        "    csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}.csv\"\n",
        "    csv_path = os.path.join(output_dir, csv_filename)\n",
        "    met_data.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.1f', na_rep='')\n",
        "    \n",
        "    num_rows = len(met_data)\n",
        "    \n",
        "    print(f\"  [OK] Created MET file: {output_filename}\")\n",
        "    print(f\"  [OK] Created CSV file: {csv_filename}\")\n",
        "    print(f\"  [OK] Total rows: {num_rows:,}\")\n",
        "    \n",
        "    # #region agent log\n",
        "    with open(r\"c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log\", \"a\") as f:\n",
        "        import json\n",
        "        log_entry = {\"location\":\"create_complete_met_file:983\",\"message\":\"Before date range access\",\"data\":{\"columns\":list(met_data.columns),\"index_name\":str(met_data.index.name),\"has_date_col\":\"date\" in met_data.columns,\"index_type\":str(type(met_data.index))},\"timestamp\":int(time.time()*1000),\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\"}\n",
        "        f.write(json.dumps(log_entry) + \"\\n\")\n",
        "    # #endregion agent log\n",
        "    \n",
        "    # Use index for date range since 'date' is the index, not a column\n",
        "    # met_data inherits the index from merged, which should be a DatetimeIndex\n",
        "    try:\n",
        "        if isinstance(met_data.index, pd.DatetimeIndex):\n",
        "            print(f\"  [OK] Date range: {met_data.index.min()} to {met_data.index.max()}\")\n",
        "        elif 'date' in met_data.columns:\n",
        "            print(f\"  [OK] Date range: {met_data['date'].min()} to {met_data['date'].max()}\")\n",
        "        else:\n",
        "            # Try to get date range from merged if available\n",
        "            print(f\"  [INFO] Date range: {num_rows:,} rows (date info not directly available)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [INFO] Could not determine date range: {e}\")\n",
        "    \n",
        "    return tav, amp, num_rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Main Processing - Extract Data and Create ACCESS MET File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Processing Coordinate: (-31.750000, 117.600000)\n",
            "Model: ACCESS CM2, Scenario: SSP585\n",
            "======================================================================\n",
            "\n",
            "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP585\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmax\n",
            "======================================================================\n",
            "  [DEBUG] Checking cache: ACCESS_CM2_SSP585_-31.75_117.60_tasmax.csv\n",
            "  [DEBUG] Full cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\\ACCESS_CM2_SSP585_-31.75_117.60_tasmax.csv\n",
            "  [INFO] Cache loaded: 10,957 records, 10,957 non-zero values\n",
            "  [OK] Loaded from cache: 10,957 records for tasmax\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmin\n",
            "======================================================================\n",
            "  [DEBUG] Checking cache: ACCESS_CM2_SSP585_-31.75_117.60_tasmin.csv\n",
            "  [DEBUG] Full cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\\ACCESS_CM2_SSP585_-31.75_117.60_tasmin.csv\n",
            "  [INFO] Cache loaded: 10,957 records, 10,957 non-zero values\n",
            "  [OK] Loaded from cache: 10,957 records for tasmin\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: pr\n",
            "======================================================================\n",
            "  [DEBUG] Checking cache: ACCESS_CM2_SSP585_-31.75_117.60_pr.csv\n",
            "  [DEBUG] Full cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\\ACCESS_CM2_SSP585_-31.75_117.60_pr.csv\n",
            "  [INFO] Cache loaded: 10,227 records, 3,974 non-zero values\n",
            "  [OK] Loaded from cache: 10,227 records for pr\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: rsds\n",
            "======================================================================\n",
            "  [DEBUG] Checking cache: ACCESS_CM2_SSP585_-31.75_117.60_rsds.csv\n",
            "  [DEBUG] Full cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\\ACCESS_CM2_SSP585_-31.75_117.60_rsds.csv\n",
            "  [INFO] Cache loaded: 10,957 records, 10,957 non-zero values\n",
            "  [OK] Loaded from cache: 10,957 records for rsds\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: hurs\n",
            "======================================================================\n",
            "  [DEBUG] Checking cache: ACCESS_CM2_SSP585_-31.75_117.60_hurs.csv\n",
            "  [DEBUG] Full cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\\ACCESS_CM2_SSP585_-31.75_117.60_hurs.csv\n",
            "  [INFO] Cache loaded: 10,957 records, 10,957 non-zero values\n",
            "  [OK] Loaded from cache: 10,957 records for hurs\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: hurs\n",
            "======================================================================\n",
            "  [DEBUG] Checking cache: ACCESS_CM2_SSP585_-31.75_117.60_hurs.csv\n",
            "  [DEBUG] Full cache path: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\\ACCESS_CM2_SSP585_-31.75_117.60_hurs.csv\n",
            "  [INFO] Cache loaded: 10,957 records, 10,957 non-zero values\n",
            "  [OK] Loaded from cache: 10,957 records for hurs\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Loading Vapor Pressure (VP) from CSV...\n",
            "======================================================================\n",
            "  [OK] Loaded VP data from: ACCESS_CM2_SSP585_neg31.75_117.60_vp.csv\n",
            "  [INFO] VP records: 10,957 days\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [INFO] VP range: 4.48 to 35.82 hPa\n",
            "  [SUCCESS] VP data loaded successfully!\n",
            "\n",
            "======================================================================\n",
            "Loading Evaporation (Evap) from CSV...\n",
            "======================================================================\n",
            "  [OK] Loaded Evap data from: ACCESS_CM2_SSP585_neg31.75_117.60_eto.csv\n",
            "  [INFO] Evap records: 10,957 days\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [INFO] Evap range: 0.32 to 15.38 mm/day\n",
            "  [SUCCESS] Evap data loaded successfully!\n",
            "\n",
            "======================================================================\n",
            "Creating ACCESS MET File...\n",
            "======================================================================\n",
            "  [DEBUG] tasmax: 10,957 records, maxt range: 11.44 to 51.09\n",
            "  [DEBUG] After tasmin merge: 10,957 records, mint range: -2.24 to 30.47\n",
            "  [DEBUG] After pr merge: 10,957 records, rain range: 0.00 to 107.27\n",
            "  [DEBUG] rsds before merge: 10,957 records, value range: 11.07 to 398.97\n",
            "  [DEBUG] After rsds merge: 10,957 records, radn range: 0.96 to 34.47\n",
            "  [DEBUG] Non-null counts - maxt: 10957, mint: 10957, rain: 10227, radn: 10957\n",
            "  [INFO] After merge: 10,957 non-zero radn values out of 10,957 total (100.0%)\n",
            "  [INFO] Before filtering: 10,957 non-zero radn values out of 10,957 total\n",
            "  [INFO] Radn range before filtering: 0.96 to 34.47 MJ/m²\n",
            "  [WARNING] Specified END_YEAR=2014 is before data start date (2035)\n",
            "  [WARNING] Data date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [INFO] Using actual data date range instead of specified START_YEAR/END_YEAR\n",
            "  [INFO] After filtering: 10,957 non-zero radn values out of 10,957 total\n",
            "  [INFO] Using actual data date range: 2035 to 2064\n",
            "  [INFO] Added 1 missing days to ensure complete date range\n",
            "  [WARNING] All 1 added days have NaN radn values - no rsds data available for this date range!\n",
            "  [WARNING] This will result in radn=0.0 after filling. Check if rsds data exists for the specified date range.\n",
            "  [INFO] Copied VP value from day 365 to day 366 for year 2064: 18.72 hPa\n",
            "  [OK] Created MET file: ACCESS_CM2_SSP585_neg31.75_117.60.met\n",
            "  [OK] Created CSV file: ACCESS_CM2_SSP585_neg31.75_117.60.csv\n",
            "  [OK] Total rows: 10,958\n",
            "  [INFO] Date range: 10,958 rows (date info not directly available)\n",
            "\n",
            "======================================================================\n",
            "[SUCCESS] ACCESS MET FILE CREATED!\n",
            "======================================================================\n",
            "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka Climate Files_2901\\-31.75_117.60_Climate Files\n",
            "  Total Rows: 10,958\n",
            "  TAV: 19.93°C\n",
            "  AMP: 7.69°C\n"
          ]
        }
      ],
      "source": [
        "# Construct data directory path\n",
        "data_dir = os.path.join(CMIP6_BASE_DIR, f\"{MODEL} {SCENARIO}\")\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    raise ValueError(f\"Data directory not found: {data_dir}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Processing Coordinate: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
        "print(f\"Model: {MODEL}, Scenario: {SCENARIO}\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nData directory: {data_dir}\\n\")\n",
        "\n",
        "# Extract data for all variables\n",
        "extracted_data = {}\n",
        "\n",
        "# Extract base variables (required for MET file)\n",
        "all_variables = BASE_VARIABLES + VP_VARIABLES + EVAP_VARIABLES\n",
        "\n",
        "for variable in all_variables:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing variable: {variable}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Check for cached data first\n",
        "    cache_path = get_cached_variable_path(OUTPUT_DIR, model_scenario, lat_str, lon_str, variable)\n",
        "    print(f\"  [DEBUG] Checking cache: {os.path.basename(cache_path)}\")\n",
        "    print(f\"  [DEBUG] Full cache path: {cache_path}\")\n",
        "    df = load_cached_variable(cache_path)\n",
        "    \n",
        "    if df is not None:\n",
        "        # Use cached data\n",
        "        extracted_data[variable] = df\n",
        "        print(f\"  [OK] Loaded from cache: {len(df):,} records for {variable}\")\n",
        "        print(f\"  [INFO] Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "    else:\n",
        "        # Extract from NetCDF files\n",
        "        print(f\"  [INFO] Cache not found, extracting from NetCDF files...\")\n",
        "        df = extract_daily_data_from_netcdf(\n",
        "            data_dir, \n",
        "            variable, \n",
        "            LATITUDE, \n",
        "            LONGITUDE, \n",
        "            tolerance=COORD_TOLERANCE\n",
        "        )\n",
        "        \n",
        "        if df is not None and len(df) > 0:\n",
        "            extracted_data[variable] = df\n",
        "            print(f\"  [OK] Extracted {len(df):,} records for {variable}\")\n",
        "            # Save to cache for future use\n",
        "            save_cached_variable(df, cache_path)\n",
        "        else:\n",
        "            if variable in BASE_VARIABLES:\n",
        "                raise ValueError(f\"Failed to extract required variable: {variable}\")\n",
        "            else:\n",
        "                print(f\"  [WARNING] Failed to extract optional variable: {variable}\")\n",
        "\n",
        "# Check if all required base variables are available\n",
        "missing_vars = [v for v in BASE_VARIABLES if v not in extracted_data]\n",
        "if missing_vars:\n",
        "    raise ValueError(f\"Missing required variables: {missing_vars}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Loading Vapor Pressure (VP) from CSV...\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Load VP from CSV file generated by CMPI6_VP_Calculation.ipynb\n",
        "vp_df = load_vp_from_csv(OUTPUT_DIR, model_scenario, lat_str, lon_str)\n",
        "\n",
        "if vp_df is None:\n",
        "    print(f\"  [WARNING] VP data not available - VP will be set to 0\")\n",
        "    # Create empty VP dataframe with same dates as tasmax\n",
        "    vp_df = extracted_data['tasmax'].copy()\n",
        "    vp_df['value'] = 0.0\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Loading Evaporation (Evap) from CSV...\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Load Evap from CSV file generated by CMPI6_Evap_Calculation.ipynb\n",
        "evap_df = load_evap_from_csv(OUTPUT_DIR, model_scenario, lat_str, lon_str)\n",
        "\n",
        "if evap_df is None:\n",
        "    print(f\"  [WARNING] Evap data not available - Evap will be set to 0\")\n",
        "    # Create empty evap dataframe with same dates as tasmax (set to 0)\n",
        "    evap_df = extracted_data['tasmax'].copy()\n",
        "    evap_df['value'] = 0.0\n",
        "    print(f\"  [INFO] Created empty evap dataframe with {len(evap_df):,} days (all values = 0.0)\")\n",
        "    print(f\"  [INFO] Date range: {evap_df['date'].min()} to {evap_df['date'].max()}\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# TEMPORARILY COMMENTED OUT: Evap Calculation (uncomment when variables are available)\n",
        "# ============================================================================\n",
        "# The evap calculation is temporarily skipped - evap is loaded from CSV instead.\n",
        "# The evap CSV is generated by CMPI6_Evap_Calculation.ipynb which already incorporates\n",
        "# all required variables including sfcWind.\n",
        "# Note: hursmax and hursmin are not available - using hurs (mean relative humidity) instead.\n",
        "\n",
        "# print(f\"\\n{'='*70}\")\n",
        "# print(\"Calculating Evaporation (Evap)...\")\n",
        "# print(f\"{'='*70}\")\n",
        "# \n",
        "# # Calculate Evap if all required variables are available\n",
        "# # Note: sfcWind would need to be extracted separately if doing evap calculation here\n",
        "# evap_required = ['hurs']  # sfcWind already used in CMPI6_Evap_Calculation.ipynb\n",
        "# evap_available = all(v in extracted_data for v in evap_required)\n",
        "# \n",
        "# if evap_available:\n",
        "#     # This would require sfcWind to be extracted, but evap is loaded from CSV instead\n",
        "#     print(f\"  [INFO] Evap calculation skipped - loading from CSV instead\")\n",
        "# else:\n",
        "#     print(f\"  [WARNING] Evap variables not available - evap will be set to 0\")\n",
        "#     # Create empty evap dataframe with same dates as tasmax\n",
        "#     evap_df = extracted_data['tasmax'].copy()\n",
        "#     evap_df['value'] = 0.0\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Creating ACCESS MET File...\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "# Create ACCESS MET file\n",
        "tav, amp, num_rows = create_complete_met_file(\n",
        "    tasmax_df=extracted_data['tasmax'],\n",
        "    tasmin_df=extracted_data['tasmin'],\n",
        "    pr_df=extracted_data['pr'],\n",
        "    rsds_df=extracted_data['rsds'],\n",
        "    vp_df=vp_df,\n",
        "    evap_df=evap_df,\n",
        "    scenario=SCENARIO,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    latitude=LATITUDE,\n",
        "    longitude=LONGITUDE,\n",
        "    model=MODEL,\n",
        "    start_year=START_YEAR,\n",
        "    end_year=END_YEAR\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"[SUCCESS] ACCESS MET FILE CREATED!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Total Rows: {num_rows:,}\")\n",
        "print(f\"  TAV: {tav:.2f}°C\")\n",
        "print(f\"  AMP: {amp:.2f}°C\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
