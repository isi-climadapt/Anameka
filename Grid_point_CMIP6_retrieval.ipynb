{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Grid Point CMIP6 Retrieval and MET Conversion Notebook\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook combines CMIP6 NetCDF data extraction with APSIM MET format conversion. It processes climate variables from NC files organized in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}` folders and converts them to MET format for APSIM simulations.\n",
        "\n",
        "## File Structure\n",
        "\n",
        "- **Input**: NC files in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\` (e.g., `ACCESS CM2 SSP245`)\n",
        "- **User Input**: Latitude and longitude coordinates (decimal degrees)\n",
        "- **Output**: MET files and CSV files for the specified coordinate\n",
        "\n",
        "## Variables Processed\n",
        "\n",
        "The notebook processes 4 climate variables:\n",
        "\n",
        "- **tasmax**: Daily maximum temperature (°C) → **maxt** in MET format\n",
        "- **tasmin**: Daily minimum temperature (°C) → **mint** in MET format\n",
        "- **pr**: Daily precipitation (mm) → **rain** in MET format\n",
        "- **rsds**: Daily surface downwelling shortwave radiation (W/m²) → **radn** (MJ/m²) in MET format\n",
        "\n",
        "**Note**: vp (vapor pressure) and code fields are left blank in the MET file\n",
        "\n",
        "## MET Format Specifications\n",
        "\n",
        "The MET format is used by APSIM for weather data input. It includes:\n",
        "\n",
        "- **Required fields**: year, day, maxt (from tasmax), mint (from tasmin), rain (from pr), radn (from rsds)\n",
        "- **radn field**: Required, converted from rsds (W/m² to MJ/m²)\n",
        "- **Blank fields**: evap (evaporation), vp (vapor pressure), code (data quality code) - left blank\n",
        "- **Metadata**: latitude, longitude, tav (annual average temperature), amp (annual amplitude)\n",
        "\n",
        "## File Naming Conventions\n",
        "\n",
        "- **Input NC files**: `{Model} {Scenario}\\*{variable}*.nc` (e.g., `ACCESS CM2 SSP245\\tasmax*.nc`)\n",
        "- **Output MET files**: `{Model}_{Scenario}_{Lat}_{Lon}.met` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60.met`)\n",
        "- **Output CSV files**: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60_tasmax.csv`)\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "1. Set the configuration parameters (Model, Scenario, and coordinates) in Section 1\n",
        "2. The output directory is automatically created based on the coordinate (e.g., `C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.75_117.60`)\n",
        "3. Run all cells sequentially to extract data and create MET files\n",
        "4. Output files will be saved with coordinate-based naming\n",
        "\n",
        "## Coordinate Matching\n",
        "\n",
        "The notebook finds the nearest grid point to your specified coordinates within a tolerance (default: 0.01 degrees ≈ 1.1 km). If the nearest point is outside tolerance, a warning will be displayed.\n",
        "\n",
        "## Notes\n",
        "\n",
        "- **CMIP6 data structure**: Data is organized by Model and Scenario in separate folders\n",
        "- **Coordinate format**: Provide coordinates in decimal degrees (latitude: -90 to 90, longitude: -180 to 180)\n",
        "- **Variable extraction**: 4 variables (tasmax, tasmin, pr, rsds) are extracted from NC files and saved as individual CSVs\n",
        "- **MET conversion**: tasmax→maxt, tasmin→mint, pr→rain, rsds→radn\n",
        "- **Unit conversions**: rsds (W/m²) is converted to radn (MJ/m²) by multiplying by 0.0864\n",
        "- **Blank fields**: vp and code are left blank in MET format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.198916Z",
          "iopub.status.busy": "2025-12-21T10:28:42.198764Z",
          "iopub.status.idle": "2025-12-21T10:28:42.528559Z",
          "shell.execute_reply": "2025-12-21T10:28:42.528131Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.547981Z",
          "iopub.status.busy": "2025-12-21T10:28:42.547706Z",
          "iopub.status.idle": "2025-12-21T10:28:42.551487Z",
          "shell.execute_reply": "2025-12-21T10:28:42.550859Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  - CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
            "  - Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.75_117.60\n",
            "  - Model: ACCESS CM2\n",
            "  - Scenario: SSP245\n",
            "  - Latitude: -31.750000°\n",
            "  - Longitude: 117.600000°\n",
            "  - Coordinate: -31.75_117.60\n",
            "  - Coordinate Tolerance: 0.01 degrees\n",
            "  - Variables: tasmax, tasmin, pr, rsds, hurs\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
        "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
        "\n",
        "# Model and Scenario - UPDATE THESE AS NEEDED\n",
        "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
        "SCENARIO = \"SSP245\"   # e.g., \"SSP245\" or \"SSP585\"\n",
        "\n",
        "# USER INPUT: Provide your target coordinates here (moved from Section 2)\n",
        "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
        "LONGITUDE = 117.60  # Target longitude in decimal degrees (-180 to 180)\n",
        "\n",
        "# Validate coordinates\n",
        "if not (-90 <= LATITUDE <= 90):\n",
        "    raise ValueError(f\"Latitude must be between -90 and 90. Provided: {LATITUDE}\")\n",
        "if not (-180 <= LONGITUDE <= 180):\n",
        "    raise ValueError(f\"Longitude must be between -180 and 180. Provided: {LONGITUDE}\")\n",
        "\n",
        "# Create coordinate string for output directory\n",
        "COORDINATE = f\"{LATITUDE:.2f}_{LONGITUDE:.2f}\"\n",
        "OUTPUT_DIR = rf\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\{COORDINATE}\"  # Output directory for MET files\n",
        "\n",
        "# Variables to process (5 variables)\n",
        "# MET format mapping:\n",
        "# - tasmax → maxt (maximum temperature)\n",
        "# - tasmin → mint (minimum temperature)\n",
        "# - pr → rain (precipitation)\n",
        "# - rsds → radn (radiation, converted from W/m² to MJ/m²)\n",
        "# - hurs → vp (vapor pressure, calculated using SILO method)\n",
        "# Note: code is hardcoded to '222222'\n",
        "VARIABLES = ['tasmax', 'tasmin', 'pr', 'rsds', 'hurs']\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Configuration loaded:\")\n",
        "print(f\"  - CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
        "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"  - Model: {MODEL}\")\n",
        "print(f\"  - Scenario: {SCENARIO}\")\n",
        "print(f\"  - Latitude: {LATITUDE:.6f}°\")\n",
        "print(f\"  - Longitude: {LONGITUDE:.6f}°\")\n",
        "print(f\"  - Coordinate: {COORDINATE}\")\n",
        "print(f\"  - Coordinate Tolerance: {COORD_TOLERANCE} degrees\")\n",
        "print(f\"  - Variables: {', '.join(VARIABLES)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Coordinate Validation\n",
        "\n",
        "Coordinates are now defined in Section 1. This section validates and displays them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.553077Z",
          "iopub.status.busy": "2025-12-21T10:28:42.552944Z",
          "iopub.status.idle": "2025-12-21T10:28:42.556150Z",
          "shell.execute_reply": "2025-12-21T10:28:42.555562Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target Coordinate:\n",
            "  Latitude: -31.750000°\n",
            "  Longitude: 117.600000°\n",
            "  Coordinate string: -31.75_117.60\n",
            "  Model: ACCESS CM2\n",
            "  Scenario: SSP245\n",
            "  Tolerance: 0.01 degrees\n"
          ]
        }
      ],
      "source": [
        "# Display coordinate information (coordinates are defined in Section 1)\n",
        "print(f\"Target Coordinate:\")\n",
        "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
        "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
        "print(f\"  Coordinate string: {COORDINATE}\")\n",
        "print(f\"  Model: {MODEL}\")\n",
        "print(f\"  Scenario: {SCENARIO}\")\n",
        "print(f\"  Tolerance: {COORD_TOLERANCE} degrees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: NetCDF Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.558014Z",
          "iopub.status.busy": "2025-12-21T10:28:42.557889Z",
          "iopub.status.idle": "2025-12-21T10:28:42.569492Z",
          "shell.execute_reply": "2025-12-21T10:28:42.569072Z"
        }
      },
      "outputs": [],
      "source": [
        "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    netcdf_dir : str\n",
        "        Directory containing NetCDF files for the variable\n",
        "    variable : str\n",
        "        Variable name (tasmax, tasmin, pr, rsds)\n",
        "    target_lat : float\n",
        "        Target latitude\n",
        "    target_lon : float\n",
        "        Target longitude\n",
        "    tolerance : float\n",
        "        Coordinate matching tolerance in degrees\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with columns: date, value\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Find all NetCDF files in the directory\n",
        "    # Pattern 1: Files directly in the directory matching *{variable}*.nc\n",
        "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
        "    \n",
        "    # Pattern 2: Files in subdirectories named {variable}_* (e.g., pr_ACCESS CM2 SSP245)\n",
        "    if len(nc_files) == 0:\n",
        "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
        "        for var_subdir in var_subdirs:\n",
        "            if os.path.isdir(var_subdir):\n",
        "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
        "                    break\n",
        "    \n",
        "    # Pattern 2b: For rsds, also check for \"rad_\" folder (folder named \"rad_\" but files contain \"rsds\")\n",
        "    # Example: rad_ACCESS CM2 SSP245/ contains files named *rsds*.nc\n",
        "    if len(nc_files) == 0 and variable == 'rsds':\n",
        "        rad_subdirs = glob.glob(os.path.join(netcdf_dir, \"rad_*\"))\n",
        "        for rad_subdir in rad_subdirs:\n",
        "            if os.path.isdir(rad_subdir):\n",
        "                # Search for files containing \"rsds\" in the rad_ folder\n",
        "                found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*rsds*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
        "                    break\n",
        "                # Fallback: if no rsds files found, try all .nc files\n",
        "                if len(nc_files) == 0:\n",
        "                    found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*.nc\")))\n",
        "                    if found_files:\n",
        "                        nc_files.extend(found_files)\n",
        "                        print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
        "                        break\n",
        "    \n",
        "    # Pattern 3: Check subdirectory named exactly after the variable\n",
        "    if len(nc_files) == 0:\n",
        "        var_dir = os.path.join(netcdf_dir, variable)\n",
        "        if os.path.exists(var_dir) and os.path.isdir(var_dir):\n",
        "            nc_files = sorted(glob.glob(os.path.join(var_dir, \"*.nc\")))\n",
        "            if len(nc_files) > 0:\n",
        "                print(f\"  Found files in subdirectory: {variable}/\")\n",
        "    \n",
        "    if len(nc_files) == 0:\n",
        "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
        "        print(f\"  Searched patterns:\")\n",
        "        print(f\"    - {netcdf_dir}/*{variable}*.nc\")\n",
        "        print(f\"    - {netcdf_dir}/{variable}_*/*.nc\")\n",
        "        if variable == 'rsds':\n",
        "            print(f\"    - {netcdf_dir}/rad_*/*rsds*.nc\")\n",
        "            print(f\"    - {netcdf_dir}/rad_*/*.nc\")\n",
        "        print(f\"    - {netcdf_dir}/{variable}/*.nc\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
        "    \n",
        "    # Cache coordinate information from first file\n",
        "    lat_name = None\n",
        "    lon_name = None\n",
        "    time_name = None\n",
        "    lat_idx = None\n",
        "    lon_idx = None\n",
        "    actual_lat = None\n",
        "    actual_lon = None\n",
        "    var_name = None\n",
        "    \n",
        "    # List to store daily data\n",
        "    all_data = []\n",
        "    \n",
        "    # Process first file to get coordinate structure\n",
        "    if len(nc_files) > 0:\n",
        "        try:\n",
        "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
        "            \n",
        "            # Get variable name\n",
        "            for v in ds_sample.data_vars:\n",
        "                if variable in v.lower() or v.lower() in variable.lower():\n",
        "                    var_name = v\n",
        "                    break\n",
        "            \n",
        "            # For rsds, also check for \"rad\" variable name (some datasets use \"rad\" instead of \"rsds\")\n",
        "            if var_name is None and variable == 'rsds':\n",
        "                for v in ds_sample.data_vars:\n",
        "                    if 'rad' in v.lower() and 'rsds' not in v.lower():\n",
        "                        var_name = v\n",
        "                        break\n",
        "            \n",
        "            if var_name is None:\n",
        "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
        "                # For rsds, also try \"rad\" as variable name\n",
        "                if variable == 'rsds':\n",
        "                    possible_names.extend(['rad', 'RAD', 'rad_day'])\n",
        "                for name in possible_names:\n",
        "                    if name in ds_sample.data_vars:\n",
        "                        var_name = name\n",
        "                        break\n",
        "            \n",
        "            # Get coordinate names\n",
        "            for coord in ds_sample.coords:\n",
        "                coord_lower = coord.lower()\n",
        "                if 'lat' in coord_lower:\n",
        "                    lat_name = coord\n",
        "                elif 'lon' in coord_lower:\n",
        "                    lon_name = coord\n",
        "                elif 'time' in coord_lower:\n",
        "                    time_name = coord\n",
        "            \n",
        "            if lat_name and lon_name:\n",
        "                # Find nearest grid point (cache indices)\n",
        "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
        "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
        "                \n",
        "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
        "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
        "                \n",
        "                # Check if within tolerance\n",
        "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
        "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
        "                else:\n",
        "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
        "            \n",
        "            ds_sample.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not read sample file: {e}\")\n",
        "    \n",
        "    if var_name is None or lat_idx is None or lon_idx is None:\n",
        "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
        "        return None\n",
        "    \n",
        "    # Process all files with progress bar\n",
        "    print(f\"  Processing files...\")\n",
        "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
        "        try:\n",
        "            # Open NetCDF file with minimal decoding for speed\n",
        "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
        "            \n",
        "            # Extract data using cached indices\n",
        "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
        "            \n",
        "            # Convert to numpy array (load into memory)\n",
        "            values = data.values\n",
        "            if values.ndim > 1:\n",
        "                values = values.flatten()\n",
        "            \n",
        "            # Get time values - try multiple methods to ensure accuracy and handle leap years (366 days)\n",
        "            time_values = None\n",
        "            \n",
        "            # Method 1: Try to use time coordinate from NetCDF file (most reliable)\n",
        "            if time_name and time_name in ds.coords:\n",
        "                try:\n",
        "                    time_coord = ds[time_name]\n",
        "                    if len(time_coord) == len(values):\n",
        "                        # Try to decode times\n",
        "                        try:\n",
        "                            # Decode time coordinate\n",
        "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
        "                            time_values = pd.to_datetime(time_decoded.values)\n",
        "                            if len(time_values) == len(values):\n",
        "                                pass  # Success - using decoded time coordinate\n",
        "                        except:\n",
        "                            # If decoding fails, try manual conversion\n",
        "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
        "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
        "                                base_date = pd.to_datetime(base_date_str)\n",
        "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
        "                                if len(time_values) != len(values):\n",
        "                                    time_values = None\n",
        "                except Exception as e:\n",
        "                    pass  # Fall back to other methods\n",
        "            \n",
        "            # Method 2: Extract year from filename and create date range\n",
        "            # This method automatically handles leap years (366 days) correctly\n",
        "            if time_values is None:\n",
        "                year = None\n",
        "                filename = os.path.basename(nc_file)\n",
        "                all_years = re.findall(r'\\d{4}', filename)\n",
        "                for year_str in all_years:\n",
        "                    year_candidate = int(year_str)\n",
        "                    if 2000 <= year_candidate <= 2100:\n",
        "                        year = year_candidate\n",
        "                        break\n",
        "                \n",
        "                if year:\n",
        "                    # Create dates based on ACTUAL data length\n",
        "                    # pd.date_range with freq='D' automatically handles leap years\n",
        "                    # For leap years (e.g., 2024, 2028), it will include Feb 29 (366 days)\n",
        "                    # For non-leap years, it will have 365 days\n",
        "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
        "                else:\n",
        "                    # Fallback: use 2035 as default (start of typical CMIP6 data range)\n",
        "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
        "            \n",
        "            # Ensure we have the correct number of dates matching the data\n",
        "            # This handles edge cases where time coordinate might not match exactly\n",
        "            if len(time_values) != len(values):\n",
        "                if len(time_values) > len(values):\n",
        "                    time_values = time_values[:len(values)]\n",
        "                else:\n",
        "                    # Extend if needed (shouldn't happen normally, but handle it)\n",
        "                    additional_days = len(values) - len(time_values)\n",
        "                    last_date = time_values[-1]\n",
        "                    additional_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=additional_days, freq='D')\n",
        "                    time_values = pd.concat([pd.Series(time_values), pd.Series(additional_dates)]).values\n",
        "            \n",
        "            # Create DataFrame for this file\n",
        "            # Use actual data length to ensure all days are included (365 or 366 for leap years)\n",
        "            if len(values) > 0:\n",
        "                df_file = pd.DataFrame({\n",
        "                    'date': time_values[:len(values)],\n",
        "                    'value': values\n",
        "                })\n",
        "                all_data.append(df_file)\n",
        "            \n",
        "            ds.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(all_data) == 0:\n",
        "        print(f\"  ERROR: No data extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Combine all data\n",
        "    print(f\"  Combining data from {len(all_data)} files...\")\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Remove duplicate dates (keep first occurrence)\n",
        "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
        "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
        "    \n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: MET Conversion Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.570994Z",
          "iopub.status.busy": "2025-12-21T10:28:42.570844Z",
          "iopub.status.idle": "2025-12-21T10:28:42.573605Z",
          "shell.execute_reply": "2025-12-21T10:28:42.573193Z"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_tav_amp(df):\n",
        "    \"\"\"\n",
        "    Calculate annual average temperature (tav) and annual amplitude (amp).\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with 'date' as index and 'maxt' and 'mint' columns\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (tav, amp)\n",
        "        tav: Annual average ambient temperature\n",
        "        amp: Annual amplitude in mean monthly temperature\n",
        "    \"\"\"\n",
        "    # Calculate daily mean temperature\n",
        "    df = df.copy()\n",
        "    df['tmean'] = (df['maxt'] + df['mint']) / 2.0\n",
        "    \n",
        "    # Calculate monthly means\n",
        "    df['year'] = df.index.year\n",
        "    df['month'] = df.index.month\n",
        "    monthly_means = df.groupby(['year', 'month'])['tmean'].mean()\n",
        "    \n",
        "    # Calculate overall annual average (tav)\n",
        "    tav = df['tmean'].mean()\n",
        "    \n",
        "    # Calculate annual amplitude (amp)\n",
        "    # Average of all January means minus average of all July means, divided by 2\n",
        "    jan_means = monthly_means[monthly_means.index.get_level_values('month') == 1].mean()\n",
        "    jul_means = monthly_means[monthly_means.index.get_level_values('month') == 7].mean()\n",
        "    amp = (jan_means - jul_means) / 2.0\n",
        "    \n",
        "    return tav, amp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.575004Z",
          "iopub.status.busy": "2025-12-21T10:28:42.574863Z",
          "iopub.status.idle": "2025-12-21T10:28:42.578044Z",
          "shell.execute_reply": "2025-12-21T10:28:42.577751Z"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_saturation_vapor_pressure(temperature):\n",
        "    \"\"\"\n",
        "    Calculate saturation vapor pressure (kPa) at a given temperature using SILO method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    temperature : float or array\n",
        "        Temperature in °C\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    float or array\n",
        "        Saturation vapor pressure in kPa\n",
        "    \"\"\"\n",
        "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
        "    return 0.611 * np.exp(17.27 * temperature / (temperature + 237.3))\n",
        "\n",
        "\n",
        "def calculate_vapor_pressure(hurs_df, tasmax_df, tasmin_df):\n",
        "    \"\"\"\n",
        "    Calculate vapor pressure (hPa) from mean relative humidity and temperature using SILO method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    hurs_df : pd.DataFrame\n",
        "        DataFrame with date and value (mean relative humidity %) columns\n",
        "    tasmax_df : pd.DataFrame\n",
        "        DataFrame with date and value (maximum temperature °C) columns\n",
        "    tasmin_df : pd.DataFrame\n",
        "        DataFrame with date and value (minimum temperature °C) columns\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with date and value (vapor pressure hPa) columns\n",
        "    \"\"\"\n",
        "    # Merge temperature dataframes\n",
        "    temp_df = tasmax_df.merge(tasmin_df, on='date', suffixes=('_max', '_min'))\n",
        "    temp_df['tmean'] = (temp_df['value_max'] + temp_df['value_min']) / 2.0\n",
        "    \n",
        "    # Merge with mean humidity\n",
        "    merged = hurs_df.merge(temp_df[['date', 'tmean']], on='date')\n",
        "    \n",
        "    # Calculate saturation vapor pressure at mean temperature (in kPa)\n",
        "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
        "    merged['es_kpa'] = calculate_saturation_vapor_pressure(merged['tmean'])\n",
        "    \n",
        "    # Calculate actual vapor pressure using mean relative humidity (in kPa)\n",
        "    # e_a = (hurs/100) × e_s(T_mean)\n",
        "    merged['ea_kpa'] = (merged['value'] / 100.0) * merged['es_kpa']\n",
        "    \n",
        "    # Convert to SILO VP units (hPa): VP(hPa) = 10 × e_a(kPa)\n",
        "    merged['vp'] = 10.0 * merged['ea_kpa']\n",
        "    \n",
        "    # Return DataFrame with date and vp columns\n",
        "    vp_df = merged[['date', 'vp']].copy()\n",
        "    vp_df = vp_df.rename(columns={'vp': 'value'})\n",
        "    \n",
        "    return vp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.579552Z",
          "iopub.status.busy": "2025-12-21T10:28:42.579436Z",
          "iopub.status.idle": "2025-12-21T10:28:42.589681Z",
          "shell.execute_reply": "2025-12-21T10:28:42.589259Z"
        }
      },
      "outputs": [],
      "source": [
        "def create_met_file(tasmax_df, tasmin_df, pr_df, rsds_df, hurs_df=None, scenario=None, \n",
        "                    output_dir=None, latitude=None, longitude=None, model=None):\n",
        "    \"\"\"\n",
        "    Create MET format file from tasmax, tasmin, pr, rsds, and optional hurs DataFrames.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    tasmax_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for maximum temperature\n",
        "    tasmin_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for minimum temperature\n",
        "    pr_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for precipitation\n",
        "    rsds_df : pd.DataFrame\n",
        "        DataFrame with date and value columns for surface downwelling shortwave radiation (W/m²) - REQUIRED\n",
        "    hurs_df : pd.DataFrame, optional\n",
        "        DataFrame with date and value columns for relative humidity (%) - if provided, VP will be calculated\n",
        "    scenario : str\n",
        "        Scenario name (e.g., SSP585 or SSP245)\n",
        "    output_dir : str\n",
        "        Output directory path\n",
        "    latitude : float\n",
        "        Latitude in decimal degrees\n",
        "    longitude : float\n",
        "        Longitude in decimal degrees\n",
        "    model : str\n",
        "        Model name (e.g., \"ACCESS CM2\")\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (tav, amp, num_rows)\n",
        "    \"\"\"\n",
        "    # Merge all dataframes on date\n",
        "    merged = tasmax_df.copy()\n",
        "    merged = merged.rename(columns={'value': 'maxt'})\n",
        "    merged['date'] = pd.to_datetime(merged['date'])\n",
        "    \n",
        "    # Merge tasmin\n",
        "    tasmin_df['date'] = pd.to_datetime(tasmin_df['date'])\n",
        "    merged = merged.merge(tasmin_df[['date', 'value']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value': 'mint'})\n",
        "    \n",
        "    # Merge pr (precipitation/rain)\n",
        "    pr_df['date'] = pd.to_datetime(pr_df['date'])\n",
        "    merged = merged.merge(pr_df[['date', 'value']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value': 'rain'})\n",
        "    \n",
        "    # Merge rsds (radiation) - REQUIRED\n",
        "    # rsds is in W/m², convert to MJ/m² by multiplying by 0.0864 (seconds per day / 1e6)\n",
        "    if rsds_df is None or len(rsds_df) == 0:\n",
        "        raise ValueError(\"rsds_df is required but is None or empty\")\n",
        "    \n",
        "    rsds_df['date'] = pd.to_datetime(rsds_df['date'])\n",
        "    # Convert W/m² to MJ/m² (multiply by seconds per day / 1e6)\n",
        "    rsds_df['value_mj'] = rsds_df['value'] * 0.0864\n",
        "    merged = merged.merge(rsds_df[['date', 'value_mj']], on='date', how='outer')\n",
        "    merged = merged.rename(columns={'value_mj': 'radn'})\n",
        "    \n",
        "    # Calculate vp (vapor pressure) if hurs is provided, otherwise leave blank\n",
        "    if hurs_df is not None and len(hurs_df) > 0:\n",
        "        # Calculate VP using SILO method\n",
        "        vp_df = calculate_vapor_pressure(hurs_df, tasmax_df, tasmin_df)\n",
        "        vp_df['date'] = pd.to_datetime(vp_df['date'])\n",
        "        merged = merged.merge(vp_df[['date', 'value']], on='date', how='left')  # Use 'left' to keep only dates that exist in merged\n",
        "        merged = merged.rename(columns={'value': 'vp'})\n",
        "        # Ensure VP is numeric only where it was calculated, leave NaN where hurs data was missing\n",
        "        merged['vp'] = pd.to_numeric(merged['vp'], errors='coerce')\n",
        "        # Count how many VP values were calculated vs missing\n",
        "        vp_calculated = merged['vp'].notna().sum()\n",
        "        vp_missing = merged['vp'].isna().sum()\n",
        "        print(f\"  [OK] Calculated vapor pressure for {vp_calculated} days\")\n",
        "        if vp_missing > 0:\n",
        "            print(f\"  [WARNING] VP data missing for {vp_missing} days (hurs data not available) - these will be left blank\")\n",
        "    else:\n",
        "        # vp (vapor pressure) is left blank if hurs not available\n",
        "        merged['vp'] = ''\n",
        "        print(f\"  [INFO] hurs not provided - VP left blank\")\n",
        "    \n",
        "    # Sort by date\n",
        "    merged = merged.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # CRITICAL FIX: Create a complete date range to ensure all days are included\n",
        "    # This ensures leap years have 366 days and non-leap years have 365 days\n",
        "    min_date = merged['date'].min()\n",
        "    max_date = merged['date'].max()\n",
        "    \n",
        "    # Ensure the last year is complete - extend to end of year if needed\n",
        "    last_year = max_date.year\n",
        "    last_day_of_year = pd.Timestamp(year=last_year, month=12, day=31)\n",
        "    if max_date < last_day_of_year:\n",
        "        # Extend max_date to end of year to ensure complete year coverage\n",
        "        max_date = last_day_of_year\n",
        "    \n",
        "    # Create complete date range (includes all days, including day 366 for leap years)\n",
        "    complete_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
        "    \n",
        "    # Set date as index for reindexing\n",
        "    merged = merged.set_index('date')\n",
        "    \n",
        "    # Reindex to include all days in the complete range\n",
        "    merged = merged.reindex(complete_date_range)\n",
        "    \n",
        "    # Fill missing values for numeric columns using forward fill then backward fill\n",
        "    # This handles gaps in the data gracefully\n",
        "    # NOTE: VP is NOT filled - if hurs data is missing, VP remains blank\n",
        "    numeric_cols = ['maxt', 'mint', 'rain', 'radn']\n",
        "    for col in numeric_cols:\n",
        "        if col in merged.columns:\n",
        "            # Forward fill first, then backward fill to handle gaps at start/end\n",
        "            merged[col] = merged[col].ffill().bfill()\n",
        "            # If still NaN, fill with 0 (shouldn't happen, but safety check)\n",
        "            merged[col] = merged[col].fillna(0.0)\n",
        "    \n",
        "    # Handle VP separately - DO NOT fill missing VP values\n",
        "    # If hurs data was missing, VP should remain blank (NaN or empty string)\n",
        "    # This includes cases where date range was extended but source data doesn't exist (e.g., Dec 31, 2064)\n",
        "    if 'vp' in merged.columns:\n",
        "        # Convert all NaN values to empty string (whether numeric or object type)\n",
        "        merged.loc[merged['vp'].isna(), 'vp'] = ''\n",
        "        # If VP was numeric and now has empty strings, convert dtype to object to allow mixed types\n",
        "        if merged['vp'].dtype != 'object':\n",
        "            # Check if we have any empty strings\n",
        "            if (merged['vp'] == '').any():\n",
        "                # Convert to object to allow both numeric and string values\n",
        "                merged['vp'] = merged['vp'].astype(object)\n",
        "                # Convert numeric values back to float\n",
        "                mask = (merged['vp'] != '') & (merged['vp'].notna())\n",
        "                merged.loc[mask, 'vp'] = pd.to_numeric(merged.loc[mask, 'vp'], errors='coerce')\n",
        "                # Convert any resulting NaN back to empty string\n",
        "                merged.loc[merged['vp'].isna(), 'vp'] = ''\n",
        "    \n",
        "    # Reset index to get date back as a column\n",
        "    merged = merged.reset_index()\n",
        "    merged = merged.rename(columns={'index': 'date'})\n",
        "    \n",
        "    # Calculate tav and amp (use only non-NaN values for calculation)\n",
        "    merged_temp = merged[['date', 'maxt', 'mint']].copy()\n",
        "    merged_temp = merged_temp.set_index('date')\n",
        "    merged_temp.index = pd.to_datetime(merged_temp.index)\n",
        "    tav, amp = calculate_tav_amp(merged_temp)\n",
        "    \n",
        "    # Create year and day columns\n",
        "    merged['year'] = merged['date'].dt.year\n",
        "    merged['day'] = merged['date'].dt.dayofyear\n",
        "    \n",
        "    # Add empty columns for evap and code\n",
        "    merged['evap'] = ''  # Leave blank\n",
        "    merged['code'] = '222222'  # Hardcoded code value for all rows\n",
        "    \n",
        "    # Ensure vp column exists (should already be set above)\n",
        "    if 'vp' not in merged.columns:\n",
        "        merged['vp'] = ''\n",
        "    \n",
        "    # Convert VP to appropriate format for output\n",
        "    # - Numeric values (calculated) should remain as float\n",
        "    # - Missing values (NaN or empty string) should be empty string\n",
        "    if merged['vp'].dtype != 'object':\n",
        "        # VP is numeric - convert NaN to empty string, keep valid values as float\n",
        "        merged.loc[merged['vp'].isna(), 'vp'] = ''\n",
        "        # Convert non-empty values to float\n",
        "        mask = (merged['vp'] != '') & (merged['vp'].notna())\n",
        "        merged.loc[mask, 'vp'] = pd.to_numeric(merged.loc[mask, 'vp'], errors='coerce')\n",
        "    # If VP is already string (empty), keep it as is\n",
        "    \n",
        "    # Check for blank VP values and issue warning\n",
        "    vp_blank_count = ((merged['vp'] == '') | (merged['vp'].isna())).sum()\n",
        "    if vp_blank_count > 0:\n",
        "        # Find which years have blank VP values\n",
        "        blank_vp_dates = merged[((merged['vp'] == '') | (merged['vp'].isna())) & merged['date'].notna()]\n",
        "        if len(blank_vp_dates) > 0:\n",
        "            blank_years = sorted(blank_vp_dates['date'].dt.year.unique())\n",
        "            years_str = ', '.join(map(str, blank_years))\n",
        "            print(f\"  [WARNING] Found {vp_blank_count} days with blank VP values (missing hurs data)\")\n",
        "            print(f\"  [WARNING] Years affected: {years_str}\")\n",
        "            print(f\"  [WARNING] These VP values will appear as blank spaces in the output files\")\n",
        "    \n",
        "    met_data = merged[['year', 'day', 'radn', 'maxt', 'mint', 'rain', 'evap', 'vp', 'code']].copy()\n",
        "    \n",
        "    # Prepare header\n",
        "    current_date = datetime.now().strftime('%Y%m%d')\n",
        "    model_scenario = f\"{model.replace(' ', '_')}_{scenario}\" if model and scenario else \"CMIP6\"\n",
        "    \n",
        "    header = f\"\"\"[weather.met.weather]\n",
        "!Your Ref:  \"\n",
        "latitude = {latitude:.2f}  (DECIMAL DEGREES)\n",
        "longitude =  {longitude:.2f}  (DECIMAL DEGREES)\n",
        "tav = {tav:.2f} (oC) ! Annual average ambient temperature.\n",
        "amp = {amp:.2f} (oC) ! Annual amplitude in mean monthly temperature.\n",
        "!Data Extracted from CMIP6 {model} {scenario} dataset on {current_date} for APSIM\n",
        "!As evaporation is read at 9am, it has been shifted to day before\n",
        "!ie The evaporation measured on 20 April is in row for 19 April\n",
        "!The 6 digit code indicates the source of the 6 data columns\n",
        "!0 actual observation, 1 actual observation composite station\n",
        "!2 interpolated from daily observations\n",
        "!3 interpolated from daily observations using anomaly interpolation method for CLIMARC data\n",
        "!6 synthetic pan\n",
        "!7 interpolated long term averages\n",
        "!more detailed two digit codes are available in SILO's 'Standard' format files\n",
        "!\n",
        "!For further information see the documentation on the datadrill\n",
        "!  http://www.longpaddock.qld.gov.au/silo\n",
        "!\n",
        "year  day radn  maxt   mint  rain  evap    vp   code\n",
        " ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\n",
        "\"\"\"\n",
        "    \n",
        "    # Create output filename with coordinate-based naming\n",
        "    # Format: {Model}_{Scenario}_{Lat}_{Lon}.met (e.g., ACCESS_CM2_SSP245_-31.75_117.60.met)\n",
        "    lat_str = f\"{latitude:.2f}\"\n",
        "    lon_str = f\"{longitude:.2f}\"\n",
        "    output_filename = f\"{model_scenario}_{lat_str}_{lon_str}.met\"\n",
        "    output_path = os.path.join(output_dir, output_filename)\n",
        "    \n",
        "    # Write MET file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(header)\n",
        "        # Write data rows\n",
        "        for _, row in met_data.iterrows():\n",
        "            # Format the row with proper spacing\n",
        "                        # Format numbers with proper spacing\n",
        "            radn_val = row['radn'] if row['radn'] != '' and pd.notna(row['radn']) else ''\n",
        "            evap_val = row['evap'] if row['evap'] != '' and pd.notna(row['evap']) else ''\n",
        "            \n",
        "            if radn_val != '':\n",
        "                radn_str = f\"{float(radn_val):6.1f}\"\n",
        "            else:\n",
        "                radn_str = \"      \"  # 6 spaces\n",
        "                \n",
        "            if evap_val != '':\n",
        "                evap_str = f\"{float(evap_val):6.1f}\"\n",
        "            else:\n",
        "                evap_str = \"      \"  # 6 spaces\n",
        "                \n",
        "            # Handle VP - can be numeric (calculated) or empty string (blank)\n",
        "            if pd.notna(row['vp']) and row['vp'] != '':\n",
        "                try:\n",
        "                    vp_str = f\"{float(row['vp']):6.1f}\"\n",
        "                except (ValueError, TypeError):\n",
        "                    vp_str = \"      \"  # 6 spaces\n",
        "            else:\n",
        "                vp_str = \"      \"  # 6 spaces\n",
        "            \n",
        "                        # Code is hardcoded to '222222' for all rows\n",
        "            code_str = \"222222\"\n",
        "            \n",
        "            # Handle NaN values for maxt, mint, rain - use 0.0 as default\n",
        "            maxt_val = row['maxt'] if pd.notna(row['maxt']) else 0.0\n",
        "            mint_val = row['mint'] if pd.notna(row['mint']) else 0.0\n",
        "            rain_val = row['rain'] if pd.notna(row['rain']) else 0.0\n",
        "            \n",
        "            # Format with proper column widths\n",
        "            line = f\"{int(row['year']):4d} {int(row['day']):4d} {radn_str} {maxt_val:6.1f} {mint_val:6.1f} {rain_val:6.1f} {evap_str} {vp_str} {code_str}\\n\"\n",
        "            f.write(line)\n",
        "    \n",
        "    # Count blank VP values in final output for warning (after file is written)\n",
        "    final_blank_vp = ((met_data['vp'] == '') | (met_data['vp'].isna())).sum()\n",
        "    \n",
        "    print(f\"  [OK] Created MET file: {output_filename}\")\n",
        "    \n",
        "    # Final warning about blank VP values\n",
        "    if final_blank_vp > 0:\n",
        "        blank_years_list = sorted(met_data[((met_data['vp'] == '') | (met_data['vp'].isna()))]['year'].unique())\n",
        "        years_str = ', '.join(map(str, blank_years_list))\n",
        "        print(f\"  [WARNING] ========================================\")\n",
        "        print(f\"  [WARNING] BLANK VP VALUES DETECTED!\")\n",
        "        print(f\"  [WARNING] {final_blank_vp} days have blank VP values (missing hurs data)\")\n",
        "        print(f\"  [WARNING] Affected years: {years_str}\")\n",
        "        print(f\"  [WARNING] These will appear as blank spaces in MET/CSV files\")\n",
        "        print(f\"  [WARNING] ========================================\")\n",
        "    \n",
        "    # Also create CSV version with same structure\n",
        "    csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}.csv\"\n",
        "    csv_path = os.path.join(output_dir, csv_filename)\n",
        "    \n",
        "    # Write CSV (without header comments, just data)\n",
        "    met_data.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.1f')\n",
        "    print(f\"  [OK] Created CSV file: {csv_filename}\")\n",
        "    \n",
        "    return tav, amp, len(met_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Main Processing\n",
        "\n",
        "This section extracts data for all variables and creates the MET file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.591211Z",
          "iopub.status.busy": "2025-12-21T10:28:42.591065Z",
          "iopub.status.idle": "2025-12-21T10:28:42.597501Z",
          "shell.execute_reply": "2025-12-21T10:28:42.597053Z"
        }
      },
      "outputs": [],
      "source": [
        "def process_coordinate(model, scenario, latitude, longitude, variables, cmip6_base_dir, output_dir, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Main processing function for user-provided coordinate.\n",
        "    Extract all variables from NC files and convert to MET format.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : str\n",
        "        Model name (e.g., \"ACCESS CM2\")\n",
        "    scenario : str\n",
        "        Scenario name (e.g., \"SSP245\")\n",
        "    latitude : float\n",
        "        Target latitude\n",
        "    longitude : float\n",
        "        Target longitude\n",
        "    variables : list\n",
        "        List of variable names to extract\n",
        "    cmip6_base_dir : str\n",
        "        Base directory containing Model Scenario folders\n",
        "    output_dir : str\n",
        "        Output directory for results\n",
        "    tolerance : float\n",
        "        Coordinate matching tolerance\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    dict: Summary statistics\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Processing Coordinate: ({latitude:.6f}, {longitude:.6f})\")\n",
        "    print(f\"Model: {model}, Scenario: {scenario}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Construct data directory path\n",
        "    data_dir = os.path.join(cmip6_base_dir, f\"{model} {scenario}\")\n",
        "    \n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"ERROR: Data directory not found: {data_dir}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"\\nData directory: {data_dir}\")\n",
        "    \n",
        "    # Extract data for all variables\n",
        "    extracted_data = {}\n",
        "    \n",
        "    for variable in variables:\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Processing variable: {variable}\")\n",
        "        print(f\"{'='*70}\")\n",
        "        \n",
        "        # Extract data from NetCDF files\n",
        "        df = extract_daily_data_from_netcdf(\n",
        "            data_dir, \n",
        "            variable, \n",
        "            latitude, \n",
        "            longitude, \n",
        "            tolerance=tolerance\n",
        "        )\n",
        "        \n",
        "        if df is not None and len(df) > 0:\n",
        "            extracted_data[variable] = df\n",
        "            \n",
        "            # Save individual variable CSV\n",
        "            # Format: {Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv\n",
        "            lat_str = f\"{latitude:.2f}\"\n",
        "            lon_str = f\"{longitude:.2f}\"\n",
        "            model_scenario = f\"{model.replace(' ', '_')}_{scenario}\"\n",
        "            csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
        "            csv_path = os.path.join(output_dir, csv_filename)\n",
        "            df.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.6f')\n",
        "            print(f\"  [OK] Saved CSV: {csv_filename}\")\n",
        "        else:\n",
        "            print(f\"  WARNING: No data extracted for {variable}\")\n",
        "    \n",
        "    # Check if required variables are available for MET conversion\n",
        "    # Note: rsds is now mandatory (required for radn in MET format)\n",
        "    required_vars = ['tasmax', 'tasmin', 'pr', 'rsds']\n",
        "    missing_vars = [v for v in required_vars if v not in extracted_data]\n",
        "    \n",
        "    if missing_vars:\n",
        "        print(f\"\\nERROR: Missing required variables for MET conversion: {missing_vars}\")\n",
        "        return None\n",
        "    \n",
        "    # Create MET file\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Creating MET file...\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Get required variables\n",
        "    tasmax_df = extracted_data['tasmax']\n",
        "    tasmin_df = extracted_data['tasmin']\n",
        "    pr_df = extracted_data['pr']\n",
        "    \n",
        "    # Get rsds variable for MET format (now mandatory)\n",
        "    rsds_df = extracted_data.get('rsds', None)\n",
        "    if rsds_df is None:\n",
        "        print(f\"  ERROR: rsds is required but was not extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Get hurs variable for VP calculation (optional but recommended)\n",
        "    hurs_df = extracted_data.get('hurs', None)\n",
        "    if hurs_df is not None:\n",
        "        print(f\"  [INFO] hurs data available - VP will be calculated using SILO method\")\n",
        "    else:\n",
        "        print(f\"  [INFO] hurs data not available - VP will be left blank\")\n",
        "    \n",
        "    # Note: code is hardcoded to '222222'\n",
        "    \n",
        "    tav, amp, num_rows = create_met_file(\n",
        "        tasmax_df=tasmax_df,\n",
        "        tasmin_df=tasmin_df,\n",
        "        pr_df=pr_df,\n",
        "        rsds_df=rsds_df,\n",
        "        hurs_df=hurs_df,\n",
        "        scenario=scenario,\n",
        "        output_dir=output_dir,\n",
        "        latitude=latitude,\n",
        "        longitude=longitude,\n",
        "        model=model\n",
        "    )\n",
        "    \n",
        "    # Summary\n",
        "    summary = {\n",
        "        'latitude': latitude,\n",
        "        'longitude': longitude,\n",
        "        'model': model,\n",
        "        'scenario': scenario,\n",
        "        'variables_extracted': list(extracted_data.keys()),\n",
        "        'num_variables': len(extracted_data),\n",
        "        'tav': tav,\n",
        "        'amp': amp,\n",
        "        'num_rows': num_rows,\n",
        "        'date_range': {\n",
        "            'start': tasmax_df['date'].min(),\n",
        "            'end': tasmax_df['date'].max()\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(\"Processing Summary\")\n",
        "    print(f\"{'='*70}\")\n",
        "    print(f\"  Variables extracted: {len(extracted_data)}\")\n",
        "    print(f\"    - {', '.join(extracted_data.keys())}\")\n",
        "    print(f\"  MET file rows: {num_rows}\")\n",
        "    print(f\"  Date range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
        "    print(f\"  tav (annual average temp): {tav:.2f} °C\")\n",
        "    print(f\"  amp (annual amplitude): {amp:.2f} °C\")\n",
        "    print(f\"  Output directory: {output_dir}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    return summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration Summary\n",
        "\n",
        "Review the configuration below before running the main processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.598952Z",
          "iopub.status.busy": "2025-12-21T10:28:42.598737Z",
          "iopub.status.idle": "2025-12-21T10:28:42.602836Z",
          "shell.execute_reply": "2025-12-21T10:28:42.602348Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "CONFIGURATION SUMMARY\n",
            "======================================================================\n",
            "\n",
            "Model: ACCESS CM2\n",
            "Scenario: SSP245\n",
            "\n",
            "Coordinates:\n",
            "  Latitude: -31.750000°\n",
            "  Longitude: 117.600000°\n",
            "  Tolerance: 0.01 degrees (≈ 1.1 km)\n",
            "\n",
            "Variables to process (5):\n",
            "  - tasmax → maxt (maximum temperature)\n",
            "  - tasmin → mint (minimum temperature)\n",
            "  - pr → rain (precipitation)\n",
            "  - rsds → radn (radiation, W/m² → MJ/m²)\n",
            "\n",
            "Directories:\n",
            "  CMIP6 Base: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
            "  Data Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP245\n",
            "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.75_117.60\n",
            "\n",
            "Output Files:\n",
            "  MET file: ACCESS_CM2_SSP245_-31.75_117.60.met\n",
            "  CSV file: ACCESS_CM2_SSP245_-31.75_117.60.csv\n",
            "  Variable CSVs: 5 files (one per variable)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Display full configuration\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURATION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nModel: {MODEL}\")\n",
        "print(f\"Scenario: {SCENARIO}\")\n",
        "print(f\"\\nCoordinates:\")\n",
        "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
        "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
        "print(f\"  Tolerance: {COORD_TOLERANCE} degrees (≈ {COORD_TOLERANCE * 111:.1f} km)\")\n",
        "print(f\"\\nVariables to process ({len(VARIABLES)}):\")\n",
        "for var in VARIABLES:\n",
        "    if var == 'tasmax':\n",
        "        print(f\"  - {var} → maxt (maximum temperature)\")\n",
        "    elif var == 'tasmin':\n",
        "        print(f\"  - {var} → mint (minimum temperature)\")\n",
        "    elif var == 'pr':\n",
        "        print(f\"  - {var} → rain (precipitation)\")\n",
        "    elif var == 'rsds':\n",
        "        print(f\"  - {var} → radn (radiation, W/m² → MJ/m²)\")\n",
        "print(f\"\\nDirectories:\")\n",
        "print(f\"  CMIP6 Base: {CMIP6_BASE_DIR}\")\n",
        "print(f\"  Data Directory: {os.path.join(CMIP6_BASE_DIR, f'{MODEL} {SCENARIO}')}\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"\\nOutput Files:\")\n",
        "print(f\"  MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
        "print(f\"  CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
        "print(f\"  Variable CSVs: {len(VARIABLES)} files (one per variable)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.604401Z",
          "iopub.status.busy": "2025-12-21T10:28:42.604209Z",
          "iopub.status.idle": "2025-12-21T10:31:27.705974Z",
          "shell.execute_reply": "2025-12-21T10:31:27.705107Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STARTING PROCESSING\n",
            "======================================================================\n",
            "Model: ACCESS CM2\n",
            "Scenario: SSP245\n",
            "Coordinates: (-31.750000, 117.600000)\n",
            "Variables to process: 5 (tasmax, tasmin, pr, rsds, hurs)\n",
            "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.75_117.60\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "Processing Coordinate: (-31.750000, 117.600000)\n",
            "Model: ACCESS CM2, Scenario: SSP245\n",
            "======================================================================\n",
            "\n",
            "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP245\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmax\n",
            "======================================================================\n",
            "  Found files in subdirectory: tasmax_ACCESS CM2 SSP245/\n",
            "  Found 30 NetCDF files\n",
            "  Using grid point: (-31.7500, 117.6000)\n",
            "  Processing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  tasmax: 100%|██████████| 30/30 [01:32<00:00,  3.10s/file]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Combining data from 30 files...\n",
            "  ✓ Extracted 10,957 daily records in 92.9 seconds\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [OK] Saved CSV: ACCESS_CM2_SSP245_-31.75_117.60_tasmax.csv\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmin\n",
            "======================================================================\n",
            "  Found files in subdirectory: tasmin_ACCESS CM2 SSP245/\n",
            "  Found 30 NetCDF files\n",
            "  Using grid point: (-31.7500, 117.6000)\n",
            "  Processing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  tasmin: 100%|██████████| 30/30 [00:57<00:00,  1.93s/file]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Combining data from 30 files...\n",
            "  ✓ Extracted 10,957 daily records in 57.8 seconds\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [OK] Saved CSV: ACCESS_CM2_SSP245_-31.75_117.60_tasmin.csv\n",
            "\n",
            "======================================================================\n",
            "Processing variable: pr\n",
            "======================================================================\n",
            "  Found files in subdirectory: pr_ACCESS CM2 SSP245/\n",
            "  Found 30 NetCDF files\n",
            "  Using grid point: (-31.7500, 117.6000)\n",
            "  Processing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  pr: 100%|██████████| 30/30 [01:03<00:00,  2.11s/file]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Combining data from 30 files...\n",
            "  ✓ Extracted 10,957 daily records in 63.3 seconds\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [OK] Saved CSV: ACCESS_CM2_SSP245_-31.75_117.60_pr.csv\n",
            "\n",
            "======================================================================\n",
            "Processing variable: rsds\n",
            "======================================================================\n",
            "  Found files in subdirectory: rsds_BARRA SSP245/\n",
            "  Found 30 NetCDF files\n",
            "  Using grid point: (-31.7500, 117.6000)\n",
            "  Processing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  rsds: 100%|██████████| 30/30 [00:04<00:00,  6.62file/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Combining data from 30 files...\n",
            "  ✓ Extracted 10,957 daily records in 4.5 seconds\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [OK] Saved CSV: ACCESS_CM2_SSP245_-31.75_117.60_rsds.csv\n",
            "\n",
            "======================================================================\n",
            "Processing variable: hurs\n",
            "======================================================================\n",
            "  Found files in subdirectory: hurs_BARRA SSP245/\n",
            "  Found 30 NetCDF files\n",
            "  Using grid point: (-31.7500, 117.6000)\n",
            "  Processing files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  hurs: 100%|██████████| 30/30 [00:04<00:00,  6.86file/s]\n",
            "C:\\Users\\ibian\\AppData\\Local\\Temp\\ipykernel_8604\\1202218906.py:119: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
            "  merged.loc[merged['vp'].isna(), 'vp'] = ''\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Combining data from 30 files...\n",
            "  ✓ Extracted 10,957 daily records in 4.4 seconds\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  [OK] Saved CSV: ACCESS_CM2_SSP245_-31.75_117.60_hurs.csv\n",
            "\n",
            "======================================================================\n",
            "Creating MET file...\n",
            "======================================================================\n",
            "  [INFO] hurs data available - VP will be calculated using SILO method\n",
            "  [OK] Calculated vapor pressure for 10957 days\n",
            "  [WARNING] Found 1 days with blank VP values (missing hurs data)\n",
            "  [WARNING] Years affected: 2064\n",
            "  [WARNING] These VP values will appear as blank spaces in the output files\n",
            "  [OK] Created MET file: ACCESS_CM2_SSP245_-31.75_117.60.met\n",
            "  [WARNING] ========================================\n",
            "  [WARNING] BLANK VP VALUES DETECTED!\n",
            "  [WARNING] 1 days have blank VP values (missing hurs data)\n",
            "  [WARNING] Affected years: 2064\n",
            "  [WARNING] These will appear as blank spaces in MET/CSV files\n",
            "  [WARNING] ========================================\n",
            "  [OK] Created CSV file: ACCESS_CM2_SSP245_-31.75_117.60.csv\n",
            "\n",
            "======================================================================\n",
            "Processing Summary\n",
            "======================================================================\n",
            "  Variables extracted: 5\n",
            "    - tasmax, tasmin, pr, rsds, hurs\n",
            "  MET file rows: 10958\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  tav (annual average temp): 19.41 °C\n",
            "  amp (annual amplitude): 7.43 °C\n",
            "  Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.75_117.60\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "✓ PROCESSING COMPLETED SUCCESSFULLY!\n",
            "======================================================================\n",
            "\n",
            "Output files created:\n",
            "  - MET file: ACCESS_CM2_SSP245_-31.75_117.60.met\n",
            "  - CSV file: ACCESS_CM2_SSP245_-31.75_117.60.csv\n",
            "  - Individual variable CSVs: 5 files\n",
            "\n",
            "All files saved to: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.75_117.60\n"
          ]
        }
      ],
      "source": [
        "# Execute main processing\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING PROCESSING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {MODEL}\")\n",
        "print(f\"Scenario: {SCENARIO}\")\n",
        "print(f\"Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
        "print(f\"Variables to process: {len(VARIABLES)} ({', '.join(VARIABLES)})\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "summary = process_coordinate(\n",
        "    model=MODEL,\n",
        "    scenario=SCENARIO,\n",
        "    latitude=LATITUDE,\n",
        "    longitude=LONGITUDE,\n",
        "    variables=VARIABLES,\n",
        "    cmip6_base_dir=CMIP6_BASE_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    tolerance=COORD_TOLERANCE\n",
        ")\n",
        "\n",
        "if summary:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ PROCESSING COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nOutput files created:\")\n",
        "    print(f\"  - MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
        "    print(f\"  - CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
        "    print(f\"  - Individual variable CSVs: {len(summary['variables_extracted'])} files\")\n",
        "    print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✗ PROCESSING FAILED\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Please check error messages above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Documentation\n",
        "\n",
        "### CMIP6 Data Structure and Conventions\n",
        "\n",
        "CMIP6 (Coupled Model Intercomparison Project Phase 6) climate data is organized by:\n",
        "- **Model**: Climate model name (e.g., \"ACCESS CM2\")\n",
        "- **Scenario**: Shared Socioeconomic Pathway (e.g., \"SSP245\", \"SSP585\")\n",
        "- **Variable**: Climate variable name (e.g., \"tasmax\", \"pr\")\n",
        "\n",
        "Data files are stored in NetCDF format (.nc) and organized in folders:\n",
        "`C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\{variable}_*\\*.nc`\n",
        "\n",
        "### Coordinate System\n",
        "\n",
        "Coordinates are provided in decimal degrees:\n",
        "- **Latitude**: -90 to 90 (negative for Southern Hemisphere)\n",
        "- **Longitude**: -180 to 180 (negative for Western Hemisphere)\n",
        "\n",
        "The notebook finds the nearest grid point within a tolerance (default: 0.01 degrees ≈ 1.1 km).\n",
        "\n",
        "### Variable Extraction Workflow\n",
        "\n",
        "1. For each variable, search for matching NetCDF files\n",
        "2. Find nearest grid point to user-specified coordinates\n",
        "3. Extract time series data from all files\n",
        "4. Extract dates using time coordinate from NetCDF (if available) or filename\n",
        "5. **Leap year handling**: Automatically reads all 366 days for leap years (e.g., 2024, 2028)\n",
        "6. Combine and sort by date\n",
        "7. Save as individual CSV files\n",
        "\n",
        "### MET Format Specifications\n",
        "\n",
        "APSIM MET format requires:\n",
        "- **Header**: Metadata including latitude, longitude, tav, amp\n",
        "- **Data columns**: year, day, radn, maxt, mint, rain, evap, vp, code\n",
        "- **Required data**: maxt (from tasmax), mint (from tasmin), rain (from pr), radn (from rsds)\n",
        "- **radn conversion**: rsds (W/m²) converted to radn (MJ/m²) - mandatory\n",
        "- **Blank fields**: vp and code are left blank\n",
        "\n",
        "### Unit Conversions\n",
        "\n",
        "- **rsds** (W/m²) → **radn** (MJ/m²): Multiply by 0.0864\n",
        "- **vp** and **code**: Left blank in MET format\n",
        "\n",
        "### File Naming Conventions\n",
        "\n",
        "All output files use coordinate-based naming:\n",
        "- MET files: `{Model}_{Scenario}_{Lat}_{Lon}.met`\n",
        "- CSV files: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv`\n",
        "\n",
        "Example: `ACCESS_CM2_SSP245_-31.75_117.60.met`\n",
        "\n",
        "### Model and Scenario Selection\n",
        "\n",
        "Update the `MODEL` and `SCENARIO` variables in Section 1 to match your data folder structure.\n",
        "\n",
        "Common scenarios:\n",
        "- **SSP245**: Middle-of-the-road scenario\n",
        "- **SSP585**: High-emissions scenario"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
