{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Point CMIP6 Retrieval and MET Conversion Notebook\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook combines CMIP6 NetCDF data extraction with APSIM MET format conversion. It processes climate variables from NC files organized in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}` folders and converts them to MET format for APSIM simulations.\n",
    "\n",
    "## File Structure\n",
    "\n",
    "- **Input**: NC files in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\` (e.g., `ACCESS CM2 SSP245`)\n",
    "- **User Input**: Latitude and longitude coordinates (decimal degrees)\n",
    "- **Output**: MET files and CSV files for the specified coordinate\n",
    "\n",
    "## Variables Processed\n",
    "\n",
    "The notebook processes 4 climate variables:\n",
    "\n",
    "- **tasmax**: Daily maximum temperature (°C) → **maxt** in MET format\n",
    "- **tasmin**: Daily minimum temperature (°C) → **mint** in MET format\n",
    "- **pr**: Daily precipitation (mm) → **rain** in MET format\n",
    "- **rsds**: Daily surface downwelling shortwave radiation (W/m²) → **radn** (MJ/m²) in MET format\n",
    "\n",
    "**Note**: vp (vapor pressure) and code fields are left blank in the MET file\n",
    "\n",
    "## MET Format Specifications\n",
    "\n",
    "The MET format is used by APSIM for weather data input. It includes:\n",
    "\n",
    "- **Required fields**: year, day, maxt (from tasmax), mint (from tasmin), rain (from pr)\n",
    "- **Optional fields**: radn (from rsds, converted from W/m² to MJ/m²)\n",
    "- **Blank fields**: evap (evaporation), vp (vapor pressure), code (data quality code) - left blank\n",
    "- **Metadata**: latitude, longitude, tav (annual average temperature), amp (annual amplitude)\n",
    "\n",
    "## File Naming Conventions\n",
    "\n",
    "- **Input NC files**: `{Model} {Scenario}\\*{variable}*.nc` (e.g., `ACCESS CM2 SSP245\\tasmax*.nc`)\n",
    "- **Output MET files**: `{Model}_{Scenario}_{Lat}_{Lon}.met` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60.met`)\n",
    "- **Output CSV files**: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60_tasmax.csv`)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. Set the configuration parameters (Model, Scenario, output directory) in Section 1\n",
    "2. Provide your target latitude and longitude coordinates in Section 2\n",
    "3. Run all cells sequentially to extract data and create MET files\n",
    "4. Output files will be saved with coordinate-based naming\n",
    "\n",
    "## Coordinate Matching\n",
    "\n",
    "The notebook finds the nearest grid point to your specified coordinates within a tolerance (default: 0.01 degrees ≈ 1.1 km). If the nearest point is outside tolerance, a warning will be displayed.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- **CMIP6 data structure**: Data is organized by Model and Scenario in separate folders\n",
    "- **Coordinate format**: Provide coordinates in decimal degrees (latitude: -90 to 90, longitude: -180 to 180)\n",
    "- **Variable extraction**: 4 variables (tasmax, tasmin, pr, rsds) are extracted from NC files and saved as individual CSVs\n",
    "- **MET conversion**: tasmax→maxt, tasmin→mint, pr→rain, rsds→radn\n",
    "- **Unit conversions**: rsds (W/m²) is converted to radn (MJ/m²) by multiplying by 0.0864\n",
    "- **Blank fields**: vp and code are left blank in MET format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\"  # Output directory for MET files\n",
    "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
    "\n",
    "# Model and Scenario - UPDATE THESE AS NEEDED\n",
    "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
    "SCENARIO = \"SSP245\"   # e.g., \"SSP245\" or \"SSP585\"\n",
    "\n",
    "# Variables to process (4 variables)\n",
    "# MET format mapping:\n",
    "# - tasmax → maxt (maximum temperature)\n",
    "# - tasmin → mint (minimum temperature)\n",
    "# - pr → rain (precipitation)\n",
    "# - rsds → radn (radiation, converted from W/m² to MJ/m²)\n",
    "# Note: vp and code are left blank in MET format\n",
    "VARIABLES = ['tasmax', 'tasmin', 'pr', 'rsds']\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model: {MODEL}\")\n",
    "print(f\"  - Scenario: {SCENARIO}\")\n",
    "print(f\"  - Coordinate Tolerance: {COORD_TOLERANCE} degrees\")\n",
    "print(f\"  - Variables: {', '.join(VARIABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: User Coordinate Input\n",
    "\n",
    "Provide the latitude and longitude coordinates for the grid point you want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER INPUT: Provide your target coordinates here\n",
    "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
    "LONGITUDE = 117.5999984741211  # Target longitude in decimal degrees (-180 to 180)\n",
    "\n",
    "# Validate coordinates\n",
    "if not (-90 <= LATITUDE <= 90):\n",
    "    raise ValueError(f\"Latitude must be between -90 and 90. Provided: {LATITUDE}\")\n",
    "if not (-180 <= LONGITUDE <= 180):\n",
    "    raise ValueError(f\"Longitude must be between -180 and 180. Provided: {LONGITUDE}\")\n",
    "\n",
    "print(f\"Target Coordinate:\")\n",
    "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
    "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Scenario: {SCENARIO}\")\n",
    "print(f\"  Tolerance: {COORD_TOLERANCE} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: NetCDF Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_dir : str\n",
    "        Directory containing NetCDF files for the variable\n",
    "    variable : str\n",
    "        Variable name (tasmax, tasmin, pr, rsds)\n",
    "    target_lat : float\n",
    "        Target latitude\n",
    "    target_lon : float\n",
    "        Target longitude\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all NetCDF files in the directory\n",
    "    # Pattern 1: Files directly in the directory matching *{variable}*.nc\n",
    "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
    "    \n",
    "    # Pattern 2: Files in subdirectories named {variable}_* (e.g., pr_ACCESS CM2 SSP245)\n",
    "    if len(nc_files) == 0:\n",
    "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
    "        for var_subdir in var_subdirs:\n",
    "            if os.path.isdir(var_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    # Pattern 3: Check subdirectory named exactly after the variable\n",
    "    if len(nc_files) == 0:\n",
    "        var_dir = os.path.join(netcdf_dir, variable)\n",
    "        if os.path.exists(var_dir) and os.path.isdir(var_dir):\n",
    "            nc_files = sorted(glob.glob(os.path.join(var_dir, \"*.nc\")))\n",
    "            if len(nc_files) > 0:\n",
    "                print(f\"  Found files in subdirectory: {variable}/\")\n",
    "    \n",
    "    if len(nc_files) == 0:\n",
    "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
    "        print(f\"  Searched patterns:\")\n",
    "        print(f\"    - {netcdf_dir}/*{variable}*.nc\")\n",
    "        print(f\"    - {netcdf_dir}/{variable}_*/*.nc\")\n",
    "        print(f\"    - {netcdf_dir}/{variable}/*.nc\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
    "    \n",
    "    # Cache coordinate information from first file\n",
    "    lat_name = None\n",
    "    lon_name = None\n",
    "    time_name = None\n",
    "    lat_idx = None\n",
    "    lon_idx = None\n",
    "    actual_lat = None\n",
    "    actual_lon = None\n",
    "    var_name = None\n",
    "    \n",
    "    # List to store daily data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process first file to get coordinate structure\n",
    "    if len(nc_files) > 0:\n",
    "        try:\n",
    "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
    "            \n",
    "            # Get variable name\n",
    "            for v in ds_sample.data_vars:\n",
    "                if variable in v.lower() or v.lower() in variable.lower():\n",
    "                    var_name = v\n",
    "                    break\n",
    "            \n",
    "            if var_name is None:\n",
    "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
    "                for name in possible_names:\n",
    "                    if name in ds_sample.data_vars:\n",
    "                        var_name = name\n",
    "                        break\n",
    "            \n",
    "            # Get coordinate names\n",
    "            for coord in ds_sample.coords:\n",
    "                coord_lower = coord.lower()\n",
    "                if 'lat' in coord_lower:\n",
    "                    lat_name = coord\n",
    "                elif 'lon' in coord_lower:\n",
    "                    lon_name = coord\n",
    "                elif 'time' in coord_lower:\n",
    "                    time_name = coord\n",
    "            \n",
    "            if lat_name and lon_name:\n",
    "                # Find nearest grid point (cache indices)\n",
    "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
    "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
    "                \n",
    "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
    "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
    "                \n",
    "                # Check if within tolerance\n",
    "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
    "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
    "                else:\n",
    "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "            \n",
    "            ds_sample.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not read sample file: {e}\")\n",
    "    \n",
    "    if var_name is None or lat_idx is None or lon_idx is None:\n",
    "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
    "        return None\n",
    "    \n",
    "    # Process all files with progress bar\n",
    "    print(f\"  Processing files...\")\n",
    "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
    "        try:\n",
    "            # Open NetCDF file with minimal decoding for speed\n",
    "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "            \n",
    "            # Extract data using cached indices\n",
    "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
    "            \n",
    "            # Convert to numpy array (load into memory)\n",
    "            values = data.values\n",
    "            if values.ndim > 1:\n",
    "                values = values.flatten()\n",
    "            \n",
    "            # Get time values - try multiple methods to ensure accuracy and handle leap years (366 days)\n",
    "            time_values = None\n",
    "            \n",
    "            # Method 1: Try to use time coordinate from NetCDF file (most reliable)\n",
    "            if time_name and time_name in ds.coords:\n",
    "                try:\n",
    "                    time_coord = ds[time_name]\n",
    "                    if len(time_coord) == len(values):\n",
    "                        # Try to decode times\n",
    "                        try:\n",
    "                            # Decode time coordinate\n",
    "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
    "                            time_values = pd.to_datetime(time_decoded.values)\n",
    "                            if len(time_values) == len(values):\n",
    "                                pass  # Success - using decoded time coordinate\n",
    "                        except:\n",
    "                            # If decoding fails, try manual conversion\n",
    "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
    "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
    "                                base_date = pd.to_datetime(base_date_str)\n",
    "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
    "                                if len(time_values) != len(values):\n",
    "                                    time_values = None\n",
    "                except Exception as e:\n",
    "                    pass  # Fall back to other methods\n",
    "            \n",
    "            # Method 2: Extract year from filename and create date range\n",
    "            # This method automatically handles leap years (366 days) correctly\n",
    "            if time_values is None:\n",
    "                year = None\n",
    "                filename = os.path.basename(nc_file)\n",
    "                all_years = re.findall(r'\\d{4}', filename)\n",
    "                for year_str in all_years:\n",
    "                    year_candidate = int(year_str)\n",
    "                    if 2000 <= year_candidate <= 2100:\n",
    "                        year = year_candidate\n",
    "                        break\n",
    "                \n",
    "                if year:\n",
    "                    # Create dates based on ACTUAL data length\n",
    "                    # pd.date_range with freq='D' automatically handles leap years\n",
    "                    # For leap years (e.g., 2024, 2028), it will include Feb 29 (366 days)\n",
    "                    # For non-leap years, it will have 365 days\n",
    "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
    "                else:\n",
    "                    # Fallback: use 2035 as default (start of typical CMIP6 data range)\n",
    "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
    "            \n",
    "            # Ensure we have the correct number of dates matching the data\n",
    "            # This handles edge cases where time coordinate might not match exactly\n",
    "            if len(time_values) != len(values):\n",
    "                if len(time_values) > len(values):\n",
    "                    time_values = time_values[:len(values)]\n",
    "                else:\n",
    "                    # Extend if needed (shouldn't happen normally, but handle it)\n",
    "                    additional_days = len(values) - len(time_values)\n",
    "                    last_date = time_values[-1]\n",
    "                    additional_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=additional_days, freq='D')\n",
    "                    time_values = pd.concat([pd.Series(time_values), pd.Series(additional_dates)]).values\n",
    "            \n",
    "            # Create DataFrame for this file\n",
    "            # Use actual data length to ensure all days are included (365 or 366 for leap years)\n",
    "            if len(values) > 0:\n",
    "                df_file = pd.DataFrame({\n",
    "                    'date': time_values[:len(values)],\n",
    "                    'value': values\n",
    "                })\n",
    "                all_data.append(df_file)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(f\"  ERROR: No data extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"  Combining data from {len(all_data)} files...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence)\n",
    "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
    "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: MET Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tav_amp(df):\n",
    "    \"\"\"\n",
    "    Calculate annual average temperature (tav) and annual amplitude (amp).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with 'date' as index and 'maxt' and 'mint' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (tav, amp)\n",
    "        tav: Annual average ambient temperature\n",
    "        amp: Annual amplitude in mean monthly temperature\n",
    "    \"\"\"\n",
    "    # Calculate daily mean temperature\n",
    "    df = df.copy()\n",
    "    df['tmean'] = (df['maxt'] + df['mint']) / 2.0\n",
    "    \n",
    "    # Calculate monthly means\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    monthly_means = df.groupby(['year', 'month'])['tmean'].mean()\n",
    "    \n",
    "    # Calculate overall annual average (tav)\n",
    "    tav = df['tmean'].mean()\n",
    "    \n",
    "    # Calculate annual amplitude (amp)\n",
    "    # Average of all January means minus average of all July means, divided by 2\n",
    "    jan_means = monthly_means[monthly_means.index.get_level_values('month') == 1].mean()\n",
    "    jul_means = monthly_means[monthly_means.index.get_level_values('month') == 7].mean()\n",
    "    amp = (jan_means - jul_means) / 2.0\n",
    "    \n",
    "    return tav, amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hurs_to_vp(hurs_df, tasmax_df, tasmin_df):\n",
    "    \"\"\"\n",
    "    Convert relative humidity (%) to vapor pressure (hPa).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hurs_df : pd.DataFrame\n",
    "        DataFrame with date and value (relative humidity %) columns\n",
    "    tasmax_df : pd.DataFrame\n",
    "        DataFrame with date and value (maximum temperature °C) columns\n",
    "    tasmin_df : pd.DataFrame\n",
    "        DataFrame with date and value (minimum temperature °C) columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with date and value (vapor pressure hPa) columns\n",
    "    \"\"\"\n",
    "    # Merge temperature dataframes\n",
    "    temp_df = tasmax_df.merge(tasmin_df, on='date', suffixes=('_max', '_min'))\n",
    "    temp_df['tmean'] = (temp_df['value_max'] + temp_df['value_min']) / 2.0\n",
    "    \n",
    "    # Merge with humidity\n",
    "    merged = hurs_df.merge(temp_df[['date', 'tmean']], on='date')\n",
    "    \n",
    "    # Convert relative humidity (%) to vapor pressure (hPa)\n",
    "    # Using Magnus formula: e = (RH/100) * 0.611 * exp((17.27 * T) / (T + 237.3))\n",
    "    # where T is temperature in Celsius\n",
    "    merged['vp'] = (merged['value'] / 100.0) * 0.611 * np.exp((17.27 * merged['tmean']) / (merged['tmean'] + 237.3))\n",
    "    \n",
    "    # Return DataFrame with date and vp columns\n",
    "    vp_df = merged[['date', 'vp']].copy()\n",
    "    vp_df = vp_df.rename(columns={'vp': 'value'})\n",
    "    \n",
    "    return vp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_met_file(tasmax_df, tasmin_df, pr_df, rsds_df=None, scenario=None, \n",
    "                    output_dir=None, latitude=None, longitude=None, model=None):\n",
    "    \"\"\"\n",
    "    Create MET format file from tasmax, tasmin, pr, and optional rsds/hurs DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tasmax_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for maximum temperature\n",
    "    tasmin_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for minimum temperature\n",
    "    pr_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for precipitation\n",
    "    rsds_df : pd.DataFrame, optional\n",
    "        DataFrame with date and value columns for surface downwelling shortwave radiation (W/m²)\n",
    "    scenario : str\n",
    "        Scenario name (e.g., SSP585 or SSP245)\n",
    "    output_dir : str\n",
    "        Output directory path\n",
    "    latitude : float\n",
    "        Latitude in decimal degrees\n",
    "    longitude : float\n",
    "        Longitude in decimal degrees\n",
    "    model : str\n",
    "        Model name (e.g., \"ACCESS CM2\")\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (tav, amp, num_rows)\n",
    "    \"\"\"\n",
    "    # Merge all dataframes on date\n",
    "    merged = tasmax_df.copy()\n",
    "    merged = merged.rename(columns={'value': 'maxt'})\n",
    "    merged['date'] = pd.to_datetime(merged['date'])\n",
    "    \n",
    "    # Merge tasmin\n",
    "    tasmin_df['date'] = pd.to_datetime(tasmin_df['date'])\n",
    "    merged = merged.merge(tasmin_df[['date', 'value']], on='date', how='outer')\n",
    "    merged = merged.rename(columns={'value': 'mint'})\n",
    "    \n",
    "    # Merge pr (precipitation/rain)\n",
    "    pr_df['date'] = pd.to_datetime(pr_df['date'])\n",
    "    merged = merged.merge(pr_df[['date', 'value']], on='date', how='outer')\n",
    "    merged = merged.rename(columns={'value': 'rain'})\n",
    "    \n",
    "    # Merge rsds (radiation) if available\n",
    "    # rsds is in W/m², convert to MJ/m² by multiplying by 0.0864 (seconds per day / 1e6)\n",
    "    if rsds_df is not None and len(rsds_df) > 0:\n",
    "        rsds_df['date'] = pd.to_datetime(rsds_df['date'])\n",
    "        # Convert W/m² to MJ/m² (multiply by seconds per day / 1e6)\n",
    "        rsds_df['value_mj'] = rsds_df['value'] * 0.0864\n",
    "        merged = merged.merge(rsds_df[['date', 'value_mj']], on='date', how='outer')\n",
    "        merged = merged.rename(columns={'value_mj': 'radn'})\n",
    "    else:\n",
    "        merged['radn'] = ''\n",
    "    \n",
    "    # vp (vapor pressure) is left blank\n",
    "    merged['vp'] = ''\n",
    "    \n",
    "    # Sort by date\n",
    "    merged = merged.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Calculate tav and amp\n",
    "    merged_temp = merged[['date', 'maxt', 'mint']].copy()\n",
    "    merged_temp = merged_temp.set_index('date')\n",
    "    merged_temp.index = pd.to_datetime(merged_temp.index)\n",
    "    tav, amp = calculate_tav_amp(merged_temp)\n",
    "    \n",
    "    # Create year and day columns\n",
    "    merged['year'] = merged['date'].dt.year\n",
    "    merged['day'] = merged['date'].dt.dayofyear\n",
    "    \n",
    "    # Add empty columns for evap and code\n",
    "    merged['evap'] = ''  # Leave blank\n",
    "    merged['code'] = ''  # Leave blank\n",
    "    \n",
    "    # Ensure radn and vp are strings if empty\n",
    "    if 'radn' not in merged.columns:\n",
    "        merged['radn'] = ''\n",
    "    if 'vp' not in merged.columns:\n",
    "        merged['vp'] = ''\n",
    "    \n",
    "    # Select and order columns for MET format\n",
    "    met_data = merged[['year', 'day', 'radn', 'maxt', 'mint', 'rain', 'evap', 'vp', 'code']].copy()\n",
    "    \n",
    "    # Prepare header\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    model_scenario = f\"{model.replace(' ', '_')}_{scenario}\" if model and scenario else \"CMIP6\"\n",
    "    \n",
    "    header = f\"\"\"[weather.met.weather]\n",
    "!Your Ref:  \"\n",
    "latitude = {latitude:.2f}  (DECIMAL DEGREES)\n",
    "longitude =  {longitude:.2f}  (DECIMAL DEGREES)\n",
    "tav = {tav:.2f} (oC) ! Annual average ambient temperature.\n",
    "amp = {amp:.2f} (oC) ! Annual amplitude in mean monthly temperature.\n",
    "!Data Extracted from CMIP6 {model} {scenario} dataset on {current_date} for APSIM\n",
    "!As evaporation is read at 9am, it has been shifted to day before\n",
    "!ie The evaporation measured on 20 April is in row for 19 April\n",
    "!The 6 digit code indicates the source of the 6 data columns\n",
    "!0 actual observation, 1 actual observation composite station\n",
    "!2 interpolated from daily observations\n",
    "!3 interpolated from daily observations using anomaly interpolation method for CLIMARC data\n",
    "!6 synthetic pan\n",
    "!7 interpolated long term averages\n",
    "!more detailed two digit codes are available in SILO's 'Standard' format files\n",
    "!\n",
    "!For further information see the documentation on the datadrill\n",
    "!  http://www.longpaddock.qld.gov.au/silo\n",
    "!\n",
    "year  day radn  maxt   mint  rain  evap    vp   code\n",
    " ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\n",
    "\"\"\"\n",
    "    \n",
    "    # Create output filename with coordinate-based naming\n",
    "    # Format: {Model}_{Scenario}_{Lat}_{Lon}.met (e.g., ACCESS_CM2_SSP245_-31.75_117.60.met)\n",
    "    lat_str = f\"{latitude:.2f}\"\n",
    "    lon_str = f\"{longitude:.2f}\"\n",
    "    output_filename = f\"{model_scenario}_{lat_str}_{lon_str}.met\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Write MET file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(header)\n",
    "        # Write data rows\n",
    "        for _, row in met_data.iterrows():\n",
    "            # Format the row with proper spacing\n",
    "            radn_val = row['radn'] if row['radn'] != '' and pd.notna(row['radn']) else ''\n",
    "            evap_val = row['evap'] if row['evap'] != '' and pd.notna(row['evap']) else ''\n",
    "            vp_val = row['vp'] if row['vp'] != '' and pd.notna(row['vp']) else ''\n",
    "            code_val = row['code'] if row['code'] != '' and pd.notna(row['code']) else ''\n",
    "            \n",
    "            # Format numbers with proper spacing\n",
    "            if radn_val != '':\n",
    "                radn_str = f\"{float(radn_val):6.1f}\"\n",
    "            else:\n",
    "                radn_str = \"      \"  # 6 spaces\n",
    "                \n",
    "            if evap_val != '':\n",
    "                evap_str = f\"{float(evap_val):6.1f}\"\n",
    "            else:\n",
    "                evap_str = \"      \"  # 6 spaces\n",
    "                \n",
    "            if vp_val != '':\n",
    "                vp_str = f\"{float(vp_val):6.1f}\"\n",
    "            else:\n",
    "                vp_str = \"      \"  # 6 spaces\n",
    "                \n",
    "            if code_val != '':\n",
    "                code_str = f\"{str(code_val):>6s}\"\n",
    "            else:\n",
    "                code_str = \"      \"  # 6 spaces\n",
    "            \n",
    "            # Handle NaN values for maxt, mint, rain - use 0.0 as default\n",
    "            maxt_val = row['maxt'] if pd.notna(row['maxt']) else 0.0\n",
    "            mint_val = row['mint'] if pd.notna(row['mint']) else 0.0\n",
    "            rain_val = row['rain'] if pd.notna(row['rain']) else 0.0\n",
    "            \n",
    "            # Format with proper column widths\n",
    "            line = f\"{int(row['year']):4d} {int(row['day']):4d} {radn_str} {maxt_val:6.1f} {mint_val:6.1f} {rain_val:6.1f} {evap_str} {vp_str} {code_str}\\n\"\n",
    "            f.write(line)\n",
    "    \n",
    "    print(f\"  [OK] Created MET file: {output_filename}\")\n",
    "    \n",
    "    # Also create CSV version with same structure\n",
    "    csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}.csv\"\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "    \n",
    "    # Write CSV (without header comments, just data)\n",
    "    met_data.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.1f')\n",
    "    print(f\"  [OK] Created CSV file: {csv_filename}\")\n",
    "    \n",
    "    return tav, amp, len(met_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Main Processing\n",
    "\n",
    "This section extracts data for all variables and creates the MET file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coordinate(model, scenario, latitude, longitude, variables, cmip6_base_dir, output_dir, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Main processing function for user-provided coordinate.\n",
    "    Extract all variables from NC files and convert to MET format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name (e.g., \"ACCESS CM2\")\n",
    "    scenario : str\n",
    "        Scenario name (e.g., \"SSP245\")\n",
    "    latitude : float\n",
    "        Target latitude\n",
    "    longitude : float\n",
    "        Target longitude\n",
    "    variables : list\n",
    "        List of variable names to extract\n",
    "    cmip6_base_dir : str\n",
    "        Base directory containing Model Scenario folders\n",
    "    output_dir : str\n",
    "        Output directory for results\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Summary statistics\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Processing Coordinate: ({latitude:.6f}, {longitude:.6f})\")\n",
    "    print(f\"Model: {model}, Scenario: {scenario}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Construct data directory path\n",
    "    data_dir = os.path.join(cmip6_base_dir, f\"{model} {scenario}\")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"ERROR: Data directory not found: {data_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nData directory: {data_dir}\")\n",
    "    \n",
    "    # Extract data for all variables\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for variable in variables:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing variable: {variable}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Extract data from NetCDF files\n",
    "        df = extract_daily_data_from_netcdf(\n",
    "            data_dir, \n",
    "            variable, \n",
    "            latitude, \n",
    "            longitude, \n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            extracted_data[variable] = df\n",
    "            \n",
    "            # Save individual variable CSV\n",
    "            # Format: {Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv\n",
    "            lat_str = f\"{latitude:.2f}\"\n",
    "            lon_str = f\"{longitude:.2f}\"\n",
    "            model_scenario = f\"{model.replace(' ', '_')}_{scenario}\"\n",
    "            csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
    "            csv_path = os.path.join(output_dir, csv_filename)\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.6f')\n",
    "            print(f\"  [OK] Saved CSV: {csv_filename}\")\n",
    "        else:\n",
    "            print(f\"  WARNING: No data extracted for {variable}\")\n",
    "    \n",
    "    # Check if required variables are available for MET conversion\n",
    "    required_vars = ['tasmax', 'tasmin', 'pr']\n",
    "    missing_vars = [v for v in required_vars if v not in extracted_data]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"\\nERROR: Missing required variables for MET conversion: {missing_vars}\")\n",
    "        return None\n",
    "    \n",
    "    # Create MET file\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Creating MET file...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get required variables\n",
    "    tasmax_df = extracted_data['tasmax']\n",
    "    tasmin_df = extracted_data['tasmin']\n",
    "    pr_df = extracted_data['pr']\n",
    "    \n",
    "    # Get optional variable for MET format\n",
    "    rsds_df = extracted_data.get('rsds', None)\n",
    "    \n",
    "    # Note: vp and code are left blank in MET format\n",
    "    \n",
    "    tav, amp, num_rows = create_met_file(\n",
    "        tasmax_df=tasmax_df,\n",
    "        tasmin_df=tasmin_df,\n",
    "        pr_df=pr_df,\n",
    "        rsds_df=rsds_df,\n",
    "        scenario=scenario,\n",
    "        output_dir=output_dir,\n",
    "        latitude=latitude,\n",
    "        longitude=longitude,\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    summary = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'model': model,\n",
    "        'scenario': scenario,\n",
    "        'variables_extracted': list(extracted_data.keys()),\n",
    "        'num_variables': len(extracted_data),\n",
    "        'tav': tav,\n",
    "        'amp': amp,\n",
    "        'num_rows': num_rows,\n",
    "        'date_range': {\n",
    "            'start': tasmax_df['date'].min(),\n",
    "            'end': tasmax_df['date'].max()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Processing Summary\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Variables extracted: {len(extracted_data)}\")\n",
    "    print(f\"    - {', '.join(extracted_data.keys())}\")\n",
    "    print(f\"  MET file rows: {num_rows}\")\n",
    "    print(f\"  Date range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
    "    print(f\"  tav (annual average temp): {tav:.2f} °C\")\n",
    "    print(f\"  amp (annual amplitude): {amp:.2f} °C\")\n",
    "    print(f\"  Output directory: {output_dir}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Summary\n",
    "\n",
    "Review the configuration below before running the main processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full configuration\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: {MODEL}\")\n",
    "print(f\"Scenario: {SCENARIO}\")\n",
    "print(f\"\\nCoordinates:\")\n",
    "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
    "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
    "print(f\"  Tolerance: {COORD_TOLERANCE} degrees (≈ {COORD_TOLERANCE * 111:.1f} km)\")\n",
    "print(f\"\\nVariables to process ({len(VARIABLES)}):\")\n",
    "for var in VARIABLES:\n",
    "    if var == 'tasmax':\n",
    "        print(f\"  - {var} → maxt (maximum temperature)\")\n",
    "    elif var == 'tasmin':\n",
    "        print(f\"  - {var} → mint (minimum temperature)\")\n",
    "    elif var == 'pr':\n",
    "        print(f\"  - {var} → rain (precipitation)\")\n",
    "    elif var == 'rsds':\n",
    "        print(f\"  - {var} → radn (radiation, W/m² → MJ/m²)\")\n",
    "print(f\"\\nDirectories:\")\n",
    "print(f\"  CMIP6 Base: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  Data Directory: {os.path.join(CMIP6_BASE_DIR, f'{MODEL} {SCENARIO}')}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
    "print(f\"  CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
    "print(f\"  Variable CSVs: {len(VARIABLES)} files (one per variable)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute main processing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Scenario: {SCENARIO}\")\n",
    "print(f\"Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"Variables to process: {len(VARIABLES)} ({', '.join(VARIABLES)})\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "summary = process_coordinate(\n",
    "    model=MODEL,\n",
    "    scenario=SCENARIO,\n",
    "    latitude=LATITUDE,\n",
    "    longitude=LONGITUDE,\n",
    "    variables=VARIABLES,\n",
    "    cmip6_base_dir=CMIP6_BASE_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    tolerance=COORD_TOLERANCE\n",
    ")\n",
    "\n",
    "if summary:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nOutput files created:\")\n",
    "    print(f\"  - MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
    "    print(f\"  - CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
    "    print(f\"  - Individual variable CSVs: {len(summary['variables_extracted'])} files\")\n",
    "    print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✗ PROCESSING FAILED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Please check error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Documentation\n",
    "\n",
    "### CMIP6 Data Structure and Conventions\n",
    "\n",
    "CMIP6 (Coupled Model Intercomparison Project Phase 6) climate data is organized by:\n",
    "- **Model**: Climate model name (e.g., \"ACCESS CM2\")\n",
    "- **Scenario**: Shared Socioeconomic Pathway (e.g., \"SSP245\", \"SSP585\")\n",
    "- **Variable**: Climate variable name (e.g., \"tasmax\", \"pr\")\n",
    "\n",
    "Data files are stored in NetCDF format (.nc) and organized in folders:\n",
    "`C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\{variable}_*\\*.nc`\n",
    "\n",
    "### Coordinate System\n",
    "\n",
    "Coordinates are provided in decimal degrees:\n",
    "- **Latitude**: -90 to 90 (negative for Southern Hemisphere)\n",
    "- **Longitude**: -180 to 180 (negative for Western Hemisphere)\n",
    "\n",
    "The notebook finds the nearest grid point within a tolerance (default: 0.01 degrees ≈ 1.1 km).\n",
    "\n",
    "### Variable Extraction Workflow\n",
    "\n",
    "1. For each variable, search for matching NetCDF files\n",
    "2. Find nearest grid point to user-specified coordinates\n",
    "3. Extract time series data from all files\n",
    "4. Extract dates using time coordinate from NetCDF (if available) or filename\n",
    "5. **Leap year handling**: Automatically reads all 366 days for leap years (e.g., 2024, 2028)\n",
    "6. Combine and sort by date\n",
    "7. Save as individual CSV files\n",
    "\n",
    "### MET Format Specifications\n",
    "\n",
    "APSIM MET format requires:\n",
    "- **Header**: Metadata including latitude, longitude, tav, amp\n",
    "- **Data columns**: year, day, radn, maxt, mint, rain, evap, vp, code\n",
    "- **Required data**: maxt (from tasmax), mint (from tasmin), rain (from pr)\n",
    "- **Optional data**: radn (from rsds, converted from W/m² to MJ/m²)\n",
    "- **Blank fields**: vp and code are left blank\n",
    "\n",
    "### Unit Conversions\n",
    "\n",
    "- **rsds** (W/m²) → **radn** (MJ/m²): Multiply by 0.0864\n",
    "- **vp** and **code**: Left blank in MET format\n",
    "\n",
    "### File Naming Conventions\n",
    "\n",
    "All output files use coordinate-based naming:\n",
    "- MET files: `{Model}_{Scenario}_{Lat}_{Lon}.met`\n",
    "- CSV files: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv`\n",
    "\n",
    "Example: `ACCESS_CM2_SSP245_-31.75_117.60.met`\n",
    "\n",
    "### Model and Scenario Selection\n",
    "\n",
    "Update the `MODEL` and `SCENARIO` variables in Section 1 to match your data folder structure.\n",
    "\n",
    "Common scenarios:\n",
    "- **SSP245**: Middle-of-the-road scenario\n",
    "- **SSP585**: High-emissions scenario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
