{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Point CMIP6 Retrieval and MET Conversion Notebook\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook combines CMIP6 NetCDF data extraction with APSIM MET format conversion. It processes climate variables from NC files organized in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}` folders and converts them to MET format for APSIM simulations.\n",
    "\n",
    "## File Structure\n",
    "\n",
    "- **Input**: NC files in `C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\` (e.g., `ACCESS CM2 SSP245`)\n",
    "- **User Input**: Latitude and longitude coordinates (decimal degrees)\n",
    "- **Output**: MET files and CSV files for the specified coordinate\n",
    "\n",
    "## Variables Processed\n",
    "\n",
    "The notebook processes 4 climate variables:\n",
    "\n",
    "- **tasmax**: Daily maximum temperature (°C) → **maxt** in MET format\n",
    "- **tasmin**: Daily minimum temperature (°C) → **mint** in MET format\n",
    "- **pr**: Daily precipitation (mm) → **rain** in MET format\n",
    "- **rsds**: Daily surface downwelling shortwave radiation (W/m²) → **radn** (MJ/m²) in MET format\n",
    "\n",
    "**Note**: vp (vapor pressure) and code fields are left blank in the MET file\n",
    "\n",
    "## MET Format Specifications\n",
    "\n",
    "The MET format is used by APSIM for weather data input. It includes:\n",
    "\n",
    "- **Required fields**: year, day, maxt (from tasmax), mint (from tasmin), rain (from pr), radn (from rsds)\n",
    "- **radn field**: Required, converted from rsds (W/m² to MJ/m²)\n",
    "- **Blank fields**: evap (evaporation), vp (vapor pressure), code (data quality code) - left blank\n",
    "- **Metadata**: latitude, longitude, tav (annual average temperature), amp (annual amplitude)\n",
    "\n",
    "## File Naming Conventions\n",
    "\n",
    "- **Input NC files**: `{Model} {Scenario}\\*{variable}*.nc` (e.g., `ACCESS CM2 SSP245\\tasmax*.nc`)\n",
    "- **Output MET files**: `{Model}_{Scenario}_{Lat}_{Lon}.met` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60.met`)\n",
    "- **Output CSV files**: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv` (e.g., `ACCESS_CM2_SSP245_-31.75_117.60_tasmax.csv`)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "1. Set the configuration parameters (Model, Scenario, output directory) in Section 1\n",
    "2. Provide your target latitude and longitude coordinates in Section 2\n",
    "3. Run all cells sequentially to extract data and create MET files\n",
    "4. Output files will be saved with coordinate-based naming\n",
    "\n",
    "## Coordinate Matching\n",
    "\n",
    "The notebook finds the nearest grid point to your specified coordinates within a tolerance (default: 0.01 degrees ≈ 1.1 km). If the nearest point is outside tolerance, a warning will be displayed.\n",
    "\n",
    "## Notes\n",
    "\n",
    "- **CMIP6 data structure**: Data is organized by Model and Scenario in separate folders\n",
    "- **Coordinate format**: Provide coordinates in decimal degrees (latitude: -90 to 90, longitude: -180 to 180)\n",
    "- **Variable extraction**: 4 variables (tasmax, tasmin, pr, rsds) are extracted from NC files and saved as individual CSVs\n",
    "- **MET conversion**: tasmax→maxt, tasmin→mint, pr→rain, rsds→radn\n",
    "- **Unit conversions**: rsds (W/m²) is converted to radn (MJ/m²) by multiplying by 0.0864\n",
    "- **Blank fields**: vp and code are left blank in MET format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.198916Z",
     "iopub.status.busy": "2025-12-21T10:28:42.198764Z",
     "iopub.status.idle": "2025-12-21T10:28:42.528559Z",
     "shell.execute_reply": "2025-12-21T10:28:42.528131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.547981Z",
     "iopub.status.busy": "2025-12-21T10:28:42.547706Z",
     "iopub.status.idle": "2025-12-21T10:28:42.551487Z",
     "shell.execute_reply": "2025-12-21T10:28:42.550859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  - CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
      "  - Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n",
      "  - Model: ACCESS CM2\n",
      "  - Scenario: obs\n",
      "  - Date Range: 1986 to 2014\n",
      "  - Coordinate Tolerance: 0.01 degrees\n",
      "  - Variables: tasmax, tasmin, pr, rsds, hurs\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\"  # Output directory for MET files\n",
    "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
    "\n",
    "# Model and Scenario - UPDATE THESE AS NEEDED\n",
    "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
    "SCENARIO = \"obs\"   # e.g., \"obs\", \"SSP245\", or \"SSP585\"\n",
    "\n",
    "# Date Range - UPDATE THESE AS NEEDED\n",
    "START_YEAR = 1986   # Start year for MET file (e.g., 1985 for obs, 2035 for future scenarios, or None for all available data)\n",
    "END_YEAR = 2014     # End year for MET file (e.g., 2014 for obs, 2064 for future, or None for all available data)\n",
    "\n",
    "# Variables to process (5 variables)\n",
    "# MET format mapping:\n",
    "# - tasmax → maxt (maximum temperature)\n",
    "# - tasmin → mint (minimum temperature)\n",
    "# - pr → rain (precipitation)\n",
    "# - rsds → radn (radiation, converted from W/m² to MJ/m²)\n",
    "# - hurs → vp (vapor pressure, calculated using SILO method)\n",
    "# Note: code is hardcoded to '222222'\n",
    "VARIABLES = ['tasmax', 'tasmin', 'pr', 'rsds', 'hurs']\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model: {MODEL}\")\n",
    "print(f\"  - Scenario: {SCENARIO}\")\n",
    "print(f\"  - Date Range: {START_YEAR if START_YEAR is not None else 'start of data'} to {END_YEAR if END_YEAR is not None else 'end of data'}\")\n",
    "print(f\"  - Coordinate Tolerance: {COORD_TOLERANCE} degrees\")\n",
    "print(f\"  - Variables: {', '.join(VARIABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: User Coordinate Input\n",
    "\n",
    "Provide the latitude and longitude coordinates for the grid point you want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.553077Z",
     "iopub.status.busy": "2025-12-21T10:28:42.552944Z",
     "iopub.status.idle": "2025-12-21T10:28:42.556150Z",
     "shell.execute_reply": "2025-12-21T10:28:42.555562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Coordinate:\n",
      "  Latitude: -31.450000°\n",
      "  Longitude: 117.550000°\n",
      "  Model: ACCESS CM2\n",
      "  Scenario: obs\n",
      "  Tolerance: 0.01 degrees\n"
     ]
    }
   ],
   "source": [
    "# USER INPUT: Provide your target coordinates here\n",
    "LATITUDE = -31.45   # Target latitude in decimal degrees (-90 to 90)\n",
    "LONGITUDE = 117.55  # Target longitude in decimal degrees (-180 to 180)\n",
    "\n",
    "# Validate coordinates\n",
    "if not (-90 <= LATITUDE <= 90):\n",
    "    raise ValueError(f\"Latitude must be between -90 and 90. Provided: {LATITUDE}\")\n",
    "if not (-180 <= LONGITUDE <= 180):\n",
    "    raise ValueError(f\"Longitude must be between -180 and 180. Provided: {LONGITUDE}\")\n",
    "\n",
    "print(f\"Target Coordinate:\")\n",
    "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
    "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
    "print(f\"  Model: {MODEL}\")\n",
    "print(f\"  Scenario: {SCENARIO}\")\n",
    "print(f\"  Tolerance: {COORD_TOLERANCE} degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: NetCDF Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.558014Z",
     "iopub.status.busy": "2025-12-21T10:28:42.557889Z",
     "iopub.status.idle": "2025-12-21T10:28:42.569492Z",
     "shell.execute_reply": "2025-12-21T10:28:42.569072Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_dir : str\n",
    "        Directory containing NetCDF files for the variable\n",
    "    variable : str\n",
    "        Variable name (tasmax, tasmin, pr, rsds)\n",
    "    target_lat : float\n",
    "        Target latitude\n",
    "    target_lon : float\n",
    "        Target longitude\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all NetCDF files in the directory\n",
    "    # Pattern 1: Files directly in the directory matching *{variable}*.nc\n",
    "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
    "    \n",
    "    # Pattern 2: Files in subdirectories named {variable}_* (e.g., pr_ACCESS CM2 SSP245)\n",
    "    if len(nc_files) == 0:\n",
    "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
    "        for var_subdir in var_subdirs:\n",
    "            if os.path.isdir(var_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    # Pattern 2b: For rsds, also check for \"rad_\" folder (folder named \"rad_\" but files contain \"rsds\")\n",
    "    # Example: rad_ACCESS CM2 SSP245/ contains files named *rsds*.nc\n",
    "    if len(nc_files) == 0 and variable == 'rsds':\n",
    "        rad_subdirs = glob.glob(os.path.join(netcdf_dir, \"rad_*\"))\n",
    "        for rad_subdir in rad_subdirs:\n",
    "            if os.path.isdir(rad_subdir):\n",
    "                # Search for files containing \"rsds\" in the rad_ folder\n",
    "                found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*rsds*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
    "                    break\n",
    "                # Fallback: if no rsds files found, try all .nc files\n",
    "                if len(nc_files) == 0:\n",
    "                    found_files = sorted(glob.glob(os.path.join(rad_subdir, \"*.nc\")))\n",
    "                    if found_files:\n",
    "                        nc_files.extend(found_files)\n",
    "                        print(f\"  Found files in subdirectory: {os.path.basename(rad_subdir)}/\")\n",
    "                        break\n",
    "    \n",
    "    # Pattern 3: Check subdirectory named exactly after the variable\n",
    "    if len(nc_files) == 0:\n",
    "        var_dir = os.path.join(netcdf_dir, variable)\n",
    "        if os.path.exists(var_dir) and os.path.isdir(var_dir):\n",
    "            nc_files = sorted(glob.glob(os.path.join(var_dir, \"*.nc\")))\n",
    "            if len(nc_files) > 0:\n",
    "                print(f\"  Found files in subdirectory: {variable}/\")\n",
    "    \n",
    "    if len(nc_files) == 0:\n",
    "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
    "        print(f\"  Searched patterns:\")\n",
    "        print(f\"    - {netcdf_dir}/*{variable}*.nc\")\n",
    "        print(f\"    - {netcdf_dir}/{variable}_*/*.nc\")\n",
    "        if variable == 'rsds':\n",
    "            print(f\"    - {netcdf_dir}/rad_*/*rsds*.nc\")\n",
    "            print(f\"    - {netcdf_dir}/rad_*/*.nc\")\n",
    "        print(f\"    - {netcdf_dir}/{variable}/*.nc\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
    "    \n",
    "    # Cache coordinate information from first file\n",
    "    lat_name = None\n",
    "    lon_name = None\n",
    "    time_name = None\n",
    "    lat_idx = None\n",
    "    lon_idx = None\n",
    "    actual_lat = None\n",
    "    actual_lon = None\n",
    "    var_name = None\n",
    "    \n",
    "    # List to store daily data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process first file to get coordinate structure\n",
    "    if len(nc_files) > 0:\n",
    "        try:\n",
    "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
    "            \n",
    "            # Get variable name\n",
    "            for v in ds_sample.data_vars:\n",
    "                if variable in v.lower() or v.lower() in variable.lower():\n",
    "                    var_name = v\n",
    "                    break\n",
    "            \n",
    "            # For rsds, also check for \"rad\" variable name (some datasets use \"rad\" instead of \"rsds\")\n",
    "            if var_name is None and variable == 'rsds':\n",
    "                for v in ds_sample.data_vars:\n",
    "                    if 'rad' in v.lower() and 'rsds' not in v.lower():\n",
    "                        var_name = v\n",
    "                        break\n",
    "            \n",
    "            if var_name is None:\n",
    "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
    "                # For rsds, also try \"rad\" as variable name\n",
    "                if variable == 'rsds':\n",
    "                    possible_names.extend(['rad', 'RAD', 'rad_day'])\n",
    "                for name in possible_names:\n",
    "                    if name in ds_sample.data_vars:\n",
    "                        var_name = name\n",
    "                        break\n",
    "            \n",
    "            # Get coordinate names\n",
    "            for coord in ds_sample.coords:\n",
    "                coord_lower = coord.lower()\n",
    "                if 'lat' in coord_lower:\n",
    "                    lat_name = coord\n",
    "                elif 'lon' in coord_lower:\n",
    "                    lon_name = coord\n",
    "                elif 'time' in coord_lower:\n",
    "                    time_name = coord\n",
    "            \n",
    "            if lat_name and lon_name:\n",
    "                # Check coordinate bounds of the NetCDF file\n",
    "                lat_values = ds_sample[lat_name].values\n",
    "                lon_values = ds_sample[lon_name].values\n",
    "                \n",
    "                lat_min = float(np.min(lat_values))\n",
    "                lat_max = float(np.max(lat_values))\n",
    "                lon_min = float(np.min(lon_values))\n",
    "                lon_max = float(np.max(lon_values))\n",
    "                \n",
    "                # Check if target coordinate is within file bounds\n",
    "                lat_in_bounds = lat_min <= target_lat <= lat_max\n",
    "                lon_in_bounds = lon_min <= target_lon <= lon_max\n",
    "                \n",
    "                print(f\"  Grid bounds: Lat [{lat_min:.4f}, {lat_max:.4f}], Lon [{lon_min:.4f}, {lon_max:.4f}]\")\n",
    "                print(f\"  Target coordinate: ({target_lat:.4f}, {target_lon:.4f})\")\n",
    "                \n",
    "                if not lat_in_bounds:\n",
    "                    print(f\"  [WARNING] Target latitude {target_lat:.4f} is OUTSIDE file bounds [{lat_min:.4f}, {lat_max:.4f}]\")\n",
    "                    print(f\"  [WARNING] This may cause all values to be zero or incorrect!\")\n",
    "                if not lon_in_bounds:\n",
    "                    print(f\"  [WARNING] Target longitude {target_lon:.4f} is OUTSIDE file bounds [{lon_min:.4f}, {lon_max:.4f}]\")\n",
    "                    print(f\"  [WARNING] This may cause all values to be zero or incorrect!\")\n",
    "                \n",
    "                if lat_in_bounds and lon_in_bounds:\n",
    "                    print(f\"  [OK] Target coordinate is within file bounds\")\n",
    "                \n",
    "                # Find nearest grid point (cache indices)\n",
    "                lat_idx = np.abs(lat_values - target_lat).argmin()\n",
    "                lon_idx = np.abs(lon_values - target_lon).argmin()\n",
    "                \n",
    "                actual_lat = float(lat_values[lat_idx])\n",
    "                actual_lon = float(lon_values[lon_idx])\n",
    "                \n",
    "                # Check if within tolerance\n",
    "                lat_diff = abs(actual_lat - target_lat)\n",
    "                lon_diff = abs(actual_lon - target_lon)\n",
    "                \n",
    "                if lat_diff > tolerance or lon_diff > tolerance:\n",
    "                    print(f\"  [WARNING] Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
    "                    print(f\"  [WARNING] Distance: {lat_diff:.4f}° lat, {lon_diff:.4f}° lon (tolerance: {tolerance:.4f}°)\")\n",
    "                else:\n",
    "                    print(f\"  [OK] Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "                    print(f\"  [OK] Distance from target: {lat_diff:.4f}° lat, {lon_diff:.4f}° lon\")\n",
    "            \n",
    "            ds_sample.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not read sample file: {e}\")\n",
    "    \n",
    "    if var_name is None or lat_idx is None or lon_idx is None:\n",
    "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
    "        return None\n",
    "    \n",
    "    # Process all files with progress bar\n",
    "    print(f\"  Processing files...\")\n",
    "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
    "        try:\n",
    "            # Open NetCDF file with minimal decoding for speed\n",
    "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "            \n",
    "            # Extract data using cached indices\n",
    "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
    "            \n",
    "            # Convert to numpy array (load into memory)\n",
    "            values = data.values\n",
    "            if values.ndim > 1:\n",
    "                values = values.flatten()\n",
    "            \n",
    "            # Get time values - try multiple methods to ensure accuracy and handle leap years (366 days)\n",
    "            time_values = None\n",
    "            \n",
    "            # Method 1: Try to use time coordinate from NetCDF file (most reliable)\n",
    "            if time_name and time_name in ds.coords:\n",
    "                try:\n",
    "                    time_coord = ds[time_name]\n",
    "                    if len(time_coord) == len(values):\n",
    "                        # Try to decode times\n",
    "                        try:\n",
    "                            # Decode time coordinate\n",
    "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
    "                            time_values = pd.to_datetime(time_decoded.values)\n",
    "                            if len(time_values) == len(values):\n",
    "                                pass  # Success - using decoded time coordinate\n",
    "                        except:\n",
    "                            # If decoding fails, try manual conversion\n",
    "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
    "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
    "                                base_date = pd.to_datetime(base_date_str)\n",
    "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
    "                                if len(time_values) != len(values):\n",
    "                                    time_values = None\n",
    "                except Exception as e:\n",
    "                    pass  # Fall back to other methods\n",
    "            \n",
    "            # Method 2: Extract year from filename and create date range\n",
    "            # This method automatically handles leap years (366 days) correctly\n",
    "            if time_values is None:\n",
    "                year = None\n",
    "                filename = os.path.basename(nc_file)\n",
    "                all_years = re.findall(r'\\d{4}', filename)\n",
    "                for year_str in all_years:\n",
    "                    year_candidate = int(year_str)\n",
    "                    if 2000 <= year_candidate <= 2100:\n",
    "                        year = year_candidate\n",
    "                        break\n",
    "                \n",
    "                if year:\n",
    "                    # Create dates based on ACTUAL data length\n",
    "                    # pd.date_range with freq='D' automatically handles leap years\n",
    "                    # For leap years (e.g., 2024, 2028), it will include Feb 29 (366 days)\n",
    "                    # For non-leap years, it will have 365 days\n",
    "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
    "                else:\n",
    "                    # Fallback: use 2035 as default (start of typical CMIP6 data range)\n",
    "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
    "            \n",
    "            # Ensure we have the correct number of dates matching the data\n",
    "            # This handles edge cases where time coordinate might not match exactly\n",
    "            if len(time_values) != len(values):\n",
    "                if len(time_values) > len(values):\n",
    "                    time_values = time_values[:len(values)]\n",
    "                else:\n",
    "                    # Extend if needed (shouldn't happen normally, but handle it)\n",
    "                    additional_days = len(values) - len(time_values)\n",
    "                    last_date = time_values[-1]\n",
    "                    additional_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=additional_days, freq='D')\n",
    "                    time_values = pd.concat([pd.Series(time_values), pd.Series(additional_dates)]).values\n",
    "            \n",
    "            # Create DataFrame for this file\n",
    "            # Use actual data length to ensure all days are included (365 or 366 for leap years)\n",
    "            if len(values) > 0:\n",
    "                df_file = pd.DataFrame({\n",
    "                    'date': time_values[:len(values)],\n",
    "                    'value': values\n",
    "                })\n",
    "                all_data.append(df_file)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(f\"  ERROR: No data extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"  Combining data from {len(all_data)} files...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence)\n",
    "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
    "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: MET Conversion Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.570994Z",
     "iopub.status.busy": "2025-12-21T10:28:42.570844Z",
     "iopub.status.idle": "2025-12-21T10:28:42.573605Z",
     "shell.execute_reply": "2025-12-21T10:28:42.573193Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_tav_amp(df):\n",
    "    \"\"\"\n",
    "    Calculate annual average temperature (tav) and annual amplitude (amp).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with 'date' as index and 'maxt' and 'mint' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (tav, amp)\n",
    "        tav: Annual average ambient temperature\n",
    "        amp: Annual amplitude in mean monthly temperature\n",
    "    \"\"\"\n",
    "    # Calculate daily mean temperature\n",
    "    df = df.copy()\n",
    "    df['tmean'] = (df['maxt'] + df['mint']) / 2.0\n",
    "    \n",
    "    # Calculate monthly means\n",
    "    df['year'] = df.index.year\n",
    "    df['month'] = df.index.month\n",
    "    monthly_means = df.groupby(['year', 'month'])['tmean'].mean()\n",
    "    \n",
    "    # Calculate overall annual average (tav)\n",
    "    tav = df['tmean'].mean()\n",
    "    \n",
    "    # Calculate annual amplitude (amp)\n",
    "    # Average of all January means minus average of all July means, divided by 2\n",
    "    jan_means = monthly_means[monthly_means.index.get_level_values('month') == 1].mean()\n",
    "    jul_means = monthly_means[monthly_means.index.get_level_values('month') == 7].mean()\n",
    "    amp = (jan_means - jul_means) / 2.0\n",
    "    \n",
    "    return tav, amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.575004Z",
     "iopub.status.busy": "2025-12-21T10:28:42.574863Z",
     "iopub.status.idle": "2025-12-21T10:28:42.578044Z",
     "shell.execute_reply": "2025-12-21T10:28:42.577751Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_saturation_vapor_pressure(temperature):\n",
    "    \"\"\"\n",
    "    Calculate saturation vapor pressure (kPa) at a given temperature using SILO method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    temperature : float or array\n",
    "        Temperature in °C\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float or array\n",
    "        Saturation vapor pressure in kPa\n",
    "    \"\"\"\n",
    "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
    "    return 0.611 * np.exp(17.27 * temperature / (temperature + 237.3))\n",
    "\n",
    "\n",
    "def calculate_vapor_pressure(hurs_df, tasmax_df, tasmin_df):\n",
    "    \"\"\"\n",
    "    Calculate vapor pressure (hPa) from mean relative humidity and temperature using SILO method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hurs_df : pd.DataFrame\n",
    "        DataFrame with date and value (mean relative humidity %) columns\n",
    "    tasmax_df : pd.DataFrame\n",
    "        DataFrame with date and value (maximum temperature °C) columns\n",
    "    tasmin_df : pd.DataFrame\n",
    "        DataFrame with date and value (minimum temperature °C) columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with date and value (vapor pressure hPa) columns\n",
    "    \"\"\"\n",
    "    # Merge temperature dataframes\n",
    "    temp_df = tasmax_df.merge(tasmin_df, on='date', suffixes=('_max', '_min'))\n",
    "    temp_df['tmean'] = (temp_df['value_max'] + temp_df['value_min']) / 2.0\n",
    "    \n",
    "    # Merge with mean humidity\n",
    "    merged = hurs_df.merge(temp_df[['date', 'tmean']], on='date')\n",
    "    \n",
    "    # Calculate saturation vapor pressure at mean temperature (in kPa)\n",
    "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
    "    merged['es_kpa'] = calculate_saturation_vapor_pressure(merged['tmean'])\n",
    "    \n",
    "    # Calculate actual vapor pressure using mean relative humidity (in kPa)\n",
    "    # e_a = (hurs/100) × e_s(T_mean)\n",
    "    merged['ea_kpa'] = (merged['value'] / 100.0) * merged['es_kpa']\n",
    "    \n",
    "    # Convert to SILO VP units (hPa): VP(hPa) = 10 × e_a(kPa)\n",
    "    merged['vp'] = 10.0 * merged['ea_kpa']\n",
    "    \n",
    "    # Return DataFrame with date and vp columns\n",
    "    vp_df = merged[['date', 'vp']].copy()\n",
    "    vp_df = vp_df.rename(columns={'vp': 'value'})\n",
    "    \n",
    "    return vp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.579552Z",
     "iopub.status.busy": "2025-12-21T10:28:42.579436Z",
     "iopub.status.idle": "2025-12-21T10:28:42.589681Z",
     "shell.execute_reply": "2025-12-21T10:28:42.589259Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_met_file(tasmax_df, tasmin_df, pr_df, rsds_df, hurs_df=None, scenario=None, \n",
    "                    output_dir=None, latitude=None, longitude=None, model=None, \n",
    "                    start_year=None, end_year=None):\n",
    "    \"\"\"\n",
    "    Create MET format file from tasmax, tasmin, pr, rsds, and optional hurs DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tasmax_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for maximum temperature\n",
    "    tasmin_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for minimum temperature\n",
    "    pr_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for precipitation\n",
    "    rsds_df : pd.DataFrame\n",
    "        DataFrame with date and value columns for surface downwelling shortwave radiation (W/m²) - REQUIRED\n",
    "    hurs_df : pd.DataFrame, optional\n",
    "        DataFrame with date and value columns for relative humidity (%) - if provided, VP will be calculated\n",
    "    scenario : str\n",
    "        Scenario name (e.g., SSP585 or SSP245)\n",
    "    output_dir : str\n",
    "        Output directory path\n",
    "    latitude : float\n",
    "        Latitude in decimal degrees\n",
    "    longitude : float\n",
    "        Longitude in decimal degrees\n",
    "    model : str\n",
    "        Model name (e.g., \"ACCESS CM2\")\n",
    "    start_year : int, optional\n",
    "        Start year for MET file. Only data from this year onwards will be included.\n",
    "        If None, uses the minimum date from the data.\n",
    "    end_year : int, optional\n",
    "        End year for MET file. Only data up to this year will be included.\n",
    "        If None, uses the maximum date from the data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple: (tav, amp, num_rows, final_date_range)\n",
    "        tav: Annual average ambient temperature\n",
    "        amp: Annual amplitude in mean monthly temperature\n",
    "        num_rows: Number of rows in the MET file\n",
    "        final_date_range: Dict with 'start' and 'end' dates from the final data\n",
    "    \"\"\"\n",
    "    # Merge all dataframes on date\n",
    "    merged = tasmax_df.copy()\n",
    "    merged = merged.rename(columns={'value': 'maxt'})\n",
    "    merged['date'] = pd.to_datetime(merged['date'])\n",
    "    \n",
    "    # Merge tasmin\n",
    "    tasmin_df['date'] = pd.to_datetime(tasmin_df['date'])\n",
    "    merged = merged.merge(tasmin_df[['date', 'value']], on='date', how='outer')\n",
    "    merged = merged.rename(columns={'value': 'mint'})\n",
    "    \n",
    "    # Merge pr (precipitation/rain)\n",
    "    pr_df['date'] = pd.to_datetime(pr_df['date'])\n",
    "    merged = merged.merge(pr_df[['date', 'value']], on='date', how='outer')\n",
    "    merged = merged.rename(columns={'value': 'rain'})\n",
    "    \n",
    "    # Merge rsds (radiation) - REQUIRED\n",
    "    # rsds is in W/m², convert to MJ/m² by multiplying by 0.0864 (seconds per day / 1e6)\n",
    "    if rsds_df is None or len(rsds_df) == 0:\n",
    "        raise ValueError(\"rsds_df is required but is None or empty\")\n",
    "    \n",
    "    rsds_df['date'] = pd.to_datetime(rsds_df['date'])\n",
    "    # Convert W/m² to MJ/m² (multiply by seconds per day / 1e6)\n",
    "    rsds_df['value_mj'] = rsds_df['value'] * 0.0864\n",
    "    merged = merged.merge(rsds_df[['date', 'value_mj']], on='date', how='outer')\n",
    "    merged = merged.rename(columns={'value_mj': 'radn'})\n",
    "    \n",
    "    # Calculate vp (vapor pressure) if hurs is provided, otherwise leave blank\n",
    "    if hurs_df is not None and len(hurs_df) > 0:\n",
    "        # Calculate VP using SILO method\n",
    "        vp_df = calculate_vapor_pressure(hurs_df, tasmax_df, tasmin_df)\n",
    "        vp_df['date'] = pd.to_datetime(vp_df['date'])\n",
    "        merged = merged.merge(vp_df[['date', 'value']], on='date', how='left')  # Use 'left' to keep only dates that exist in merged\n",
    "        merged = merged.rename(columns={'value': 'vp'})\n",
    "        # Ensure VP is numeric only where it was calculated, leave NaN where hurs data was missing\n",
    "        merged['vp'] = pd.to_numeric(merged['vp'], errors='coerce')\n",
    "        # Count how many VP values were calculated vs missing\n",
    "        vp_calculated = merged['vp'].notna().sum()\n",
    "        vp_missing = merged['vp'].isna().sum()\n",
    "        print(f\"  [OK] Calculated vapor pressure for {vp_calculated} days\")\n",
    "        if vp_missing > 0:\n",
    "            print(f\"  [WARNING] VP data missing for {vp_missing} days (hurs data not available) - these will be left blank\")\n",
    "    else:\n",
    "        # vp (vapor pressure) is left blank if hurs not available\n",
    "        merged['vp'] = ''\n",
    "        print(f\"  [INFO] hurs not provided - VP left blank\")\n",
    "    \n",
    "    # Sort by date\n",
    "    merged = merged.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Filter data to only include dates from start_year onwards and up to end_year\n",
    "    original_count_before_filter = len(merged)\n",
    "    original_date_range = f\"{merged['date'].min()} to {merged['date'].max()}\" if len(merged) > 0 else \"no data\"\n",
    "    \n",
    "    if start_year is not None:\n",
    "        start_date = pd.Timestamp(year=start_year, month=1, day=1)\n",
    "        merged = merged[merged['date'] >= start_date].copy()\n",
    "        removed_count = original_count_before_filter - len(merged)\n",
    "        if removed_count > 0:\n",
    "            print(f\"  [INFO] Filtered data to start from {start_year}: removed {removed_count:,} earlier records\")\n",
    "            if len(merged) == 0:\n",
    "                print(f\"  [WARNING] All data was filtered out! Original date range was {original_date_range}\")\n",
    "                print(f\"  [WARNING] Consider setting START_YEAR=None or to an earlier year\")\n",
    "    \n",
    "    if end_year is not None:\n",
    "        end_date = pd.Timestamp(year=end_year, month=12, day=31)\n",
    "        count_before_end_filter = len(merged)\n",
    "        merged = merged[merged['date'] <= end_date].copy()\n",
    "        removed_count = count_before_end_filter - len(merged)\n",
    "        if removed_count > 0:\n",
    "            print(f\"  [INFO] Filtered data to end at {end_year}: removed {removed_count:,} later records\")\n",
    "    \n",
    "    # CRITICAL FIX: Create a complete date range to ensure all days are included\n",
    "    # This ensures leap years have 366 days and non-leap years have 365 days\n",
    "    # IMPORTANT: Use the actual min/max dates from the filtered data\n",
    "    # Only extend to full years if the data actually covers those dates\n",
    "    if len(merged) == 0:\n",
    "        raise ValueError(\"No data remaining after filtering! Check START_YEAR and END_YEAR settings.\")\n",
    "    \n",
    "    # Get actual min and max dates from filtered data (these are the dates that actually have data)\n",
    "    actual_min_date = merged['date'].min()\n",
    "    actual_max_date = merged['date'].max()\n",
    "    \n",
    "    # CRITICAL FIX: Normalize dates to remove time components for proper matching\n",
    "    # This ensures dates like \"1986-01-02 09:00:00\" match \"1986-01-02 00:00:00\"\n",
    "    # Without this, reindexing fails and all values become NaN (then filled with 0.0)\n",
    "    merged['date'] = pd.to_datetime(merged['date']).dt.normalize()\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence) - duplicates can occur after normalization\n",
    "    # if different variables had the same date with different time components\n",
    "    duplicate_count = merged.duplicated(subset='date').sum()\n",
    "    if duplicate_count > 0:\n",
    "        print(f\"  [INFO] Found {duplicate_count} duplicate dates after normalization, removing duplicates (keeping first occurrence)\")\n",
    "        merged = merged.drop_duplicates(subset='date', keep='first').reset_index(drop=True)\n",
    "    \n",
    "    # Use the actual data range, rounded to full years\n",
    "    # This ensures we don't create dates that don't exist in the source data\n",
    "    min_date = pd.Timestamp(year=actual_min_date.year, month=1, day=1)\n",
    "    max_date = pd.Timestamp(year=actual_max_date.year, month=12, day=31)\n",
    "    \n",
    "    # Create complete date range (includes all days, including day 366 for leap years)\n",
    "    complete_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    complete_date_range = pd.to_datetime(complete_date_range).normalize()\n",
    "    \n",
    "    # Set date as index for reindexing\n",
    "    merged = merged.set_index('date')\n",
    "    \n",
    "    # Reindex to include all days in the complete range\n",
    "    merged = merged.reindex(complete_date_range)\n",
    "    \n",
    "    # Fill missing values for numeric columns using forward fill then backward fill\n",
    "    # This handles gaps in the data gracefully\n",
    "    # NOTE: VP is NOT filled - if hurs data is missing, VP remains blank\n",
    "    numeric_cols = ['maxt', 'mint', 'rain', 'radn']\n",
    "    for col in numeric_cols:\n",
    "        if col in merged.columns:\n",
    "            # Forward fill first, then backward fill to handle gaps at start/end\n",
    "            # This ensures that if data starts mid-year, early dates are filled from the first available value\n",
    "            merged[col] = merged[col].ffill().bfill()\n",
    "            # Only fill remaining NaNs with 0 if there's absolutely no data (shouldn't happen after ffill/bfill)\n",
    "            # But check if we have any valid data first\n",
    "            if merged[col].notna().any():\n",
    "                # If we have some data, use the mean of available values for any remaining NaNs\n",
    "                # This is safer than 0.0 and handles edge cases better\n",
    "                mean_val = merged[col].mean()\n",
    "                if pd.notna(mean_val):\n",
    "                    merged[col] = merged[col].fillna(mean_val)\n",
    "                else:\n",
    "                    merged[col] = merged[col].fillna(0.0)\n",
    "            else:\n",
    "                # No data at all - this shouldn't happen, but fill with 0 as last resort\n",
    "                print(f\"  [WARNING] Column {col} has no valid data! All values will be 0.0\")\n",
    "                merged[col] = merged[col].fillna(0.0)\n",
    "    \n",
    "    # Handle VP separately - DO NOT fill missing VP values\n",
    "    # If hurs data was missing, VP should remain blank (NaN or empty string)\n",
    "    # This includes cases where date range was extended but source data doesn't exist (e.g., Dec 31, 2064)\n",
    "    if 'vp' in merged.columns:\n",
    "        # Convert all NaN values to empty string (whether numeric or object type)\n",
    "        merged.loc[merged['vp'].isna(), 'vp'] = ''\n",
    "        # If VP was numeric and now has empty strings, convert dtype to object to allow mixed types\n",
    "        if merged['vp'].dtype != 'object':\n",
    "            # Check if we have any empty strings\n",
    "            if (merged['vp'] == '').any():\n",
    "                # Convert to object to allow both numeric and string values\n",
    "                merged['vp'] = merged['vp'].astype(object)\n",
    "                # Convert numeric values back to float\n",
    "                mask = (merged['vp'] != '') & (merged['vp'].notna())\n",
    "                merged.loc[mask, 'vp'] = pd.to_numeric(merged.loc[mask, 'vp'], errors='coerce')\n",
    "                # Convert any resulting NaN back to empty string\n",
    "                merged.loc[merged['vp'].isna(), 'vp'] = ''\n",
    "    \n",
    "    # Reset index to get date back as a column\n",
    "    merged = merged.reset_index()\n",
    "    merged = merged.rename(columns={'index': 'date'})\n",
    "    \n",
    "    # Diagnostic: Check data before calculating tav/amp\n",
    "    if len(merged) > 0:\n",
    "        maxt_valid = merged['maxt'].notna().sum()\n",
    "        mint_valid = merged['mint'].notna().sum()\n",
    "        maxt_nonzero = (merged['maxt'] != 0).sum() if 'maxt' in merged.columns else 0\n",
    "        mint_nonzero = (merged['mint'] != 0).sum() if 'mint' in merged.columns else 0\n",
    "        print(f\"  [INFO] After date range creation: {len(merged)} total rows\")\n",
    "        print(f\"  [INFO] Valid maxt: {maxt_valid}, non-zero maxt: {maxt_nonzero}\")\n",
    "        print(f\"  [INFO] Valid mint: {mint_valid}, non-zero mint: {mint_nonzero}\")\n",
    "        if maxt_nonzero == 0 or mint_nonzero == 0:\n",
    "            print(f\"  [WARNING] All temperature values are zero! This may indicate a data issue.\")\n",
    "            print(f\"  [WARNING] Date range: {merged['date'].min()} to {merged['date'].max()}\")\n",
    "    \n",
    "    # Calculate tav and amp (use only non-NaN values for calculation)\n",
    "    merged_temp = merged[['date', 'maxt', 'mint']].copy()\n",
    "    merged_temp = merged_temp.set_index('date')\n",
    "    merged_temp.index = pd.to_datetime(merged_temp.index)\n",
    "    tav, amp = calculate_tav_amp(merged_temp)\n",
    "    \n",
    "    # Create year and day columns\n",
    "    merged['year'] = merged['date'].dt.year\n",
    "    merged['day'] = merged['date'].dt.dayofyear\n",
    "    \n",
    "    # Add empty columns for evap and code\n",
    "    merged['evap'] = ''  # Leave blank\n",
    "    merged['code'] = '222222'  # Hardcoded code value for all rows\n",
    "    \n",
    "    # Ensure vp column exists (should already be set above)\n",
    "    if 'vp' not in merged.columns:\n",
    "        merged['vp'] = ''\n",
    "    \n",
    "    # Convert VP to appropriate format for output\n",
    "    # - Numeric values (calculated) should remain as float\n",
    "    # - Missing values (NaN or empty string) should be empty string\n",
    "    if merged['vp'].dtype != 'object':\n",
    "        # VP is numeric - convert NaN to empty string, keep valid values as float\n",
    "        merged.loc[merged['vp'].isna(), 'vp'] = ''\n",
    "        # Convert non-empty values to float\n",
    "        mask = (merged['vp'] != '') & (merged['vp'].notna())\n",
    "        merged.loc[mask, 'vp'] = pd.to_numeric(merged.loc[mask, 'vp'], errors='coerce')\n",
    "    # If VP is already string (empty), keep it as is\n",
    "    \n",
    "    # Check for blank VP values and issue warning\n",
    "    vp_blank_count = ((merged['vp'] == '') | (merged['vp'].isna())).sum()\n",
    "    if vp_blank_count > 0:\n",
    "        # Find which years have blank VP values\n",
    "        blank_vp_dates = merged[((merged['vp'] == '') | (merged['vp'].isna())) & merged['date'].notna()]\n",
    "        if len(blank_vp_dates) > 0:\n",
    "            blank_years = sorted(blank_vp_dates['date'].dt.year.unique())\n",
    "            years_str = ', '.join(map(str, blank_years))\n",
    "            print(f\"  [WARNING] Found {vp_blank_count} days with blank VP values (missing hurs data)\")\n",
    "            print(f\"  [WARNING] Years affected: {years_str}\")\n",
    "            print(f\"  [WARNING] These VP values will appear as blank spaces in the output files\")\n",
    "    \n",
    "    met_data = merged[['year', 'day', 'radn', 'maxt', 'mint', 'rain', 'evap', 'vp', 'code']].copy()\n",
    "    \n",
    "    # Prepare header\n",
    "    current_date = datetime.now().strftime('%Y%m%d')\n",
    "    model_scenario = f\"{model.replace(' ', '_')}_{scenario}\" if model and scenario else \"CMIP6\"\n",
    "    \n",
    "    header = f\"\"\"[weather.met.weather]\n",
    "!Your Ref:  \"\n",
    "latitude = {latitude:.2f}  (DECIMAL DEGREES)\n",
    "longitude =  {longitude:.2f}  (DECIMAL DEGREES)\n",
    "tav = {tav:.2f} (oC) ! Annual average ambient temperature.\n",
    "amp = {amp:.2f} (oC) ! Annual amplitude in mean monthly temperature.\n",
    "!Data Extracted from CMIP6 {model} {scenario} dataset on {current_date} for APSIM\n",
    "!As evaporation is read at 9am, it has been shifted to day before\n",
    "!ie The evaporation measured on 20 April is in row for 19 April\n",
    "!The 6 digit code indicates the source of the 6 data columns\n",
    "!0 actual observation, 1 actual observation composite station\n",
    "!2 interpolated from daily observations\n",
    "!3 interpolated from daily observations using anomaly interpolation method for CLIMARC data\n",
    "!6 synthetic pan\n",
    "!7 interpolated long term averages\n",
    "!more detailed two digit codes are available in SILO's 'Standard' format files\n",
    "!\n",
    "!For further information see the documentation on the datadrill\n",
    "!  http://www.longpaddock.qld.gov.au/silo\n",
    "!\n",
    "year  day radn  maxt   mint  rain  evap    vp   code\n",
    " ()   () (MJ/m^2) (oC)  (oC)  (mm)  (mm) (hPa)     ()\n",
    "\"\"\"\n",
    "    \n",
    "    # Create output filename with coordinate-based naming\n",
    "    # Format: {Model}_{Scenario}_{Lat}_{Lon}.met (e.g., ACCESS_CM2_SSP245_-31.75_117.60.met)\n",
    "    lat_str = f\"{latitude:.2f}\"\n",
    "    lon_str = f\"{longitude:.2f}\"\n",
    "    output_filename = f\"{model_scenario}_{lat_str}_{lon_str}.met\"\n",
    "    output_path = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Write MET file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(header)\n",
    "        # Write data rows\n",
    "        for _, row in met_data.iterrows():\n",
    "            # Format the row with proper spacing\n",
    "                        # Format numbers with proper spacing\n",
    "            radn_val = row['radn'] if row['radn'] != '' and pd.notna(row['radn']) else ''\n",
    "            evap_val = row['evap'] if row['evap'] != '' and pd.notna(row['evap']) else ''\n",
    "            \n",
    "            if radn_val != '':\n",
    "                radn_str = f\"{float(radn_val):6.1f}\"\n",
    "            else:\n",
    "                radn_str = \"      \"  # 6 spaces\n",
    "                \n",
    "            if evap_val != '':\n",
    "                evap_str = f\"{float(evap_val):6.1f}\"\n",
    "            else:\n",
    "                evap_str = \"      \"  # 6 spaces\n",
    "                \n",
    "            # Handle VP - can be numeric (calculated) or empty string (blank)\n",
    "            if pd.notna(row['vp']) and row['vp'] != '':\n",
    "                try:\n",
    "                    vp_str = f\"{float(row['vp']):6.1f}\"\n",
    "                except (ValueError, TypeError):\n",
    "                    vp_str = \"      \"  # 6 spaces\n",
    "            else:\n",
    "                vp_str = \"      \"  # 6 spaces\n",
    "            \n",
    "                        # Code is hardcoded to '222222' for all rows\n",
    "            code_str = \"222222\"\n",
    "            \n",
    "            # Handle NaN values for maxt, mint, rain - use 0.0 as default\n",
    "            maxt_val = row['maxt'] if pd.notna(row['maxt']) else 0.0\n",
    "            mint_val = row['mint'] if pd.notna(row['mint']) else 0.0\n",
    "            rain_val = row['rain'] if pd.notna(row['rain']) else 0.0\n",
    "            \n",
    "            # Format with proper column widths\n",
    "            line = f\"{int(row['year']):4d} {int(row['day']):4d} {radn_str} {maxt_val:6.1f} {mint_val:6.1f} {rain_val:6.1f} {evap_str} {vp_str} {code_str}\\n\"\n",
    "            f.write(line)\n",
    "    \n",
    "    # Count blank VP values in final output for warning (after file is written)\n",
    "    final_blank_vp = ((met_data['vp'] == '') | (met_data['vp'].isna())).sum()\n",
    "    \n",
    "    print(f\"  [OK] Created MET file: {output_filename}\")\n",
    "    \n",
    "    # Final warning about blank VP values\n",
    "    if final_blank_vp > 0:\n",
    "        blank_years_list = sorted(met_data[((met_data['vp'] == '') | (met_data['vp'].isna()))]['year'].unique())\n",
    "        years_str = ', '.join(map(str, blank_years_list))\n",
    "        print(f\"  [WARNING] ========================================\")\n",
    "        print(f\"  [WARNING] BLANK VP VALUES DETECTED!\")\n",
    "        print(f\"  [WARNING] {final_blank_vp} days have blank VP values (missing hurs data)\")\n",
    "        print(f\"  [WARNING] Affected years: {years_str}\")\n",
    "        print(f\"  [WARNING] These will appear as blank spaces in MET/CSV files\")\n",
    "        print(f\"  [WARNING] ========================================\")\n",
    "    \n",
    "    # Also create CSV version with same structure\n",
    "    csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}.csv\"\n",
    "    csv_path = os.path.join(output_dir, csv_filename)\n",
    "    \n",
    "    # Write CSV (without header comments, just data)\n",
    "    met_data.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.1f')\n",
    "    print(f\"  [OK] Created CSV file: {csv_filename}\")\n",
    "    \n",
    "    # Get date range from merged dataframe (date column exists after reset_index)\n",
    "    if 'date' in merged.columns and len(merged) > 0:\n",
    "        final_date_range = {\n",
    "            'start': merged['date'].min(),\n",
    "            'end': merged['date'].max()\n",
    "        }\n",
    "    else:\n",
    "        # Fallback: reconstruct from year and day if needed\n",
    "        final_date_range = {'start': None, 'end': None}\n",
    "    \n",
    "    return tav, amp, len(met_data), final_date_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Main Processing\n",
    "\n",
    "This section extracts data for all variables and creates the MET file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.591211Z",
     "iopub.status.busy": "2025-12-21T10:28:42.591065Z",
     "iopub.status.idle": "2025-12-21T10:28:42.597501Z",
     "shell.execute_reply": "2025-12-21T10:28:42.597053Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_coordinate(model, scenario, latitude, longitude, variables, cmip6_base_dir, output_dir, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Main processing function for user-provided coordinate.\n",
    "    Extract all variables from NC files and convert to MET format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : str\n",
    "        Model name (e.g., \"ACCESS CM2\")\n",
    "    scenario : str\n",
    "        Scenario name (e.g., \"SSP245\")\n",
    "    latitude : float\n",
    "        Target latitude\n",
    "    longitude : float\n",
    "        Target longitude\n",
    "    variables : list\n",
    "        List of variable names to extract\n",
    "    cmip6_base_dir : str\n",
    "        Base directory containing Model Scenario folders\n",
    "    output_dir : str\n",
    "        Output directory for results\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Summary statistics\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Processing Coordinate: ({latitude:.6f}, {longitude:.6f})\")\n",
    "    print(f\"Model: {model}, Scenario: {scenario}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Construct data directory path\n",
    "    data_dir = os.path.join(cmip6_base_dir, f\"{model} {scenario}\")\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"ERROR: Data directory not found: {data_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nData directory: {data_dir}\")\n",
    "    \n",
    "    # Extract data for all variables\n",
    "    extracted_data = {}\n",
    "    \n",
    "    for variable in variables:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Processing variable: {variable}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Extract data from NetCDF files\n",
    "        df = extract_daily_data_from_netcdf(\n",
    "            data_dir, \n",
    "            variable, \n",
    "            latitude, \n",
    "            longitude, \n",
    "            tolerance=tolerance\n",
    "        )\n",
    "        \n",
    "        if df is not None and len(df) > 0:\n",
    "            extracted_data[variable] = df\n",
    "            \n",
    "            # Save individual variable CSV\n",
    "            # Format: {Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv\n",
    "            lat_str = f\"{latitude:.2f}\"\n",
    "            lon_str = f\"{longitude:.2f}\"\n",
    "            model_scenario = f\"{model.replace(' ', '_')}_{scenario}\"\n",
    "            csv_filename = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
    "            csv_path = os.path.join(output_dir, csv_filename)\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8', float_format='%.6f')\n",
    "            print(f\"  [OK] Saved CSV: {csv_filename}\")\n",
    "        else:\n",
    "            print(f\"  WARNING: No data extracted for {variable}\")\n",
    "    \n",
    "    # Check if required variables are available for MET conversion\n",
    "    # Note: rsds is now mandatory (required for radn in MET format)\n",
    "    required_vars = ['tasmax', 'tasmin', 'pr', 'rsds']\n",
    "    missing_vars = [v for v in required_vars if v not in extracted_data]\n",
    "    \n",
    "    if missing_vars:\n",
    "        print(f\"\\nERROR: Missing required variables for MET conversion: {missing_vars}\")\n",
    "        return None\n",
    "    \n",
    "    # Create MET file\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Creating MET file...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get required variables\n",
    "    tasmax_df = extracted_data['tasmax']\n",
    "    tasmin_df = extracted_data['tasmin']\n",
    "    pr_df = extracted_data['pr']\n",
    "    \n",
    "    # Get rsds variable for MET format (now mandatory)\n",
    "    rsds_df = extracted_data.get('rsds', None)\n",
    "    if rsds_df is None:\n",
    "        print(f\"  ERROR: rsds is required but was not extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Get hurs variable for VP calculation (optional but recommended)\n",
    "    hurs_df = extracted_data.get('hurs', None)\n",
    "    if hurs_df is not None:\n",
    "        print(f\"  [INFO] hurs data available - VP will be calculated using SILO method\")\n",
    "    else:\n",
    "        print(f\"  [INFO] hurs data not available - VP will be left blank\")\n",
    "    \n",
    "    # Note: code is hardcoded to '222222'\n",
    "    \n",
    "    tav, amp, num_rows, final_date_range = create_met_file(\n",
    "        tasmax_df=tasmax_df,\n",
    "        tasmin_df=tasmin_df,\n",
    "        pr_df=pr_df,\n",
    "        rsds_df=rsds_df,\n",
    "        hurs_df=hurs_df,\n",
    "        scenario=scenario,\n",
    "        output_dir=output_dir,\n",
    "        latitude=latitude,\n",
    "        longitude=longitude,\n",
    "        model=model,\n",
    "        start_year=START_YEAR,\n",
    "        end_year=END_YEAR\n",
    "    )\n",
    "    \n",
    "    # Summary\n",
    "    summary = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'model': model,\n",
    "        'scenario': scenario,\n",
    "        'variables_extracted': list(extracted_data.keys()),\n",
    "        'num_variables': len(extracted_data),\n",
    "        'tav': tav,\n",
    "        'amp': amp,\n",
    "        'num_rows': num_rows,\n",
    "        'date_range': final_date_range if final_date_range['start'] is not None else {\n",
    "            'start': tasmax_df['date'].min(),\n",
    "            'end': tasmax_df['date'].max()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Processing Summary\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"  Variables extracted: {len(extracted_data)}\")\n",
    "    print(f\"    - {', '.join(extracted_data.keys())}\")\n",
    "    print(f\"  MET file rows: {num_rows}\")\n",
    "    print(f\"  Date range: {summary['date_range']['start']} to {summary['date_range']['end']}\")\n",
    "    print(f\"  tav (annual average temp): {tav:.2f} °C\")\n",
    "    print(f\"  amp (annual amplitude): {amp:.2f} °C\")\n",
    "    print(f\"  Output directory: {output_dir}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Summary\n",
    "\n",
    "Review the configuration below before running the main processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.598952Z",
     "iopub.status.busy": "2025-12-21T10:28:42.598737Z",
     "iopub.status.idle": "2025-12-21T10:28:42.602836Z",
     "shell.execute_reply": "2025-12-21T10:28:42.602348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFIGURATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Model: ACCESS CM2\n",
      "Scenario: obs\n",
      "\n",
      "Coordinates:\n",
      "  Latitude: -31.450000°\n",
      "  Longitude: 117.550000°\n",
      "  Tolerance: 0.01 degrees (≈ 1.1 km)\n",
      "\n",
      "Variables to process (5):\n",
      "  - tasmax → maxt (maximum temperature)\n",
      "  - tasmin → mint (minimum temperature)\n",
      "  - pr → rain (precipitation)\n",
      "  - rsds → radn (radiation, W/m² → MJ/m²)\n",
      "\n",
      "Directories:\n",
      "  CMIP6 Base: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
      "  Data Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 obs\n",
      "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n",
      "\n",
      "Output Files:\n",
      "  MET file: ACCESS_CM2_obs_-31.45_117.55.met\n",
      "  CSV file: ACCESS_CM2_obs_-31.45_117.55.csv\n",
      "  Variable CSVs: 5 files (one per variable)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Display full configuration\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nModel: {MODEL}\")\n",
    "print(f\"Scenario: {SCENARIO}\")\n",
    "print(f\"\\nCoordinates:\")\n",
    "print(f\"  Latitude: {LATITUDE:.6f}°\")\n",
    "print(f\"  Longitude: {LONGITUDE:.6f}°\")\n",
    "print(f\"  Tolerance: {COORD_TOLERANCE} degrees (≈ {COORD_TOLERANCE * 111:.1f} km)\")\n",
    "print(f\"\\nVariables to process ({len(VARIABLES)}):\")\n",
    "for var in VARIABLES:\n",
    "    if var == 'tasmax':\n",
    "        print(f\"  - {var} → maxt (maximum temperature)\")\n",
    "    elif var == 'tasmin':\n",
    "        print(f\"  - {var} → mint (minimum temperature)\")\n",
    "    elif var == 'pr':\n",
    "        print(f\"  - {var} → rain (precipitation)\")\n",
    "    elif var == 'rsds':\n",
    "        print(f\"  - {var} → radn (radiation, W/m² → MJ/m²)\")\n",
    "print(f\"\\nDirectories:\")\n",
    "print(f\"  CMIP6 Base: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  Data Directory: {os.path.join(CMIP6_BASE_DIR, f'{MODEL} {SCENARIO}')}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
    "print(f\"  CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
    "print(f\"  Variable CSVs: {len(VARIABLES)} files (one per variable)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T10:28:42.604401Z",
     "iopub.status.busy": "2025-12-21T10:28:42.604209Z",
     "iopub.status.idle": "2025-12-21T10:31:27.705974Z",
     "shell.execute_reply": "2025-12-21T10:31:27.705107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING PROCESSING\n",
      "======================================================================\n",
      "Model: ACCESS CM2\n",
      "Scenario: obs\n",
      "Coordinates: (-31.450000, 117.550000)\n",
      "Variables to process: 5 (tasmax, tasmin, pr, rsds, hurs)\n",
      "Output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\-31.45_117.55\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Processing Coordinate: (-31.450000, 117.550000)\n",
      "Model: ACCESS CM2, Scenario: obs\n",
      "======================================================================\n",
      "\n",
      "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 obs\n",
      "\n",
      "======================================================================\n",
      "Processing variable: tasmax\n",
      "======================================================================\n",
      "  Found files in subdirectory: tasmax_ACCESS CM2_obs/\n",
      "  Found 30 NetCDF files\n",
      "  Grid bounds: Lat [-44.5000, -10.0000], Lon [112.0000, 156.2500]\n",
      "  Target coordinate: (-31.4500, 117.5500)\n",
      "  [OK] Target coordinate is within file bounds\n",
      "  [OK] Using grid point: (-31.4500, 117.5500)\n",
      "  [OK] Distance from target: 0.0000° lat, 0.0000° lon\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  tasmax: 100%|██████████| 30/30 [01:15<00:00,  2.52s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 75.9 seconds\n",
      "  Date range: 1985-01-01 09:00:00 to 2014-12-31 09:00:00\n",
      "  [OK] Saved CSV: ACCESS_CM2_obs_-31.45_117.55_tasmax.csv\n",
      "\n",
      "======================================================================\n",
      "Processing variable: tasmin\n",
      "======================================================================\n",
      "  Found files in subdirectory: tasmin_ACCESS CM2_obs/\n",
      "  Found 30 NetCDF files\n",
      "  Grid bounds: Lat [-44.5000, -10.0000], Lon [112.0000, 156.2500]\n",
      "  Target coordinate: (-31.4500, 117.5500)\n",
      "  [OK] Target coordinate is within file bounds\n",
      "  [OK] Using grid point: (-31.4500, 117.5500)\n",
      "  [OK] Distance from target: 0.0000° lat, 0.0000° lon\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  tasmin: 100%|██████████| 30/30 [01:54<00:00,  3.81s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 114.3 seconds\n",
      "  Date range: 1985-01-01 09:00:00 to 2014-12-31 09:00:00\n",
      "  [OK] Saved CSV: ACCESS_CM2_obs_-31.45_117.55_tasmin.csv\n",
      "\n",
      "======================================================================\n",
      "Processing variable: pr\n",
      "======================================================================\n",
      "  Found files in subdirectory: pr_ACCESS CM2_obs/\n",
      "  Found 30 NetCDF files\n",
      "  Grid bounds: Lat [-44.5000, -10.0000], Lon [112.0000, 156.2500]\n",
      "  Target coordinate: (-31.4500, 117.5500)\n",
      "  [OK] Target coordinate is within file bounds\n",
      "  [OK] Using grid point: (-31.4500, 117.5500)\n",
      "  [OK] Distance from target: 0.0000° lat, 0.0000° lon\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  pr: 100%|██████████| 30/30 [01:59<00:00,  3.98s/file]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 119.3 seconds\n",
      "  Date range: 1985-01-01 09:00:00 to 2014-12-31 09:00:00\n",
      "  [OK] Saved CSV: ACCESS_CM2_obs_-31.45_117.55_pr.csv\n",
      "\n",
      "======================================================================\n",
      "Processing variable: rsds\n",
      "======================================================================\n",
      "  Found files in subdirectory: rsds_BARRA_obs/\n",
      "  Found 30 NetCDF files\n",
      "  Grid bounds: Lat [-44.5000, -10.0000], Lon [112.0000, 156.2500]\n",
      "  Target coordinate: (-31.4500, 117.5500)\n",
      "  [OK] Target coordinate is within file bounds\n",
      "  [OK] Using grid point: (-31.4500, 117.5500)\n",
      "  [OK] Distance from target: 0.0000° lat, 0.0000° lon\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  rsds: 100%|██████████| 30/30 [00:12<00:00,  2.48file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 12.1 seconds\n",
      "  Date range: 1985-01-01 12:00:00 to 2014-12-31 12:00:00\n",
      "  [OK] Saved CSV: ACCESS_CM2_obs_-31.45_117.55_rsds.csv\n",
      "\n",
      "======================================================================\n",
      "Processing variable: hurs\n",
      "======================================================================\n",
      "  Found files in subdirectory: hurs_BARRA_obs/\n",
      "  Found 30 NetCDF files\n",
      "  Grid bounds: Lat [-44.5000, -10.0000], Lon [112.0000, 156.2500]\n",
      "  Target coordinate: (-31.4500, 117.5500)\n",
      "  [OK] Target coordinate is within file bounds\n",
      "  [OK] Using grid point: (-31.4500, 117.5500)\n",
      "  [OK] Distance from target: 0.0000° lat, 0.0000° lon\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  hurs: 100%|██████████| 30/30 [00:08<00:00,  3.40file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 8.9 seconds\n",
      "  Date range: 1985-01-01 12:00:00 to 2014-12-31 12:00:00\n",
      "  [OK] Saved CSV: ACCESS_CM2_obs_-31.45_117.55_hurs.csv\n",
      "\n",
      "======================================================================\n",
      "Creating MET file...\n",
      "======================================================================\n",
      "  [INFO] hurs data available - VP will be calculated using SILO method\n",
      "  [OK] Calculated vapor pressure for 0 days\n",
      "  [WARNING] VP data missing for 21914 days (hurs data not available) - these will be left blank\n",
      "  [INFO] Filtered data to start from 1986: removed 730 earlier records\n",
      "  [INFO] Filtered data to end at 2014: removed 2 later records\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex on an axis with duplicate labels",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m summary \u001b[38;5;241m=\u001b[39m process_coordinate(\n\u001b[0;32m     13\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL,\n\u001b[0;32m     14\u001b[0m     scenario\u001b[38;5;241m=\u001b[39mSCENARIO,\n\u001b[0;32m     15\u001b[0m     latitude\u001b[38;5;241m=\u001b[39mLATITUDE,\n\u001b[0;32m     16\u001b[0m     longitude\u001b[38;5;241m=\u001b[39mLONGITUDE,\n\u001b[0;32m     17\u001b[0m     variables\u001b[38;5;241m=\u001b[39mVARIABLES,\n\u001b[0;32m     18\u001b[0m     cmip6_base_dir\u001b[38;5;241m=\u001b[39mCMIP6_BASE_DIR,\n\u001b[0;32m     19\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mOUTPUT_DIR,\n\u001b[0;32m     20\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39mCOORD_TOLERANCE\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m summary:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 109\u001b[0m, in \u001b[0;36mprocess_coordinate\u001b[1;34m(model, scenario, latitude, longitude, variables, cmip6_base_dir, output_dir, tolerance)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  [INFO] hurs data not available - VP will be left blank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Note: code is hardcoded to '222222'\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m tav, amp, num_rows, final_date_range \u001b[38;5;241m=\u001b[39m create_met_file(\n\u001b[0;32m    110\u001b[0m     tasmax_df\u001b[38;5;241m=\u001b[39mtasmax_df,\n\u001b[0;32m    111\u001b[0m     tasmin_df\u001b[38;5;241m=\u001b[39mtasmin_df,\n\u001b[0;32m    112\u001b[0m     pr_df\u001b[38;5;241m=\u001b[39mpr_df,\n\u001b[0;32m    113\u001b[0m     rsds_df\u001b[38;5;241m=\u001b[39mrsds_df,\n\u001b[0;32m    114\u001b[0m     hurs_df\u001b[38;5;241m=\u001b[39mhurs_df,\n\u001b[0;32m    115\u001b[0m     scenario\u001b[38;5;241m=\u001b[39mscenario,\n\u001b[0;32m    116\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m    117\u001b[0m     latitude\u001b[38;5;241m=\u001b[39mlatitude,\n\u001b[0;32m    118\u001b[0m     longitude\u001b[38;5;241m=\u001b[39mlongitude,\n\u001b[0;32m    119\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    120\u001b[0m     start_year\u001b[38;5;241m=\u001b[39mSTART_YEAR,\n\u001b[0;32m    121\u001b[0m     end_year\u001b[38;5;241m=\u001b[39mEND_YEAR\n\u001b[0;32m    122\u001b[0m )\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Summary\u001b[39;00m\n\u001b[0;32m    125\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m: latitude,\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m: longitude,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     }\n\u001b[0;32m    139\u001b[0m }\n",
      "Cell \u001b[1;32mIn[7], line 144\u001b[0m, in \u001b[0;36mcreate_met_file\u001b[1;34m(tasmax_df, tasmin_df, pr_df, rsds_df, hurs_df, scenario, output_dir, latitude, longitude, model, start_year, end_year)\u001b[0m\n\u001b[0;32m    141\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Reindex to include all days in the complete range\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m merged \u001b[38;5;241m=\u001b[39m merged\u001b[38;5;241m.\u001b[39mreindex(complete_date_range)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Fill missing values for numeric columns using forward fill then backward fill\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# This handles gaps in the data gracefully\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# NOTE: VP is NOT filled - if hurs data is missing, VP remains blank\u001b[39;00m\n\u001b[0;32m    149\u001b[0m numeric_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmint\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mradn\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ibian\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5378\u001b[0m, in \u001b[0;36mDataFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5359\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m   5360\u001b[0m     NDFrame\u001b[38;5;241m.\u001b[39mreindex,\n\u001b[0;32m   5361\u001b[0m     klass\u001b[38;5;241m=\u001b[39m_shared_doc_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mklass\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5376\u001b[0m     tolerance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5377\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m-> 5378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[0;32m   5379\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5380\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5381\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5382\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5383\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m   5384\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   5385\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5386\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m   5387\u001b[0m         limit\u001b[38;5;241m=\u001b[39mlimit,\n\u001b[0;32m   5388\u001b[0m         tolerance\u001b[38;5;241m=\u001b[39mtolerance,\n\u001b[0;32m   5389\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ibian\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:5610\u001b[0m, in \u001b[0;36mNDFrame.reindex\u001b[1;34m(self, labels, index, columns, axis, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[0;32m   5607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_multi(axes, copy, fill_value)\n\u001b[0;32m   5609\u001b[0m \u001b[38;5;66;03m# perform the reindex on the axes\u001b[39;00m\n\u001b[1;32m-> 5610\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_axes(\n\u001b[0;32m   5611\u001b[0m     axes, level, limit, tolerance, method, fill_value, copy\n\u001b[0;32m   5612\u001b[0m )\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreindex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ibian\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:5633\u001b[0m, in \u001b[0;36mNDFrame._reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   5630\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   5632\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(a)\n\u001b[1;32m-> 5633\u001b[0m new_index, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mreindex(\n\u001b[0;32m   5634\u001b[0m     labels, level\u001b[38;5;241m=\u001b[39mlevel, limit\u001b[38;5;241m=\u001b[39mlimit, tolerance\u001b[38;5;241m=\u001b[39mtolerance, method\u001b[38;5;241m=\u001b[39mmethod\n\u001b[0;32m   5635\u001b[0m )\n\u001b[0;32m   5637\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(a)\n\u001b[0;32m   5638\u001b[0m obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[0;32m   5639\u001b[0m     {axis: [new_index, indexer]},\n\u001b[0;32m   5640\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m   5641\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   5642\u001b[0m     allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   5643\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ibian\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:4429\u001b[0m, in \u001b[0;36mIndex.reindex\u001b[1;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[0;32m   4426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot handle a non-unique multi-index!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[0;32m   4428\u001b[0m     \u001b[38;5;66;03m# GH#42568\u001b[39;00m\n\u001b[1;32m-> 4429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reindex on an axis with duplicate labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4431\u001b[0m     indexer, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_indexer_non_unique(target)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex on an axis with duplicate labels"
     ]
    }
   ],
   "source": [
    "# Execute main processing\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Scenario: {SCENARIO}\")\n",
    "print(f\"Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"Variables to process: {len(VARIABLES)} ({', '.join(VARIABLES)})\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "summary = process_coordinate(\n",
    "    model=MODEL,\n",
    "    scenario=SCENARIO,\n",
    "    latitude=LATITUDE,\n",
    "    longitude=LONGITUDE,\n",
    "    variables=VARIABLES,\n",
    "    cmip6_base_dir=CMIP6_BASE_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    tolerance=COORD_TOLERANCE\n",
    ")\n",
    "\n",
    "if summary:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✓ PROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nOutput files created:\")\n",
    "    print(f\"  - MET file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.met\")\n",
    "    print(f\"  - CSV file: {MODEL.replace(' ', '_')}_{SCENARIO}_{LATITUDE:.2f}_{LONGITUDE:.2f}.csv\")\n",
    "    print(f\"  - Individual variable CSVs: {len(summary['variables_extracted'])} files\")\n",
    "    print(f\"\\nAll files saved to: {OUTPUT_DIR}\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"✗ PROCESSING FAILED\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"Please check error messages above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Documentation\n",
    "\n",
    "### CMIP6 Data Structure and Conventions\n",
    "\n",
    "CMIP6 (Coupled Model Intercomparison Project Phase 6) climate data is organized by:\n",
    "- **Model**: Climate model name (e.g., \"ACCESS CM2\")\n",
    "- **Scenario**: Shared Socioeconomic Pathway (e.g., \"SSP245\", \"SSP585\")\n",
    "- **Variable**: Climate variable name (e.g., \"tasmax\", \"pr\")\n",
    "\n",
    "Data files are stored in NetCDF format (.nc) and organized in folders:\n",
    "`C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\{Model} {Scenario}\\{variable}_*\\*.nc`\n",
    "\n",
    "### Coordinate System\n",
    "\n",
    "Coordinates are provided in decimal degrees:\n",
    "- **Latitude**: -90 to 90 (negative for Southern Hemisphere)\n",
    "- **Longitude**: -180 to 180 (negative for Western Hemisphere)\n",
    "\n",
    "The notebook finds the nearest grid point within a tolerance (default: 0.01 degrees ≈ 1.1 km).\n",
    "\n",
    "### Variable Extraction Workflow\n",
    "\n",
    "1. For each variable, search for matching NetCDF files\n",
    "2. Find nearest grid point to user-specified coordinates\n",
    "3. Extract time series data from all files\n",
    "4. Extract dates using time coordinate from NetCDF (if available) or filename\n",
    "5. **Leap year handling**: Automatically reads all 366 days for leap years (e.g., 2024, 2028)\n",
    "6. Combine and sort by date\n",
    "7. Save as individual CSV files\n",
    "\n",
    "### MET Format Specifications\n",
    "\n",
    "APSIM MET format requires:\n",
    "- **Header**: Metadata including latitude, longitude, tav, amp\n",
    "- **Data columns**: year, day, radn, maxt, mint, rain, evap, vp, code\n",
    "- **Required data**: maxt (from tasmax), mint (from tasmin), rain (from pr), radn (from rsds)\n",
    "- **radn conversion**: rsds (W/m²) converted to radn (MJ/m²) - mandatory\n",
    "- **Blank fields**: vp and code are left blank\n",
    "\n",
    "### Unit Conversions\n",
    "\n",
    "- **rsds** (W/m²) → **radn** (MJ/m²): Multiply by 0.0864\n",
    "- **vp** and **code**: Left blank in MET format\n",
    "\n",
    "### File Naming Conventions\n",
    "\n",
    "All output files use coordinate-based naming:\n",
    "- MET files: `{Model}_{Scenario}_{Lat}_{Lon}.met`\n",
    "- CSV files: `{Model}_{Scenario}_{Lat}_{Lon}_{variable}.csv`\n",
    "\n",
    "Example: `ACCESS_CM2_SSP245_-31.75_117.60.met`\n",
    "\n",
    "### Model and Scenario Selection\n",
    "\n",
    "Update the `MODEL` and `SCENARIO` variables in Section 1 to match your data folder structure.\n",
    "\n",
    "Common scenarios:\n",
    "- **SSP245**: Middle-of-the-road scenario\n",
    "- **SSP585**: High-emissions scenario"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
