{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMIP6 Vapor Pressure (VP) Calculation - SILO Method\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook calculates vapor pressure (VP) from CMIP6 climate data using the SILO method. This method matches SILO units (hPa) and uses mean relative humidity and mean temperature.\n",
    "\n",
    "## Input Variables\n",
    "\n",
    "- **hurs**: Mean relative humidity (%) - required\n",
    "- **tasmax**: Daily maximum temperature (°C) - required\n",
    "- **tasmin**: Daily minimum temperature (°C) - required\n",
    "\n",
    "## Output\n",
    "\n",
    "- **vp**: Vapor pressure (hPa) matching SILO units\n",
    "\n",
    "## Calculation Method (SILO)\n",
    "\n",
    "### Saturation Vapor Pressure (kPa)\n",
    "\n",
    "For mean temperature T_mean in °C:\n",
    "\n",
    "```\n",
    "e_s(T_mean) = 0.611 × exp(17.27 × T_mean / (T_mean + 237.3))\n",
    "```\n",
    "\n",
    "Where: T_mean = (tasmax + tasmin) / 2\n",
    "\n",
    "### Actual Vapor Pressure (hPa)\n",
    "\n",
    "Using mean relative humidity (hurs) and saturation vapor pressure:\n",
    "\n",
    "```\n",
    "VP(hPa) = 10 × (hurs/100) × e_s(T_mean)\n",
    "```\n",
    "\n",
    "Or directly in hPa:\n",
    "\n",
    "```\n",
    "VP(hPa) = (hurs/100) × 0.611 × exp(17.27 × T_mean / (T_mean + 237.3)) × 10\n",
    "```\n",
    "\n",
    "This gives a daily VP proxy. SILO VP is \"9am-like\"; to replicate SILO closely, you would bias-correct this VP proxy to SILO over a historical overlap period.\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Set configuration parameters (Model, Scenario, coordinates)\n",
    "2. Extract hurs, tasmax, and tasmin data from NetCDF files\n",
    "3. Calculate vapor pressure using the SILO method with mean humidity\n",
    "4. Save results to CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T09:51:49.933225Z",
     "iopub.status.busy": "2025-12-19T09:51:49.932996Z",
     "iopub.status.idle": "2025-12-19T09:51:50.376252Z",
     "shell.execute_reply": "2025-12-19T09:51:50.375780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T09:51:50.396782Z",
     "iopub.status.busy": "2025-12-19T09:51:50.396274Z",
     "iopub.status.idle": "2025-12-19T09:51:50.403238Z",
     "shell.execute_reply": "2025-12-19T09:51:50.402790Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  - CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
      "  - Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\n",
      "  - Model: ACCESS CM2\n",
      "  - Scenario: SSP585\n",
      "  - Coordinates: (-31.750000, 117.599998)\n",
      "  - Required Variables: hurs, tasmax, tasmin\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
    "OUTPUT_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\"  # Output directory\n",
    "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
    "\n",
    "# Model and Scenario\n",
    "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
    "SCENARIO = \"SSP585\"   # e.g., \"SSP245\" or \"SSP585\"\n",
    "\n",
    "# Coordinates\n",
    "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
    "LONGITUDE = 117.5999984741211  # Target longitude in decimal degrees (-180 to 180)\n",
    "\n",
    "# Variables required for VP calculation (SILO method with mean humidity)\n",
    "REQUIRED_VARIABLES = ['hurs', 'tasmax', 'tasmin']\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded:\")\n",
    "print(f\"  - CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
    "print(f\"  - Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Model: {MODEL}\")\n",
    "print(f\"  - Scenario: {SCENARIO}\")\n",
    "print(f\"  - Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"  - Required Variables: {', '.join(REQUIRED_VARIABLES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: NetCDF Data Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T09:51:50.405064Z",
     "iopub.status.busy": "2025-12-19T09:51:50.404929Z",
     "iopub.status.idle": "2025-12-19T09:51:50.414723Z",
     "shell.execute_reply": "2025-12-19T09:51:50.413831Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
    "    \"\"\"\n",
    "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    netcdf_dir : str\n",
    "        Directory containing NetCDF files for the variable\n",
    "    variable : str\n",
    "        Variable name (hurs, tasmax, tasmin)\n",
    "    target_lat : float\n",
    "        Target latitude\n",
    "    target_lon : float\n",
    "        Target longitude\n",
    "    tolerance : float\n",
    "        Coordinate matching tolerance in degrees\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns: date, value\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Find all NetCDF files in the directory\n",
    "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
    "    \n",
    "    # Pattern 2: Files in subdirectories named {variable}_*\n",
    "    if len(nc_files) == 0:\n",
    "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
    "        for var_subdir in var_subdirs:\n",
    "            if os.path.isdir(var_subdir):\n",
    "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
    "                if found_files:\n",
    "                    nc_files.extend(found_files)\n",
    "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
    "                    break\n",
    "    \n",
    "    if len(nc_files) == 0:\n",
    "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
    "    \n",
    "    # Cache coordinate information from first file\n",
    "    lat_name = None\n",
    "    lon_name = None\n",
    "    time_name = None\n",
    "    lat_idx = None\n",
    "    lon_idx = None\n",
    "    var_name = None\n",
    "    \n",
    "    # List to store daily data\n",
    "    all_data = []\n",
    "    \n",
    "    # Process first file to get coordinate structure\n",
    "    if len(nc_files) > 0:\n",
    "        try:\n",
    "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
    "            \n",
    "            # Get variable name\n",
    "            for v in ds_sample.data_vars:\n",
    "                if variable in v.lower() or v.lower() in variable.lower():\n",
    "                    var_name = v\n",
    "                    break\n",
    "            \n",
    "            if var_name is None:\n",
    "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
    "                for name in possible_names:\n",
    "                    if name in ds_sample.data_vars:\n",
    "                        var_name = name\n",
    "                        break\n",
    "            \n",
    "            # Get coordinate names\n",
    "            for coord in ds_sample.coords:\n",
    "                coord_lower = coord.lower()\n",
    "                if 'lat' in coord_lower:\n",
    "                    lat_name = coord\n",
    "                elif 'lon' in coord_lower:\n",
    "                    lon_name = coord\n",
    "                elif 'time' in coord_lower:\n",
    "                    time_name = coord\n",
    "            \n",
    "            if lat_name and lon_name:\n",
    "                # Find nearest grid point\n",
    "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
    "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
    "                \n",
    "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
    "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
    "                \n",
    "                # Check if within tolerance\n",
    "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
    "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
    "                else:\n",
    "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
    "            \n",
    "            ds_sample.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not read sample file: {e}\")\n",
    "    \n",
    "    if var_name is None or lat_idx is None or lon_idx is None:\n",
    "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
    "        return None\n",
    "    \n",
    "    # Process all files with progress bar\n",
    "    print(f\"  Processing files...\")\n",
    "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
    "        try:\n",
    "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
    "            \n",
    "            # Extract data using cached indices\n",
    "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
    "            \n",
    "            # Convert to numpy array\n",
    "            values = data.values\n",
    "            if values.ndim > 1:\n",
    "                values = values.flatten()\n",
    "            \n",
    "            # Get time values\n",
    "            time_values = None\n",
    "            \n",
    "            # Method 1: Try to use time coordinate from NetCDF file\n",
    "            if time_name and time_name in ds.coords:\n",
    "                try:\n",
    "                    time_coord = ds[time_name]\n",
    "                    if len(time_coord) == len(values):\n",
    "                        try:\n",
    "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
    "                            time_values = pd.to_datetime(time_decoded.values)\n",
    "                        except:\n",
    "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
    "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
    "                                base_date = pd.to_datetime(base_date_str)\n",
    "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            \n",
    "            # Method 2: Extract year from filename\n",
    "            if time_values is None:\n",
    "                year = None\n",
    "                filename = os.path.basename(nc_file)\n",
    "                all_years = re.findall(r'\\d{4}', filename)\n",
    "                for year_str in all_years:\n",
    "                    year_candidate = int(year_str)\n",
    "                    if 2000 <= year_candidate <= 2100:\n",
    "                        year = year_candidate\n",
    "                        break\n",
    "                \n",
    "                if year:\n",
    "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
    "                else:\n",
    "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
    "            \n",
    "            # Ensure correct number of dates\n",
    "            if len(time_values) != len(values):\n",
    "                if len(time_values) > len(values):\n",
    "                    time_values = time_values[:len(values)]\n",
    "            \n",
    "            # Create DataFrame for this file\n",
    "            if len(values) > 0:\n",
    "                df_file = pd.DataFrame({\n",
    "                    'date': time_values[:len(values)],\n",
    "                    'value': values\n",
    "                })\n",
    "                all_data.append(df_file)\n",
    "            \n",
    "            ds.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if len(all_data) == 0:\n",
    "        print(f\"  ERROR: No data extracted\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all data\n",
    "    print(f\"  Combining data from {len(all_data)} files...\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicate dates (keep first occurrence)\n",
    "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
    "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Vapor Pressure Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T09:51:50.417490Z",
     "iopub.status.busy": "2025-12-19T09:51:50.417105Z",
     "iopub.status.idle": "2025-12-19T09:51:50.421269Z",
     "shell.execute_reply": "2025-12-19T09:51:50.420758Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_saturation_vapor_pressure(temperature):\n",
    "    \"\"\"\n",
    "    Calculate saturation vapor pressure (kPa) at a given temperature.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    temperature : float or array\n",
    "        Temperature in °C\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float or array\n",
    "        Saturation vapor pressure in kPa\n",
    "    \"\"\"\n",
    "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
    "    return 0.611 * np.exp(17.27 * temperature / (temperature + 237.3))\n",
    "\n",
    "\n",
    "def calculate_vapor_pressure(hurs_df, tasmax_df, tasmin_df):\n",
    "    \"\"\"\n",
    "    Calculate vapor pressure (hPa) from mean relative humidity and temperature using SILO method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hurs_df : pd.DataFrame\n",
    "        DataFrame with date and value (mean relative humidity %) columns\n",
    "    tasmax_df : pd.DataFrame\n",
    "        DataFrame with date and value (maximum temperature °C) columns\n",
    "    tasmin_df : pd.DataFrame\n",
    "        DataFrame with date and value (minimum temperature °C) columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with date and value (vapor pressure hPa) columns\n",
    "    \"\"\"\n",
    "    # Merge temperature dataframes\n",
    "    temp_df = tasmax_df.merge(tasmin_df, on='date', suffixes=('_max', '_min'))\n",
    "    temp_df['tmean'] = (temp_df['value_max'] + temp_df['value_min']) / 2.0\n",
    "    \n",
    "    # Merge with mean humidity\n",
    "    merged = hurs_df.merge(temp_df[['date', 'tmean']], on='date')\n",
    "    \n",
    "    # Calculate saturation vapor pressure at mean temperature (in kPa)\n",
    "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
    "    merged['es_kpa'] = calculate_saturation_vapor_pressure(merged['tmean'])\n",
    "    \n",
    "    # Calculate actual vapor pressure using mean relative humidity (in kPa)\n",
    "    # e_a = (hurs/100) × e_s(T_mean)\n",
    "    merged['ea_kpa'] = (merged['value'] / 100.0) * merged['es_kpa']\n",
    "    \n",
    "    # Convert to SILO VP units (hPa): VP(hPa) = 10 × e_a(kPa)\n",
    "    merged['vp'] = 10.0 * merged['ea_kpa']\n",
    "    \n",
    "    # Return DataFrame with date and vp columns\n",
    "    vp_df = merged[['date', 'vp']].copy()\n",
    "    vp_df = vp_df.rename(columns={'vp': 'value'})\n",
    "    \n",
    "    return vp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Main Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-19T09:51:50.423218Z",
     "iopub.status.busy": "2025-12-19T09:51:50.422976Z",
     "iopub.status.idle": "2025-12-19T09:53:35.675069Z",
     "shell.execute_reply": "2025-12-19T09:53:35.674661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Processing Coordinate: (-31.750000, 117.599998)\n",
      "Model: ACCESS CM2, Scenario: SSP585\n",
      "======================================================================\n",
      "\n",
      "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP585\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Processing variable: hurs\n",
      "======================================================================\n",
      "  Found files in subdirectory: hurs_ACCESS CM2 SSP585/\n",
      "  Found 30 NetCDF files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:   3%|▎         | 1/30 [00:00<00:04,  6.88file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:   7%|▋         | 2/30 [00:00<00:04,  6.96file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  10%|█         | 3/30 [00:00<00:03,  7.17file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  13%|█▎        | 4/30 [00:00<00:03,  6.84file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  17%|█▋        | 5/30 [00:00<00:03,  6.97file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  20%|██        | 6/30 [00:00<00:03,  7.05file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  23%|██▎       | 7/30 [00:01<00:03,  7.01file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  27%|██▋       | 8/30 [00:01<00:03,  7.06file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  30%|███       | 9/30 [00:01<00:03,  6.95file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  33%|███▎      | 10/30 [00:01<00:02,  6.95file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  37%|███▋      | 11/30 [00:01<00:02,  6.98file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  40%|████      | 12/30 [00:01<00:02,  6.94file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  43%|████▎     | 13/30 [00:01<00:02,  7.04file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  47%|████▋     | 14/30 [00:02<00:02,  6.97file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  50%|█████     | 15/30 [00:02<00:02,  6.95file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  53%|█████▎    | 16/30 [00:02<00:01,  7.05file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  57%|█████▋    | 17/30 [00:02<00:01,  7.14file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  60%|██████    | 18/30 [00:02<00:01,  6.86file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  63%|██████▎   | 19/30 [00:02<00:01,  6.85file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  67%|██████▋   | 20/30 [00:02<00:01,  6.93file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  70%|███████   | 21/30 [00:03<00:01,  6.94file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  73%|███████▎  | 22/30 [00:03<00:01,  6.96file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  77%|███████▋  | 23/30 [00:03<00:01,  6.95file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  80%|████████  | 24/30 [00:03<00:00,  7.00file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  83%|████████▎ | 25/30 [00:03<00:00,  6.94file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  87%|████████▋ | 26/30 [00:03<00:00,  6.99file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  90%|█████████ | 27/30 [00:03<00:00,  7.00file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  93%|█████████▎| 28/30 [00:04<00:00,  7.06file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs:  93%|█████████▎| 28/30 [00:04<00:00,  7.06file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs: 100%|██████████| 30/30 [00:04<00:00,  9.10file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  hurs: 100%|██████████| 30/30 [00:04<00:00,  7.22file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Error processing hurs_day_ACCESS-CM2_ssp585_r4i1p1f1_AUS-05i_2063_qdc-multiplicative-monthly-q100-linear_BARRA-R2-baseline-1985-2014_model-baseline-1985-2014.nc: [Errno -101] NetCDF: HDF error: 'C:\\\\Users\\\\ibian\\\\Desktop\\\\ClimAdapt\\\\CMIP6\\\\ACCESS CM2 SSP585\\\\hurs_ACCESS CM2 SSP585\\\\hurs_day_ACCESS-CM2_ssp585_r4i1p1f1_AUS-05i_2063_qdc-multiplicative-monthly-q100-linear_BARRA-R2-baseline-1985-2014_model-baseline-1985-2014.nc'\n",
      "  Combining data from 29 files...\n",
      "  ✓ Extracted 10,592 daily records in 4.7 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "  [OK] Extracted 10,592 records for hurs\n",
      "\n",
      "======================================================================\n",
      "Processing variable: tasmax\n",
      "======================================================================\n",
      "  Found files in subdirectory: tasmax_ACCESS CM2 SSP585/\n",
      "  Found 30 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   3%|▎         | 1/30 [00:01<00:54,  1.87s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:   7%|▋         | 2/30 [00:03<00:48,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  10%|█         | 3/30 [00:05<00:46,  1.71s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  13%|█▎        | 4/30 [00:06<00:44,  1.70s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  17%|█▋        | 5/30 [00:08<00:41,  1.65s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  20%|██        | 6/30 [00:10<00:38,  1.62s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  23%|██▎       | 7/30 [00:11<00:36,  1.60s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  27%|██▋       | 8/30 [00:13<00:34,  1.59s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  30%|███       | 9/30 [00:14<00:33,  1.57s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  33%|███▎      | 10/30 [00:16<00:31,  1.57s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  37%|███▋      | 11/30 [00:17<00:29,  1.57s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  40%|████      | 12/30 [00:19<00:28,  1.57s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  43%|████▎     | 13/30 [00:20<00:26,  1.58s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  47%|████▋     | 14/30 [00:22<00:25,  1.59s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  50%|█████     | 15/30 [00:24<00:24,  1.60s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  53%|█████▎    | 16/30 [00:25<00:22,  1.61s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  57%|█████▋    | 17/30 [00:27<00:21,  1.63s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  60%|██████    | 18/30 [00:29<00:19,  1.63s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  63%|██████▎   | 19/30 [00:30<00:17,  1.61s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  67%|██████▋   | 20/30 [00:32<00:15,  1.60s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  70%|███████   | 21/30 [00:33<00:14,  1.60s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  73%|███████▎  | 22/30 [00:35<00:12,  1.62s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  77%|███████▋  | 23/30 [00:37<00:11,  1.64s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  80%|████████  | 24/30 [00:39<00:11,  1.85s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  83%|████████▎ | 25/30 [00:43<00:12,  2.59s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  87%|████████▋ | 26/30 [00:47<00:11,  2.81s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  90%|█████████ | 27/30 [00:48<00:07,  2.49s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  93%|█████████▎| 28/30 [00:50<00:04,  2.24s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax:  97%|█████████▋| 29/30 [00:52<00:02,  2.07s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax: 100%|██████████| 30/30 [00:54<00:00,  1.97s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmax: 100%|██████████| 30/30 [00:54<00:00,  1.80s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 54.0 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "  [OK] Extracted 10,957 records for tasmax\n",
      "\n",
      "======================================================================\n",
      "Processing variable: tasmin\n",
      "======================================================================\n",
      "  Found files in subdirectory: tasmin_ACCESS CM2 SSP585/\n",
      "  Found 30 NetCDF files\n",
      "  Using grid point: (-31.7500, 117.6000)\n",
      "  Processing files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   3%|▎         | 1/30 [00:01<00:49,  1.70s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:   7%|▋         | 2/30 [00:03<00:48,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  10%|█         | 3/30 [00:05<00:46,  1.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  13%|█▎        | 4/30 [00:07<00:46,  1.78s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  17%|█▋        | 5/30 [00:08<00:44,  1.76s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  20%|██        | 6/30 [00:10<00:41,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  23%|██▎       | 7/30 [00:12<00:39,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  27%|██▋       | 8/30 [00:13<00:37,  1.71s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  30%|███       | 9/30 [00:15<00:36,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  33%|███▎      | 10/30 [00:17<00:34,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  37%|███▋      | 11/30 [00:19<00:32,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  40%|████      | 12/30 [00:20<00:31,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  43%|████▎     | 13/30 [00:22<00:29,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  47%|████▋     | 14/30 [00:24<00:27,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  50%|█████     | 15/30 [00:25<00:25,  1.71s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  53%|█████▎    | 16/30 [00:27<00:24,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  57%|█████▋    | 17/30 [00:29<00:22,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  60%|██████    | 18/30 [00:31<00:20,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  63%|██████▎   | 19/30 [00:32<00:18,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  67%|██████▋   | 20/30 [00:34<00:17,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  70%|███████   | 21/30 [00:36<00:15,  1.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  73%|███████▎  | 22/30 [00:38<00:13,  1.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  77%|███████▋  | 23/30 [00:39<00:12,  1.74s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  80%|████████  | 24/30 [00:41<00:10,  1.73s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  83%|████████▎ | 25/30 [00:43<00:08,  1.72s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  87%|████████▋ | 26/30 [00:44<00:06,  1.70s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin:  90%|█████████ | 27/30 [00:46<00:05,  1.68s/file]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  tasmin: 100%|██████████| 30/30 [00:46<00:00,  1.55s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Combining data from 30 files...\n",
      "  ✓ Extracted 10,957 daily records in 46.5 seconds\n",
      "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "  [OK] Extracted 10,957 records for tasmin\n",
      "\n",
      "======================================================================\n",
      "Calculating Vapor Pressure...\n",
      "======================================================================\n",
      "  [OK] Calculated vapor pressure for 10,592 days\n",
      "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
      "  VP range: 4.48 to 35.51 hPa\n",
      "  VP mean: 13.10 hPa\n",
      "\n",
      "  [OK] Saved vapor pressure data to: ACCESS_CM2_SSP585_-31.75_117.60_vp.csv\n",
      "\n",
      "======================================================================\n",
      "[SUCCESS] VAPOR PRESSURE CALCULATION COMPLETED!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct data directory path\n",
    "data_dir = os.path.join(CMIP6_BASE_DIR, f\"{MODEL} {SCENARIO}\")\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise ValueError(f\"Data directory not found: {data_dir}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Processing Coordinate: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
    "print(f\"Model: {MODEL}, Scenario: {SCENARIO}\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nData directory: {data_dir}\\n\")\n",
    "\n",
    "# Extract data for all required variables\n",
    "extracted_data = {}\n",
    "\n",
    "for variable in REQUIRED_VARIABLES:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Processing variable: {variable}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    df = extract_daily_data_from_netcdf(\n",
    "        data_dir, \n",
    "        variable, \n",
    "        LATITUDE, \n",
    "        LONGITUDE, \n",
    "        tolerance=COORD_TOLERANCE\n",
    "    )\n",
    "    \n",
    "    if df is not None and len(df) > 0:\n",
    "        extracted_data[variable] = df\n",
    "        print(f\"  [OK] Extracted {len(df):,} records for {variable}\")\n",
    "    else:\n",
    "        print(f\"  [ERROR] Failed to extract data for {variable}\")\n",
    "\n",
    "# Check if all required variables are available\n",
    "missing_vars = [v for v in REQUIRED_VARIABLES if v not in extracted_data]\n",
    "\n",
    "if missing_vars:\n",
    "    raise ValueError(f\"Missing required variables: {missing_vars}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Calculating Vapor Pressure...\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Calculate vapor pressure using SILO method\n",
    "vp_df = calculate_vapor_pressure(\n",
    "    extracted_data['hurs'],\n",
    "    extracted_data['tasmax'],\n",
    "    extracted_data['tasmin']\n",
    ")\n",
    "\n",
    "print(f\"  [OK] Calculated vapor pressure for {len(vp_df):,} days\")\n",
    "print(f\"  Date range: {vp_df['date'].min()} to {vp_df['date'].max()}\")\n",
    "print(f\"  VP range: {vp_df['value'].min():.2f} to {vp_df['value'].max():.2f} hPa\")\n",
    "print(f\"  VP mean: {vp_df['value'].mean():.2f} hPa\")\n",
    "\n",
    "# Save to CSV\n",
    "lat_str = f\"{LATITUDE:.2f}\"\n",
    "lon_str = f\"{LONGITUDE:.2f}\"\n",
    "model_scenario = f\"{MODEL.replace(' ', '_')}_{SCENARIO}\"\n",
    "output_filename = f\"{model_scenario}_{lat_str}_{lon_str}_vp.csv\"\n",
    "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "\n",
    "vp_df.to_csv(output_path, index=False, encoding='utf-8', float_format='%.2f')\n",
    "print(f\"\\n  [OK] Saved vapor pressure data to: {output_filename}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[SUCCESS] VAPOR PRESSURE CALCULATION COMPLETED!\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
