{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CMIP6 Vapor Pressure (VP) Calculation - SILO Method\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook calculates vapor pressure (VP) from CMIP6 climate data using the SILO method. This method matches SILO units (hPa) and uses mean relative humidity and mean temperature.\n",
        "\n",
        "## Input Variables\n",
        "\n",
        "- **hurs**: Mean relative humidity (%) - required\n",
        "- **tasmax**: Daily maximum temperature (°C) - required\n",
        "- **tasmin**: Daily minimum temperature (°C) - required\n",
        "\n",
        "## Output\n",
        "\n",
        "- **vp**: Vapor pressure (hPa) matching SILO units\n",
        "\n",
        "## Calculation Method (SILO)\n",
        "\n",
        "### Saturation Vapor Pressure (kPa)\n",
        "\n",
        "For mean temperature T_mean in °C:\n",
        "\n",
        "```\n",
        "e_s(T_mean) = 0.611 × exp(17.27 × T_mean / (T_mean + 237.3))\n",
        "```\n",
        "\n",
        "Where: T_mean = (tasmax + tasmin) / 2\n",
        "\n",
        "### Actual Vapor Pressure (hPa)\n",
        "\n",
        "Using mean relative humidity (hurs) and saturation vapor pressure:\n",
        "\n",
        "```\n",
        "VP(hPa) = 10 × (hurs/100) × e_s(T_mean)\n",
        "```\n",
        "\n",
        "Or directly in hPa:\n",
        "\n",
        "```\n",
        "VP(hPa) = (hurs/100) × 0.611 × exp(17.27 × T_mean / (T_mean + 237.3)) × 10\n",
        "```\n",
        "\n",
        "This gives a daily VP proxy. SILO VP is \"9am-like\"; to replicate SILO closely, you would bias-correct this VP proxy to SILO over a historical overlap period.\n",
        "\n",
        "## Usage\n",
        "\n",
        "1. Set configuration parameters (Model, Scenario, coordinates)\n",
        "2. Extract hurs, tasmax, and tasmin data from NetCDF files\n",
        "3. Calculate vapor pressure using the SILO method with mean humidity\n",
        "4. Save results to CSV file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Imports and Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:42.625937Z",
          "iopub.status.busy": "2025-12-21T10:28:42.625383Z",
          "iopub.status.idle": "2025-12-21T10:28:43.009051Z",
          "shell.execute_reply": "2025-12-21T10:28:43.008610Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xarray as xr\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"Libraries imported successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:43.030888Z",
          "iopub.status.busy": "2025-12-21T10:28:43.030535Z",
          "iopub.status.idle": "2025-12-21T10:28:43.034431Z",
          "shell.execute_reply": "2025-12-21T10:28:43.033986Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  [INFO] Using manual output directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\n",
            "======================================================================\n",
            "CONFIGURATION\n",
            "======================================================================\n",
            "  Model: ACCESS CM2\n",
            "  Scenario: SSP245\n",
            "  Coordinates: (-31.750000, 117.599998)\n",
            "  CMIP6 Base Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\n",
            "  Output Directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\n",
            "  Required Variables: hurs, tasmax, tasmin\n",
            "======================================================================\n",
            "\n",
            "All paths and filenames will automatically use the above settings.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# CONFIGURATION - CHANGE VALUES BELOW AS NEEDED\n",
        "# ============================================================================\n",
        "# All other settings will automatically adjust based on these values\n",
        "\n",
        "# Output Directory - OPTIONAL: Set to None to auto-generate, or specify a custom path\n",
        "OUTPUT_DIR_MANUAL = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Anameka_South_16_226042\"  # Set to None for auto-generation\n",
        "\n",
        "# Model (usually doesn't need to change)\n",
        "MODEL = \"ACCESS CM2\"  # e.g., \"ACCESS CM2\"\n",
        "\n",
        "# Scenario - CHANGE THIS\n",
        "SCENARIO = \"SSP245\"   # Options: \"SSP245\", \"SSP585\", etc.\n",
        "\n",
        "# Coordinates - CHANGE THESE\n",
        "LATITUDE = -31.75   # Target latitude in decimal degrees (-90 to 90)\n",
        "LONGITUDE = 117.5999984741211  # Target longitude in decimal degrees (-180 to 180)\n",
        "\n",
        "# ============================================================================\n",
        "# AUTOMATIC SETTINGS (derived from above - no need to change)\n",
        "# ============================================================================\n",
        "\n",
        "# Base directories\n",
        "CMIP6_BASE_DIR = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\"\n",
        "base_output_dir = r\"C:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\"\n",
        "COORD_TOLERANCE = 0.01  # degrees (approximately 1.1 km)\n",
        "\n",
        "# Auto-generate output directory and filename components based on scenario and coordinates\n",
        "# For directory names, use underscore format (filesystem-friendly)\n",
        "lat_str_dir = f\"{LATITUDE:.2f}\".replace('.', '_').replace('-', 'neg')\n",
        "lon_str_dir = f\"{LONGITUDE:.2f}\".replace('.', '_').replace('-', 'neg')\n",
        "# For output filenames, use decimal format (keep dots and minus signs)\n",
        "lat_str = f\"{LATITUDE:.2f}\"\n",
        "lon_str = f\"{LONGITUDE:.2f}\"\n",
        "model_scenario = f\"{MODEL.replace(' ', '_')}_{SCENARIO}\"\n",
        "model_scenario_dir = f\"{model_scenario}_{lat_str_dir}_{lon_str_dir}\"\n",
        "\n",
        "# Use manual output directory if specified, otherwise auto-generate\n",
        "if OUTPUT_DIR_MANUAL is not None and OUTPUT_DIR_MANUAL != \"\":\n",
        "    OUTPUT_DIR = OUTPUT_DIR_MANUAL\n",
        "    print(f\"  [INFO] Using manual output directory: {OUTPUT_DIR}\")\n",
        "else:\n",
        "    OUTPUT_DIR = os.path.join(base_output_dir, model_scenario_dir)\n",
        "    print(f\"  [INFO] Auto-generated output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Variables required for VP calculation (SILO method with mean humidity)\n",
        "REQUIRED_VARIABLES = ['hurs', 'tasmax', 'tasmin']\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Model: {MODEL}\")\n",
        "print(f\"  Scenario: {SCENARIO}\")\n",
        "print(f\"  Coordinates: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
        "print(f\"  CMIP6 Base Directory: {CMIP6_BASE_DIR}\")\n",
        "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
        "print(f\"  Required Variables: {', '.join(REQUIRED_VARIABLES)}\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAll paths and filenames will automatically use the above settings.\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2.5: Caching Functions\n",
        "\n",
        "To optimize performance, extracted NetCDF data is cached to CSV files. On subsequent runs, \n",
        "cached data is loaded automatically instead of re-extracting from NetCDF files.\n",
        "\n",
        "**Cache Location:** Cached files are saved in the output directory with naming convention:\n",
        "`{model_scenario}_{lat_str}_{lon_str}_{variable}.csv`\n",
        "\n",
        "**Cache Behavior:**\n",
        "- If cache exists and is valid → Load from cache (fast)\n",
        "- If cache doesn't exist or is invalid → Extract from NetCDF files and save to cache\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cached_variable_path(output_dir, model_scenario, lat_str, lon_str, variable):\n",
        "    \"\"\"\n",
        "    Generate the path for a cached variable CSV file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    output_dir : str\n",
        "        Output directory\n",
        "    model_scenario : str\n",
        "        Model and scenario string (e.g., \"ACCESS_CM2_SSP245\")\n",
        "    lat_str : str\n",
        "        Latitude formatted as string (e.g., \"-31.75\")\n",
        "    lon_str : str\n",
        "        Longitude formatted as string (e.g., \"117.60\")\n",
        "    variable : str\n",
        "        Variable name (e.g., \"tasmax\", \"hurs\")\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    str\n",
        "        Path to cached CSV file\n",
        "    \"\"\"\n",
        "    cache_filename = f\"{model_scenario}_{lat_str}_{lon_str}_{variable}.csv\"\n",
        "    cache_path = os.path.join(output_dir, cache_filename)\n",
        "    return cache_path\n",
        "\n",
        "\n",
        "def load_cached_variable(cache_path):\n",
        "    \"\"\"\n",
        "    Load cached variable data from CSV file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    cache_path : str\n",
        "        Path to cached CSV file\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame or None\n",
        "        DataFrame with date and value columns if file exists and is valid, None otherwise\n",
        "    \"\"\"\n",
        "    print(f\"  [INFO] Checking cache: {os.path.basename(cache_path)}\")\n",
        "    \n",
        "    if not os.path.exists(cache_path):\n",
        "        print(f\"  [INFO] Cache file not found, will extract from NetCDF\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(cache_path)\n",
        "        if 'date' not in df.columns or 'value' not in df.columns:\n",
        "            print(f\"  [WARNING] Cached file missing required columns, will re-extract\")\n",
        "            return None\n",
        "        \n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        \n",
        "        # Basic validation - check if file has data\n",
        "        if len(df) == 0:\n",
        "            print(f\"  [WARNING] Cached file is empty, will re-extract\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  [INFO] Cache file found and valid\")\n",
        "        return df\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"  [WARNING] Error loading cached file: {e}, will re-extract\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def save_cached_variable(df, cache_path):\n",
        "    \"\"\"\n",
        "    Save extracted variable data to CSV cache file.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame with date and value columns\n",
        "    cache_path : str\n",
        "        Path to save cached CSV file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df[['date', 'value']].to_csv(\n",
        "            cache_path,\n",
        "            index=False,\n",
        "            encoding='utf-8',\n",
        "            float_format='%.6f'\n",
        "        )\n",
        "        print(f\"  [INFO] Saved to cache: {os.path.basename(cache_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  [WARNING] Failed to save cache: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: NetCDF Data Extraction Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:43.036177Z",
          "iopub.status.busy": "2025-12-21T10:28:43.035959Z",
          "iopub.status.idle": "2025-12-21T10:28:43.044965Z",
          "shell.execute_reply": "2025-12-21T10:28:43.044368Z"
        }
      },
      "outputs": [],
      "source": [
        "def extract_daily_data_from_netcdf(netcdf_dir, variable, target_lat, target_lon, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Extract daily time series data for a specific coordinate from NetCDF files.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    netcdf_dir : str\n",
        "        Directory containing NetCDF files for the variable\n",
        "    variable : str\n",
        "        Variable name (hurs, tasmax, tasmin)\n",
        "    target_lat : float\n",
        "        Target latitude\n",
        "    target_lon : float\n",
        "        Target longitude\n",
        "    tolerance : float\n",
        "        Coordinate matching tolerance in degrees\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with columns: date, value\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Find all NetCDF files in the directory\n",
        "    nc_files = sorted(glob.glob(os.path.join(netcdf_dir, f\"*{variable}*.nc\")))\n",
        "    \n",
        "    # Pattern 2: Files in subdirectories named {variable}_*\n",
        "    if len(nc_files) == 0:\n",
        "        var_subdirs = glob.glob(os.path.join(netcdf_dir, f\"{variable}_*\"))\n",
        "        for var_subdir in var_subdirs:\n",
        "            if os.path.isdir(var_subdir):\n",
        "                found_files = sorted(glob.glob(os.path.join(var_subdir, \"*.nc\")))\n",
        "                if found_files:\n",
        "                    nc_files.extend(found_files)\n",
        "                    print(f\"  Found files in subdirectory: {os.path.basename(var_subdir)}/\")\n",
        "                    break\n",
        "    \n",
        "    if len(nc_files) == 0:\n",
        "        print(f\"  ERROR: No NetCDF files found in {netcdf_dir}\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"  Found {len(nc_files)} NetCDF files\")\n",
        "    \n",
        "    # Cache coordinate information from first file\n",
        "    lat_name = None\n",
        "    lon_name = None\n",
        "    time_name = None\n",
        "    lat_idx = None\n",
        "    lon_idx = None\n",
        "    var_name = None\n",
        "    \n",
        "    # List to store daily data\n",
        "    all_data = []\n",
        "    \n",
        "    # Process first file to get coordinate structure\n",
        "    if len(nc_files) > 0:\n",
        "        try:\n",
        "            ds_sample = xr.open_dataset(nc_files[0], decode_times=False)\n",
        "            \n",
        "            # Get variable name\n",
        "            for v in ds_sample.data_vars:\n",
        "                if variable in v.lower() or v.lower() in variable.lower():\n",
        "                    var_name = v\n",
        "                    break\n",
        "            \n",
        "            if var_name is None:\n",
        "                possible_names = [variable, variable.upper(), f'{variable}_day']\n",
        "                for name in possible_names:\n",
        "                    if name in ds_sample.data_vars:\n",
        "                        var_name = name\n",
        "                        break\n",
        "            \n",
        "            # Get coordinate names\n",
        "            for coord in ds_sample.coords:\n",
        "                coord_lower = coord.lower()\n",
        "                if 'lat' in coord_lower:\n",
        "                    lat_name = coord\n",
        "                elif 'lon' in coord_lower:\n",
        "                    lon_name = coord\n",
        "                elif 'time' in coord_lower:\n",
        "                    time_name = coord\n",
        "            \n",
        "            if lat_name and lon_name:\n",
        "                # Find nearest grid point\n",
        "                lat_idx = np.abs(ds_sample[lat_name].values - target_lat).argmin()\n",
        "                lon_idx = np.abs(ds_sample[lon_name].values - target_lon).argmin()\n",
        "                \n",
        "                actual_lat = float(ds_sample[lat_name].values[lat_idx])\n",
        "                actual_lon = float(ds_sample[lon_name].values[lon_idx])\n",
        "                \n",
        "                # Check if within tolerance\n",
        "                if abs(actual_lat - target_lat) > tolerance or abs(actual_lon - target_lon) > tolerance:\n",
        "                    print(f\"  Warning: Nearest point ({actual_lat:.4f}, {actual_lon:.4f}) is outside tolerance\")\n",
        "                else:\n",
        "                    print(f\"  Using grid point: ({actual_lat:.4f}, {actual_lon:.4f})\")\n",
        "            \n",
        "            ds_sample.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Warning: Could not read sample file: {e}\")\n",
        "    \n",
        "    if var_name is None or lat_idx is None or lon_idx is None:\n",
        "        print(f\"  ERROR: Could not determine coordinate structure\")\n",
        "        return None\n",
        "    \n",
        "    # Process all files with progress bar\n",
        "    print(f\"  Processing files...\")\n",
        "    for nc_file in tqdm(nc_files, desc=f\"  {variable}\", unit=\"file\"):\n",
        "        try:\n",
        "            ds = xr.open_dataset(nc_file, decode_times=False)\n",
        "            \n",
        "            # Extract data using cached indices\n",
        "            data = ds[var_name].isel({lat_name: lat_idx, lon_name: lon_idx})\n",
        "            \n",
        "            # Convert to numpy array\n",
        "            values = data.values\n",
        "            if values.ndim > 1:\n",
        "                values = values.flatten()\n",
        "            \n",
        "            # Get time values\n",
        "            time_values = None\n",
        "            \n",
        "            # Method 1: Try to use time coordinate from NetCDF file\n",
        "            if time_name and time_name in ds.coords:\n",
        "                try:\n",
        "                    time_coord = ds[time_name]\n",
        "                    if len(time_coord) == len(values):\n",
        "                        try:\n",
        "                            time_decoded = xr.decode_cf(ds[[time_name]])[time_name]\n",
        "                            time_values = pd.to_datetime(time_decoded.values)\n",
        "                        except:\n",
        "                            if hasattr(time_coord, 'units') and 'days since' in time_coord.units.lower():\n",
        "                                base_date_str = time_coord.units.split('since')[1].strip().split()[0]\n",
        "                                base_date = pd.to_datetime(base_date_str)\n",
        "                                time_values = base_date + pd.to_timedelta(time_coord.values, unit='D')\n",
        "                except Exception as e:\n",
        "                    pass\n",
        "            \n",
        "            # Method 2: Extract year from filename\n",
        "            if time_values is None:\n",
        "                year = None\n",
        "                filename = os.path.basename(nc_file)\n",
        "                all_years = re.findall(r'\\d{4}', filename)\n",
        "                for year_str in all_years:\n",
        "                    year_candidate = int(year_str)\n",
        "                    if 2000 <= year_candidate <= 2100:\n",
        "                        year = year_candidate\n",
        "                        break\n",
        "                \n",
        "                if year:\n",
        "                    time_values = pd.date_range(start=f'{year}-01-01', periods=len(values), freq='D')\n",
        "                else:\n",
        "                    time_values = pd.date_range(start='2035-01-01', periods=len(values), freq='D')\n",
        "            \n",
        "            # Ensure correct number of dates\n",
        "            if len(time_values) != len(values):\n",
        "                if len(time_values) > len(values):\n",
        "                    time_values = time_values[:len(values)]\n",
        "            \n",
        "            # Create DataFrame for this file\n",
        "            if len(values) > 0:\n",
        "                df_file = pd.DataFrame({\n",
        "                    'date': time_values[:len(values)],\n",
        "                    'value': values\n",
        "                })\n",
        "                all_data.append(df_file)\n",
        "            \n",
        "            ds.close()\n",
        "            \n",
        "        except Exception as e:\n",
        "            tqdm.write(f\"    Error processing {os.path.basename(nc_file)}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if len(all_data) == 0:\n",
        "        print(f\"  ERROR: No data extracted\")\n",
        "        return None\n",
        "    \n",
        "    # Combine all data\n",
        "    print(f\"  Combining data from {len(all_data)} files...\")\n",
        "    combined_df = pd.concat(all_data, ignore_index=True)\n",
        "    \n",
        "    # Sort by date\n",
        "    combined_df = combined_df.sort_values('date').reset_index(drop=True)\n",
        "    \n",
        "    # Remove duplicate dates (keep first occurrence)\n",
        "    combined_df = combined_df.drop_duplicates(subset='date', keep='first')\n",
        "    \n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(f\"  ✓ Extracted {len(combined_df):,} daily records in {elapsed_time:.1f} seconds\")\n",
        "    print(f\"  Date range: {combined_df['date'].min()} to {combined_df['date'].max()}\")\n",
        "    \n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Vapor Pressure Calculation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:43.046640Z",
          "iopub.status.busy": "2025-12-21T10:28:43.046444Z",
          "iopub.status.idle": "2025-12-21T10:28:43.049925Z",
          "shell.execute_reply": "2025-12-21T10:28:43.049476Z"
        }
      },
      "outputs": [],
      "source": [
        "def calculate_saturation_vapor_pressure(temperature):\n",
        "    \"\"\"\n",
        "    Calculate saturation vapor pressure (kPa) at a given temperature.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    temperature : float or array\n",
        "        Temperature in °C\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    float or array\n",
        "        Saturation vapor pressure in kPa\n",
        "    \"\"\"\n",
        "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
        "    return 0.611 * np.exp(17.27 * temperature / (temperature + 237.3))\n",
        "\n",
        "\n",
        "def calculate_vapor_pressure(hurs_df, tasmax_df, tasmin_df):\n",
        "    \"\"\"\n",
        "    Calculate vapor pressure (hPa) from mean relative humidity and temperature using SILO method.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    hurs_df : pd.DataFrame\n",
        "        DataFrame with date and value (mean relative humidity %) columns\n",
        "    tasmax_df : pd.DataFrame\n",
        "        DataFrame with date and value (maximum temperature °C) columns\n",
        "    tasmin_df : pd.DataFrame\n",
        "        DataFrame with date and value (minimum temperature °C) columns\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame with date and value (vapor pressure hPa) columns\n",
        "    \"\"\"\n",
        "    # #region agent log\n",
        "    import json\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\",\"location\":\"calculate_vapor_pressure:entry\",\"message\":\"Function entry\",\"data\":{\"hurs_shape\":list(hurs_df.shape) if hurs_df is not None else None,\"tasmax_shape\":list(tasmax_df.shape) if tasmax_df is not None else None,\"tasmin_shape\":list(tasmin_df.shape) if tasmin_df is not None else None},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    # Merge temperature dataframes\n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"B\",\"location\":\"calculate_vapor_pressure:before_temp_merge\",\"message\":\"Before temp merge\",\"data\":{\"tasmax_dates_count\":len(tasmax_df['date'].unique()) if 'date' in tasmax_df.columns else 0,\"tasmin_dates_count\":len(tasmin_df['date'].unique()) if 'date' in tasmin_df.columns else 0,\"tasmax_has_date\":'date' in tasmax_df.columns,\"tasmin_has_date\":'date' in tasmin_df.columns},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    temp_df = tasmax_df.merge(tasmin_df, on='date', suffixes=('_max', '_min'))\n",
        "    \n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"B\",\"location\":\"calculate_vapor_pressure:after_temp_merge\",\"message\":\"After temp merge\",\"data\":{\"temp_df_shape\":list(temp_df.shape),\"temp_df_cols\":list(temp_df.columns)},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    temp_df['tmean'] = (temp_df['value_max'] + temp_df['value_min']) / 2.0\n",
        "    \n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"C\",\"location\":\"calculate_vapor_pressure:after_tmean\",\"message\":\"After tmean calc\",\"data\":{\"tmean_nan_count\":int(temp_df['tmean'].isna().sum()),\"tmean_min\":float(temp_df['tmean'].min()) if not temp_df['tmean'].isna().all() else None,\"tmean_max\":float(temp_df['tmean'].max()) if not temp_df['tmean'].isna().all() else None},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    # Merge with mean humidity\n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"D\",\"location\":\"calculate_vapor_pressure:before_hurs_merge\",\"message\":\"Before hurs merge\",\"data\":{\"hurs_dates_count\":len(hurs_df['date'].unique()) if 'date' in hurs_df.columns else 0,\"temp_dates_count\":len(temp_df['date'].unique()),\"hurs_has_value\":'value' in hurs_df.columns},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    merged = hurs_df.merge(temp_df[['date', 'tmean']], on='date')\n",
        "    \n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"D\",\"location\":\"calculate_vapor_pressure:after_hurs_merge\",\"message\":\"After hurs merge\",\"data\":{\"merged_shape\":list(merged.shape),\"merged_cols\":list(merged.columns),\"merged_empty\":merged.empty},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    # Calculate saturation vapor pressure at mean temperature (in kPa)\n",
        "    # SILO formula: e_s(T) = 0.611 × exp(17.27 × T / (T + 237.3))\n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"E\",\"location\":\"calculate_vapor_pressure:before_es_calc\",\"message\":\"Before es calculation\",\"data\":{\"tmean_has_nan\":merged['tmean'].isna().any(),\"tmean_dtype\":str(merged['tmean'].dtype)},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    merged['es_kpa'] = calculate_saturation_vapor_pressure(merged['tmean'])\n",
        "    \n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"E\",\"location\":\"calculate_vapor_pressure:after_es_calc\",\"message\":\"After es calculation\",\"data\":{\"es_kpa_nan_count\":int(merged['es_kpa'].isna().sum()),\"es_kpa_min\":float(merged['es_kpa'].min()) if not merged['es_kpa'].isna().all() else None},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    # Calculate actual vapor pressure using mean relative humidity (in kPa)\n",
        "    # e_a = (hurs/100) × e_s(T_mean)\n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"F\",\"location\":\"calculate_vapor_pressure:before_ea_calc\",\"message\":\"Before ea calculation\",\"data\":{\"hurs_value_nan_count\":int(merged['value'].isna().sum()) if 'value' in merged.columns else -1,\"hurs_value_dtype\":str(merged['value'].dtype) if 'value' in merged.columns else \"missing\"},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    merged['ea_kpa'] = (merged['value'] / 100.0) * merged['es_kpa']\n",
        "    \n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"F\",\"location\":\"calculate_vapor_pressure:after_ea_calc\",\"message\":\"After ea calculation\",\"data\":{\"ea_kpa_nan_count\":int(merged['ea_kpa'].isna().sum())},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    # Convert to SILO VP units (hPa): VP(hPa) = 10 × e_a(kPa)\n",
        "    merged['vp'] = 10.0 * merged['ea_kpa']\n",
        "    \n",
        "    # Return DataFrame with date and vp columns\n",
        "    vp_df = merged[['date', 'vp']].copy()\n",
        "    vp_df = vp_df.rename(columns={'vp': 'value'})\n",
        "    \n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\",\"location\":\"calculate_vapor_pressure:exit\",\"message\":\"Function exit\",\"data\":{\"vp_df_shape\":list(vp_df.shape),\"vp_df_has_value\":'value' in vp_df.columns},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    \n",
        "    return vp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Main Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-12-21T10:28:43.051522Z",
          "iopub.status.busy": "2025-12-21T10:28:43.051284Z",
          "iopub.status.idle": "2025-12-21T10:30:25.973061Z",
          "shell.execute_reply": "2025-12-21T10:30:25.972457Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "Processing Coordinate: (-31.750000, 117.599998)\n",
            "Model: ACCESS CM2, Scenario: SSP245\n",
            "======================================================================\n",
            "\n",
            "Data directory: C:\\Users\\ibian\\Desktop\\ClimAdapt\\CMIP6\\ACCESS CM2 SSP245\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Processing variable: hurs\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_SSP245_-31.75_117.60_hurs.csv\n",
            "  [INFO] Cache file found and valid\n",
            "  [OK] Loaded from cache: 10,957 records for hurs\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmax\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_SSP245_-31.75_117.60_tasmax.csv\n",
            "  [INFO] Cache file found and valid\n",
            "  [OK] Loaded from cache: 10,957 records for tasmax\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Processing variable: tasmin\n",
            "======================================================================\n",
            "  [INFO] Checking cache: ACCESS_CM2_SSP245_-31.75_117.60_tasmin.csv\n",
            "  [INFO] Cache file found and valid\n",
            "  [OK] Loaded from cache: 10,957 records for tasmin\n",
            "  [INFO] Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "\n",
            "======================================================================\n",
            "Calculating Vapor Pressure...\n",
            "======================================================================\n",
            "[DEBUG] About to call calculate_vapor_pressure\n",
            "[DEBUG] extracted_data keys: ['hurs', 'tasmax', 'tasmin']\n",
            "[DEBUG] Calling calculate_vapor_pressure...\n",
            "[DEBUG] calculate_vapor_pressure returned, shape: (10957, 2)\n",
            "  [OK] Calculated vapor pressure for 10,957 days\n",
            "  Date range: 2035-01-01 00:00:00 to 2064-12-30 00:00:00\n",
            "  VP range: 4.10 to 34.15 hPa\n",
            "  VP mean: 12.41 hPa\n",
            "\n",
            "  [INFO] Filename components: lat_str='-31.75', lon_str='117.60'\n",
            "\n",
            "  [OK] Saved vapor pressure data to: ACCESS_CM2_SSP245_-31.75_117.60_vp.csv\n",
            "\n",
            "======================================================================\n",
            "[SUCCESS] VAPOR PRESSURE CALCULATION COMPLETED!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Construct data directory path\n",
        "data_dir = os.path.join(CMIP6_BASE_DIR, f\"{MODEL} {SCENARIO}\")\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    raise ValueError(f\"Data directory not found: {data_dir}\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(f\"Processing Coordinate: ({LATITUDE:.6f}, {LONGITUDE:.6f})\")\n",
        "print(f\"Model: {MODEL}, Scenario: {SCENARIO}\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nData directory: {data_dir}\\n\")\n",
        "\n",
        "# Extract data for all required variables\n",
        "extracted_data = {}\n",
        "\n",
        "for variable in REQUIRED_VARIABLES:\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Processing variable: {variable}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Check for cached data first\n",
        "    cache_path = get_cached_variable_path(OUTPUT_DIR, model_scenario, lat_str, lon_str, variable)\n",
        "    df = load_cached_variable(cache_path)\n",
        "    \n",
        "    if df is not None:\n",
        "        # Use cached data\n",
        "        extracted_data[variable] = df\n",
        "        print(f\"  [OK] Loaded from cache: {len(df):,} records for {variable}\")\n",
        "        print(f\"  [INFO] Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "    else:\n",
        "        # Extract from NetCDF files\n",
        "        df = extract_daily_data_from_netcdf(\n",
        "            data_dir, \n",
        "            variable, \n",
        "            LATITUDE, \n",
        "            LONGITUDE, \n",
        "            tolerance=COORD_TOLERANCE\n",
        "        )\n",
        "        \n",
        "        if df is not None and len(df) > 0:\n",
        "            extracted_data[variable] = df\n",
        "            print(f\"  [OK] Extracted {len(df):,} records for {variable}\")\n",
        "            # Save to cache for future runs\n",
        "            save_cached_variable(df, cache_path)\n",
        "        else:\n",
        "            print(f\"  [ERROR] Failed to extract data for {variable}\")\n",
        "\n",
        "# Check if all required variables are available\n",
        "missing_vars = [v for v in REQUIRED_VARIABLES if v not in extracted_data]\n",
        "\n",
        "if missing_vars:\n",
        "    raise ValueError(f\"Missing required variables: {missing_vars}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Calculating Vapor Pressure...\")\n",
        "print(f\"{'='*70}\")\n",
        "\n",
        "print(f\"[DEBUG] About to call calculate_vapor_pressure\")\n",
        "print(f\"[DEBUG] extracted_data keys: {list(extracted_data.keys())}\")\n",
        "\n",
        "# #region agent log\n",
        "import json\n",
        "try:\n",
        "    with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "        f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\",\"location\":\"cell11:before_calc\",\"message\":\"Before calculate_vapor_pressure call\",\"data\":{\"extracted_keys\":list(extracted_data.keys())},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "except Exception as log_err:\n",
        "    print(f\"[DEBUG] Log write failed: {log_err}\")\n",
        "# #endregion\n",
        "\n",
        "# Calculate vapor pressure using SILO method\n",
        "print(f\"[DEBUG] Calling calculate_vapor_pressure...\")\n",
        "try:\n",
        "    vp_df = calculate_vapor_pressure(\n",
        "        extracted_data['hurs'],\n",
        "        extracted_data['tasmax'],\n",
        "        extracted_data['tasmin']\n",
        "    )\n",
        "    print(f\"[DEBUG] calculate_vapor_pressure returned, shape: {vp_df.shape if vp_df is not None else None}\")\n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\",\"location\":\"cell11:after_calc\",\"message\":\"After calculate_vapor_pressure call\",\"data\":{\"vp_df_created\":vp_df is not None,\"vp_df_shape\":list(vp_df.shape) if vp_df is not None else None},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "except Exception as e:\n",
        "    print(f\"[DEBUG] EXCEPTION CAUGHT: {type(e).__name__}: {str(e)}\")\n",
        "    import traceback\n",
        "    print(f\"[DEBUG] Traceback:\\n{traceback.format_exc()}\")\n",
        "    # #region agent log\n",
        "    try:\n",
        "        with open(r'c:\\Users\\ibian\\Desktop\\ClimAdapt\\Anameka\\Grid\\Anameka\\.cursor\\debug.log', 'a', encoding='utf-8') as f:\n",
        "            f.write(json.dumps({\"sessionId\":\"debug-session\",\"runId\":\"run1\",\"hypothesisId\":\"A\",\"location\":\"cell11:exception\",\"message\":\"Exception in calculate_vapor_pressure\",\"data\":{\"exception_type\":type(e).__name__,\"exception_msg\":str(e),\"traceback\":traceback.format_exc()},\"timestamp\":int(__import__('time').time()*1000)})+'\\n')\n",
        "    except: pass\n",
        "    # #endregion\n",
        "    raise\n",
        "\n",
        "print(f\"  [OK] Calculated vapor pressure for {len(vp_df):,} days\")\n",
        "print(f\"  Date range: {vp_df['date'].min()} to {vp_df['date'].max()}\")\n",
        "print(f\"  VP range: {vp_df['value'].min():.2f} to {vp_df['value'].max():.2f} hPa\")\n",
        "print(f\"  VP mean: {vp_df['value'].mean():.2f} hPa\")\n",
        "\n",
        "# Save to CSV (using auto-generated filename components from configuration)\n",
        "# Verify lat_str and lon_str are in decimal format\n",
        "print(f\"\\n  [INFO] Filename components: lat_str='{lat_str}', lon_str='{lon_str}'\")\n",
        "output_filename = f\"{model_scenario}_{lat_str}_{lon_str}_vp.csv\"\n",
        "output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "vp_df.to_csv(output_path, index=False, encoding='utf-8', float_format='%.2f')\n",
        "print(f\"\\n  [OK] Saved vapor pressure data to: {output_filename}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"[SUCCESS] VAPOR PRESSURE CALCULATION COMPLETED!\")\n",
        "print(f\"{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
